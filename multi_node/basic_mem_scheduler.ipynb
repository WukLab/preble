{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import GRB\n",
    "import gurobipy as gp\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from uuid import uuid4\n",
    "import copy\n",
    "import random\n",
    "import threading\n",
    "from enum import Enum, auto\n",
    "import logging\n",
    "from benchmarks.benchmark_utils import RequestFuncOutput\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "logging = logging.getLogger(__name__)\n",
    "DEBUG_COUNTER = 0\n",
    "class LpNode:\n",
    "    def __init__(self, node_id, num_gpus):\n",
    "        self.node_id = node_id\n",
    "        self.variables = [\n",
    "            None for _ in range(num_gpus)\n",
    "        ]  # Will be initialized as binary variables in the model\n",
    "        self.children_token_cost_at_max_depth = 0  # Issue is that depth_limit will cut off the tokens for children and that will treat it as free\n",
    "        self.randomly_selected_gpu = None\n",
    "        self.load_variables = [None for _ in range(num_gpus)]\n",
    "        self.common_load = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        variable_values = [var.x if var else None for var in self.variables]\n",
    "        load_variable_values = [var.x if var else None for var in self.load_variables]\n",
    "        common_load = self.common_load.x if self.common_load else None\n",
    "        # ignore printing laod variables if None\n",
    "        if any(load_variable_values):\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values}, load_variables={load_variable_values}, common_load={common_load})\"\n",
    "        else:\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values})\"\n",
    "\n",
    "\n",
    "class LPTreeNode:\n",
    "    def __init__(self):\n",
    "        self.id = uuid4()\n",
    "        self.children = defaultdict(LPTreeNode)\n",
    "        self.parent: Optional[LPTreeNode] = None\n",
    "        self.value = None\n",
    "        self.ref_counter = 0\n",
    "        self.last_access_time = time.time()\n",
    "        self.gpu_selections = set()\n",
    "        self.is_leaf = False\n",
    "        self.decode_length = 0\n",
    "        self.context_length = 0\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.value)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.last_access_time < other.last_access_time\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, LPTreeNode):\n",
    "            return self.id == other.id  # Compare nodes based on their unique ID\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)  # Use the unique ID for hashing\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"LPTreeNode(id={self.id}, ref_counter={self.ref_counter}, gpu_selections={self.gpu_selections})\"\n",
    "\n",
    "\n",
    "def match(key, seq):\n",
    "    i = 0\n",
    "    for k, w in zip(key, seq):\n",
    "        if k != w:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "class LPRadixCache:\n",
    "    def __init__(self, disable=False):\n",
    "        self.reset()\n",
    "        self.disable = disable\n",
    "\n",
    "    ##### Public API #####\n",
    "\n",
    "    def reset(self):\n",
    "        self.root_node = LPTreeNode()\n",
    "        self.root_node.value = []\n",
    "        self.root_node.ref_counter = 1\n",
    "        self.evictable_size_ = 0\n",
    "\n",
    "    def find_node(self, key):\n",
    "        if self.disable:\n",
    "            return None\n",
    "        current_gpu_selection, node = self.match_prefix_get_gpu_selection(key)\n",
    "        return node\n",
    "\n",
    "    def match_prefix_get_gpu_selection(self, key, path_to_node=[]):\n",
    "        if self.disable:\n",
    "            return [], self.root_node\n",
    "\n",
    "        value = []\n",
    "        current_gpu_selection = self.root_node.gpu_selections\n",
    "        current_gpu_selection, node = self._match_prefix_helper_gpu_selection(\n",
    "            self.root_node, key, value, current_gpu_selection\n",
    "        )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def _match_prefix_helper_gpu_selection(\n",
    "        self, node, key, value, current_gpu_selection\n",
    "    ):\n",
    "        child: LPTreeNode\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len != 0:\n",
    "                if child.gpu_selections:\n",
    "                    current_gpu_selection = child.gpu_selections\n",
    "                if prefix_len < len(c_key):\n",
    "                    print(prefix_len, len(c_key))\n",
    "                    assert False\n",
    "                    new_node = self._split_node(\n",
    "                        c_key, child, prefix_len, new_nodes_created=new_nodes_created\n",
    "                    )\n",
    "                    value.append(new_node.value)\n",
    "                    # last_node[0] = new_node\n",
    "                else:\n",
    "                    value.append(child.value)\n",
    "                    # last_node[0] = child\n",
    "                    return self._match_prefix_helper_gpu_selection(\n",
    "                        child, key[prefix_len:], value, current_gpu_selection\n",
    "                    )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def match_prefix_return_str(self, key):\n",
    "        return \"\".join(self.match_prefix(key)[0])\n",
    "\n",
    "    def insert(\n",
    "        self,\n",
    "        key,\n",
    "        value=None,\n",
    "        node_map=None,\n",
    "        all_modified_nodes=None,\n",
    "        split_nodes=None,\n",
    "        depth_limit=0,\n",
    "    ):\n",
    "        if node_map is None:\n",
    "            node_map = {}\n",
    "        if all_modified_nodes is None:\n",
    "            all_modified_nodes = set()\n",
    "        if split_nodes is None:\n",
    "            split_nodes = {}  # key -> node\n",
    "        if self.disable:\n",
    "            return len(key)\n",
    "\n",
    "        if value is None:\n",
    "            value = [x for x in key]\n",
    "        modified_nodes = set()\n",
    "        created_node = self._insert_helper(\n",
    "            self.root_node,\n",
    "            key,\n",
    "            value,\n",
    "            node_map=node_map,\n",
    "            modified_nodes=modified_nodes,\n",
    "            depth_limit=depth_limit,\n",
    "            current_depth=0,\n",
    "            split_nodes=split_nodes,\n",
    "        )\n",
    "\n",
    "        node: LPTreeNode = created_node\n",
    "        while node is not None:\n",
    "            if node in all_modified_nodes:\n",
    "                break\n",
    "            all_modified_nodes.add(node)\n",
    "            node = node.parent\n",
    "        return created_node\n",
    "\n",
    "    def pretty_print(self):\n",
    "        self._print_helper(self.root_node, 0)\n",
    "        print(f\"#tokens: {self.total_size()}\")\n",
    "\n",
    "    def total_size(self):\n",
    "        return self._total_size_helper(self.root_node)\n",
    "\n",
    "    def evict(self, num_tokens, evict_callback):\n",
    "        if self.disable:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        leaves = self._collect_leaves()\n",
    "        heapq.heapify(leaves)\n",
    "\n",
    "        num_evicted = 0\n",
    "        while num_evicted < num_tokens and len(leaves):\n",
    "            x = heapq.heappop(leaves)\n",
    "\n",
    "            if x == self.root_node:\n",
    "                break\n",
    "            if x.ref_counter > 0:\n",
    "                continue\n",
    "\n",
    "            num_evicted += evict_callback(x)\n",
    "            self._delete_leaf(x)\n",
    "\n",
    "            if len(x.parent.children) == 0:\n",
    "                heapq.heappush(leaves, x.parent)\n",
    "\n",
    "    def inc_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 0:\n",
    "                self.evictable_size_ -= len(node.value)\n",
    "                delta -= len(node.value)\n",
    "            node.ref_counter += 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def dec_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            # if node.ref_counter == 1: TODO why does this exist?\n",
    "            #     self.evictable_size_ += len(node.value)\n",
    "            #     delta += len(node.value)\n",
    "            node.ref_counter -= 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def remove_completed_input_ids(self, input_ids):\n",
    "        node = self.find_node(input_ids)\n",
    "        self.dec_ref_counter(node)  # remove reference counter up to parent\n",
    "    \n",
    "    def evictable_size(self):\n",
    "        return self.evictable_size_\n",
    "\n",
    "    def _split_node(\n",
    "        self, key, child: LPTreeNode, split_len, node_map, depth_limit, current_depth\n",
    "    ):\n",
    "        # new_node -> child\n",
    "        new_node = LPTreeNode()\n",
    "        new_node.gpu_selections = copy.deepcopy(child.gpu_selections)\n",
    "        new_node.children = {key[split_len:]: child}\n",
    "        new_node.parent = child.parent\n",
    "        new_node.ref_counter = child.ref_counter\n",
    "        new_node.context_length = child.parent.context_length + split_len\n",
    "\n",
    "        new_node.value = child.value[:split_len]\n",
    "        child.parent = new_node\n",
    "        child.value = child.value[split_len:]\n",
    "\n",
    "        new_node.parent.children[key[:split_len]] = new_node\n",
    "        del new_node.parent.children[key]\n",
    "        return new_node\n",
    "\n",
    "    def _insert_helper(\n",
    "        self,\n",
    "        node: LPTreeNode,\n",
    "        key,\n",
    "        value,\n",
    "        node_map,\n",
    "        modified_nodes,\n",
    "        depth_limit,\n",
    "        current_depth,\n",
    "        split_nodes,\n",
    "        parent_context_length = 0\n",
    "    ):\n",
    "        node.last_access_time = time.time()\n",
    "        node.ref_counter += 1\n",
    "\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len == len(c_key):\n",
    "                if prefix_len == len(key):\n",
    "                    child.ref_counter += 1\n",
    "                    modified_nodes.add(child)\n",
    "                    return child\n",
    "                else:\n",
    "                    key = key[prefix_len:]\n",
    "                    value = value[prefix_len:]\n",
    "                    return self._insert_helper(\n",
    "                        child,\n",
    "                        key,\n",
    "                        value,\n",
    "                        node_map=node_map,\n",
    "                        modified_nodes=modified_nodes,\n",
    "                        depth_limit=depth_limit,\n",
    "                        current_depth=current_depth + 1,\n",
    "                        split_nodes=split_nodes,\n",
    "                        parent_context_length=parent_context_length + prefix_len,\n",
    "                    )\n",
    "\n",
    "            if prefix_len:\n",
    "                new_node = self._split_node(\n",
    "                    c_key,\n",
    "                    child,\n",
    "                    prefix_len,\n",
    "                    node_map,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                )\n",
    "                # modified_nodes.add(new_node)\n",
    "                # modified_nodes.add(child)\n",
    "                # TODO check if this makes sense to ignore this?\n",
    "                # if child in node_map and current_depth < depth_limit:\n",
    "                split_nodes[child] = new_node\n",
    "                return self._insert_helper(\n",
    "                    new_node,\n",
    "                    key[prefix_len:],\n",
    "                    value[prefix_len:],\n",
    "                    node_map=node_map,\n",
    "                    modified_nodes=modified_nodes,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                    split_nodes=split_nodes,\n",
    "                    parent_context_length=parent_context_length + prefix_len,\n",
    "                )\n",
    "\n",
    "        if len(key):\n",
    "            new_node = LPTreeNode()\n",
    "            new_node.gpu_selections = set()\n",
    "            new_node.parent = node\n",
    "            new_node.value = value\n",
    "            new_node.ref_counter = 1\n",
    "            new_node.context_length = parent_context_length + len(key)\n",
    "\n",
    "            node.children[key] = new_node\n",
    "            self.evictable_size_ += len(value)\n",
    "            # if current_depth < depth_limit:\n",
    "            modified_nodes.add(new_node)\n",
    "            # return new_node\n",
    "            return new_node\n",
    "        return node\n",
    "\n",
    "    def _print_helper(self, node, indent, depth=0):\n",
    "        if depth == 5:\n",
    "            return\n",
    "        for key, child in node.children.items():\n",
    "            print(\" \" * indent, child)\n",
    "            self._print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "\n",
    "    def _delete_leaf(self, node):\n",
    "        for k, v in node.parent.children.items():\n",
    "            if v == node:\n",
    "                break\n",
    "        del node.parent.children[k]\n",
    "        self.evictable_size_ -= len(k)\n",
    "\n",
    "    def _total_size_helper(self, node):\n",
    "        x = len(node.value)\n",
    "        for child in node.children.values():\n",
    "            x += self._total_size_helper(child)\n",
    "        return x\n",
    "\n",
    "    def _collect_leaves(self):\n",
    "        ret_list = []\n",
    "\n",
    "        def dfs_(cur_node):\n",
    "            if len(cur_node.children) == 0:\n",
    "                ret_list.append(cur_node)\n",
    "\n",
    "            for x in cur_node.children.values():\n",
    "                dfs_(x)\n",
    "\n",
    "        dfs_(self.root_node)\n",
    "        return ret_list\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.benchmark_workload_gen import WorkloadPrefixDataLoader, ToolBenchDataLoader, LooGLEDataset, LoadDistribution\n",
    "\n",
    "random_workload = WorkloadPrefixDataLoader(\n",
    "        num_patterns=200,\n",
    "        total_num_requests=400,\n",
    "        tokenizer=tokenizer,\n",
    "        load_dist = LoadDistribution.EVEN,\n",
    "        distribution_of_non_shared = 0.2,\n",
    "        output_len=16,\n",
    "        num_in_context_examples = 3,\n",
    "        random_workload_path=\"benchmarks/datasets/ShareGPT_V3_unfiltered_cleaned_split.json\"\n",
    ")\n",
    "requests = random_workload.generate_workload(k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbench_workload = ToolBenchDataLoader(\n",
    "    num_patterns=200,\n",
    "    total_num_requests=400,\n",
    "    tokenizer=tokenizer,\n",
    "    load_dist = LoadDistribution.EVEN,\n",
    "    data_path=\"benchmarks/datasets/G1_workload_updated_input_output_lengths_4096.json\",\n",
    ")\n",
    "toolbench_requets = toolbench_workload.generate_workload(k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mem_cost = [0, 0]\n",
    "num_gpus = 2\n",
    "gpu_allocations = defaultdict(set)\n",
    "\n",
    "def get_recomp_cost(node: LPTreeNode, gpu_id):\n",
    "    if not node or gpu_id in gpu_allocations[node]:\n",
    "        return 0\n",
    "    else:\n",
    "        return node.num_tokens + get_recomp_cost(node.parent, gpu_id)\n",
    "\n",
    "def update_gpu_selections_of_parent(node: LPTreeNode, gpu_id):\n",
    "    if not node:\n",
    "        return\n",
    "    node.gpu_selections.add(gpu_id)\n",
    "    update_gpu_selections_of_parent(node.parent, gpu_id)\n",
    "def handle_split_nodes(split_nodes, gpu_allocations):\n",
    "    for k, v in split_nodes.items():\n",
    "        gpu_allocations[k] = gpu_allocations[v].copy()\n",
    "\n",
    "cache = LPRadixCache()\n",
    "for request in requests[:64]:\n",
    "    split_nodes = {}\n",
    "    leaf_node = cache.insert(tuple(request[\"input_ids\"]), split_nodes=split_nodes)\n",
    "    handle_split_nodes(split_nodes, gpu_allocations)\n",
    "    recom_costs = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        recomputation_cost = get_recomp_cost(leaf_node, gpu_id)\n",
    "        recom_costs.append(recomputation_cost)\n",
    "    gpu_selected = np.argmin([recom_costs[gpu_id] + mem_cost[gpu_id] for gpu_id in range(num_gpus)])\n",
    "    mem_cost[gpu_selected] += recom_costs[gpu_selected]\n",
    "    update_gpu_selections_of_parent(leaf_node, gpu_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = []\n",
    "for req in toolbench_requets:\n",
    "    reqs.append(req[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_helper(node, indent, depth=0):\n",
    "    for key, child in node.children.items():\n",
    "        print(\" \" * indent, tokenizer.decode(child.value)[:20], child.gpu_selections, len(child.value))\n",
    "        _print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "_print_helper(cache.root_node, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mem_cost = [0, 0]\n",
    "num_gpus = 2\n",
    "gpu_allocations = defaultdict(set)\n",
    "\n",
    "def get_recomp_cost(node: LPTreeNode, gpu_id):\n",
    "    if not node or gpu_id in gpu_allocations[node]:\n",
    "        return 0\n",
    "    else:\n",
    "        return node.num_tokens + get_recomp_cost(node.parent, gpu_id)\n",
    "\n",
    "def update_gpu_selections_of_parent(node: LPTreeNode, gpu_id):\n",
    "    if not node:\n",
    "        return\n",
    "    node.gpu_selections = node.gpu_selections.union(gpu_id)\n",
    "    update_gpu_selections_of_parent(node.parent, gpu_id)\n",
    "def handle_split_nodes(split_nodes, gpu_allocations):\n",
    "    for k, v in split_nodes.items():\n",
    "        gpu_allocations[k] = gpu_allocations[v].copy()\n",
    "\n",
    "def get_parent_gpu_selections(node: LPTreeNode):\n",
    "    if not node:\n",
    "        return set()\n",
    "    if node.gpu_selections:\n",
    "        return node.gpu_selections\n",
    "    return get_parent_gpu_selections(node.parent)\n",
    "\n",
    "cache = LPRadixCache()\n",
    "for request in toolbench_requets[:64]:\n",
    "    split_nodes = {}\n",
    "    leaf_node = cache.insert(tuple(request[\"input_ids\"]), split_nodes=split_nodes)\n",
    "    handle_split_nodes(split_nodes, gpu_allocations)\n",
    "    print(leaf_node.num_tokens, leaf_node.context_length)\n",
    "    \n",
    "    gpu_selected:set\n",
    "    if leaf_node.num_tokens < leaf_node.context_length - leaf_node.num_tokens:\n",
    "        gpu_selected = get_parent_gpu_selections(leaf_node)\n",
    "        for gpu in gpu_selected:\n",
    "            mem_cost[gpu] += get_recomp_cost(leaf_node, gpu)\n",
    "    else:\n",
    "        recom_costs = []\n",
    "        for gpu_id in range(num_gpus):\n",
    "            recomputation_cost = get_recomp_cost(leaf_node, gpu_id)\n",
    "            recom_costs.append(recomputation_cost)\n",
    "        gpu_selected = np.argmin([recom_costs[gpu_id] + mem_cost[gpu_id] for gpu_id in range(num_gpus)])\n",
    "        mem_cost[gpu_selected] += recom_costs[gpu_selected]\n",
    "        gpu_selected = set([gpu_selected])\n",
    "    update_gpu_selections_of_parent(leaf_node, gpu_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks.benchmark_workload_gen import LooGLEDataset, LooGLEDatasetType\n",
    "dataloader = LooGLEDataset(\n",
    "    num_patterns=24,\n",
    "    total_num_requests=250,\n",
    "    tokenizer=tokenizer,\n",
    "    loogle_dataset_type=LooGLEDatasetType.SHORT_QA\n",
    ")\n",
    "requests = dataloader.generate_workload(max_length=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mem_cost = [0, 0]\n",
    "num_gpus = 2\n",
    "gpu_allocations = defaultdict(set)\n",
    "\n",
    "def get_recomp_cost(node: LPTreeNode, gpu_id):\n",
    "    if not node or gpu_id in gpu_allocations[node]:\n",
    "        return 0\n",
    "    else:\n",
    "        return node.num_tokens + get_recomp_cost(node.parent, gpu_id)\n",
    "\n",
    "def update_gpu_selections_of_parent(node: LPTreeNode, gpu_id):\n",
    "    if not node:\n",
    "        return\n",
    "    node.gpu_selections = node.gpu_selections.union(gpu_id)\n",
    "    update_gpu_selections_of_parent(node.parent, gpu_id)\n",
    "def handle_split_nodes(split_nodes, gpu_allocations):\n",
    "    for k, v in split_nodes.items():\n",
    "        gpu_allocations[k] = gpu_allocations[v].copy()\n",
    "\n",
    "def get_parent_gpu_selections(node: LPTreeNode):\n",
    "    if not node:\n",
    "        return set()\n",
    "    if node.gpu_selections:\n",
    "        return node.gpu_selections\n",
    "    return get_parent_gpu_selections(node.parent)\n",
    "\n",
    "cache = LPRadixCache()\n",
    "for request in toolbench_requets[:64]:\n",
    "    split_nodes = {}\n",
    "    leaf_node = cache.insert(tuple(request[\"input_ids\"]), split_nodes=split_nodes)\n",
    "    handle_split_nodes(split_nodes, gpu_allocations)\n",
    "    print(leaf_node.num_tokens, leaf_node.context_length)\n",
    "    \n",
    "    gpu_selected:set\n",
    "    if leaf_node.num_tokens < leaf_node.context_length - leaf_node.num_tokens:\n",
    "        gpu_selected = get_parent_gpu_selections(leaf_node)\n",
    "        for gpu in gpu_selected:\n",
    "            mem_cost[gpu] += get_recomp_cost(leaf_node, gpu)\n",
    "    else:\n",
    "        recom_costs = []\n",
    "        for gpu_id in range(num_gpus):\n",
    "            recomputation_cost = get_recomp_cost(leaf_node, gpu_id)\n",
    "            recom_costs.append(recomputation_cost)\n",
    "        gpu_selected = np.argmin([recom_costs[gpu_id] + mem_cost[gpu_id] for gpu_id in range(num_gpus)])\n",
    "        mem_cost[gpu_selected] += recom_costs[gpu_selected]\n",
    "        gpu_selected = set([gpu_selected])\n",
    "    update_gpu_selections_of_parent(leaf_node, gpu_selected)\n",
    "_print_helper(cache.root_node, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
