{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import GRB\n",
    "import gurobipy as gp\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from uuid import uuid4\n",
    "import copy\n",
    "import random\n",
    "import threading\n",
    "from enum import Enum, auto\n",
    "import logging\n",
    "from benchmarks.benchmark_utils import RequestFuncOutput\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "logging = logging.getLogger(__name__)\n",
    "DEBUG_COUNTER = 0\n",
    "class LpNode:\n",
    "    def __init__(self, node_id, num_gpus):\n",
    "        self.node_id = node_id\n",
    "        self.variables = [\n",
    "            None for _ in range(num_gpus)\n",
    "        ]  # Will be initialized as binary variables in the model\n",
    "        self.children_token_cost_at_max_depth = 0  # Issue is that depth_limit will cut off the tokens for children and that will treat it as free\n",
    "        self.randomly_selected_gpu = None\n",
    "        self.load_variables = [None for _ in range(num_gpus)]\n",
    "        self.common_load = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        variable_values = [var.x if var else None for var in self.variables]\n",
    "        load_variable_values = [var.x if var else None for var in self.load_variables]\n",
    "        common_load = self.common_load.x if self.common_load else None\n",
    "        # ignore printing laod variables if None\n",
    "        if any(load_variable_values):\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values}, load_variables={load_variable_values}, common_load={common_load})\"\n",
    "        else:\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values})\"\n",
    "\n",
    "\n",
    "class LPTreeNode:\n",
    "    def __init__(self):\n",
    "        self.id = uuid4()\n",
    "        self.children = defaultdict(LPTreeNode)\n",
    "        self.parent: Optional[LPTreeNode] = None\n",
    "        self.value = None\n",
    "        self.ref_counter = 0\n",
    "        self.last_access_time = time.time()\n",
    "        self.gpu_selections = set()\n",
    "        self.is_leaf = False\n",
    "        self.decode_length = 0\n",
    "        self.context_length = 0\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.value)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.last_access_time < other.last_access_time\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, LPTreeNode):\n",
    "            return self.id == other.id  # Compare nodes based on their unique ID\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)  # Use the unique ID for hashing\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"LPTreeNode(id={self.id}, ref_counter={self.ref_counter}, gpu_selections={self.gpu_selections})\"\n",
    "\n",
    "\n",
    "def match(key, seq):\n",
    "    i = 0\n",
    "    for k, w in zip(key, seq):\n",
    "        if k != w:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "class LPRadixCache:\n",
    "    def __init__(self, disable=False):\n",
    "        self.reset()\n",
    "        self.disable = disable\n",
    "\n",
    "    ##### Public API #####\n",
    "\n",
    "    def reset(self):\n",
    "        self.root_node = LPTreeNode()\n",
    "        self.root_node.value = []\n",
    "        self.root_node.ref_counter = 1\n",
    "        self.evictable_size_ = 0\n",
    "\n",
    "    def find_node(self, key):\n",
    "        if self.disable:\n",
    "            return None\n",
    "        current_gpu_selection, node = self.match_prefix_get_gpu_selection(key)\n",
    "        return node\n",
    "\n",
    "    def match_prefix_get_gpu_selection(self, key, path_to_node=[]):\n",
    "        if self.disable:\n",
    "            return [], self.root_node\n",
    "\n",
    "        value = []\n",
    "        current_gpu_selection = self.root_node.gpu_selections\n",
    "        current_gpu_selection, node = self._match_prefix_helper_gpu_selection(\n",
    "            self.root_node, key, value, current_gpu_selection\n",
    "        )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def _match_prefix_helper_gpu_selection(\n",
    "        self, node, key, value, current_gpu_selection\n",
    "    ):\n",
    "        child: LPTreeNode\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len != 0:\n",
    "                if child.gpu_selections:\n",
    "                    current_gpu_selection = child.gpu_selections\n",
    "                if prefix_len < len(c_key):\n",
    "                    print(prefix_len, len(c_key))\n",
    "                    assert False\n",
    "                    new_node = self._split_node(\n",
    "                        c_key, child, prefix_len, new_nodes_created=new_nodes_created\n",
    "                    )\n",
    "                    value.append(new_node.value)\n",
    "                    # last_node[0] = new_node\n",
    "                else:\n",
    "                    value.append(child.value)\n",
    "                    # last_node[0] = child\n",
    "                    return self._match_prefix_helper_gpu_selection(\n",
    "                        child, key[prefix_len:], value, current_gpu_selection\n",
    "                    )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def match_prefix_return_str(self, key):\n",
    "        return \"\".join(self.match_prefix(key)[0])\n",
    "\n",
    "    def insert(\n",
    "        self,\n",
    "        key,\n",
    "        value=None,\n",
    "        node_map=None,\n",
    "        all_modified_nodes=None,\n",
    "        split_nodes=None,\n",
    "        depth_limit=0,\n",
    "    ):\n",
    "        if node_map is None:\n",
    "            node_map = {}\n",
    "        if all_modified_nodes is None:\n",
    "            all_modified_nodes = set()\n",
    "        if split_nodes is None:\n",
    "            split_nodes = {}  # key -> node\n",
    "        if self.disable:\n",
    "            return len(key)\n",
    "\n",
    "        if value is None:\n",
    "            value = [x for x in key]\n",
    "        modified_nodes = set()\n",
    "        created_node = self._insert_helper(\n",
    "            self.root_node,\n",
    "            key,\n",
    "            value,\n",
    "            node_map=node_map,\n",
    "            modified_nodes=modified_nodes,\n",
    "            depth_limit=depth_limit,\n",
    "            current_depth=0,\n",
    "            split_nodes=split_nodes,\n",
    "        )\n",
    "\n",
    "        node: LPTreeNode = created_node\n",
    "        while node is not None:\n",
    "            if node in all_modified_nodes:\n",
    "                break\n",
    "            all_modified_nodes.add(node)\n",
    "            node = node.parent\n",
    "        return created_node\n",
    "\n",
    "    def pretty_print(self):\n",
    "        self._print_helper(self.root_node, 0)\n",
    "        print(f\"#tokens: {self.total_size()}\")\n",
    "\n",
    "    def total_size(self):\n",
    "        return self._total_size_helper(self.root_node)\n",
    "\n",
    "    def evict(self, num_tokens, evict_callback):\n",
    "        if self.disable:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        leaves = self._collect_leaves()\n",
    "        heapq.heapify(leaves)\n",
    "\n",
    "        num_evicted = 0\n",
    "        while num_evicted < num_tokens and len(leaves):\n",
    "            x = heapq.heappop(leaves)\n",
    "\n",
    "            if x == self.root_node:\n",
    "                break\n",
    "            if x.ref_counter > 0:\n",
    "                continue\n",
    "\n",
    "            num_evicted += evict_callback(x)\n",
    "            self._delete_leaf(x)\n",
    "\n",
    "            if len(x.parent.children) == 0:\n",
    "                heapq.heappush(leaves, x.parent)\n",
    "\n",
    "    def inc_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 0:\n",
    "                self.evictable_size_ -= len(node.value)\n",
    "                delta -= len(node.value)\n",
    "            node.ref_counter += 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def dec_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            # if node.ref_counter == 1: TODO why does this exist?\n",
    "            #     self.evictable_size_ += len(node.value)\n",
    "            #     delta += len(node.value)\n",
    "            node.ref_counter -= 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def remove_completed_input_ids(self, input_ids):\n",
    "        node = self.find_node(input_ids)\n",
    "        self.dec_ref_counter(node)  # remove reference counter up to parent\n",
    "    \n",
    "    def evictable_size(self):\n",
    "        return self.evictable_size_\n",
    "\n",
    "    def _split_node(\n",
    "        self, key, child: LPTreeNode, split_len, node_map, depth_limit, current_depth\n",
    "    ):\n",
    "        # new_node -> child\n",
    "        new_node = LPTreeNode()\n",
    "        new_node.gpu_selections = copy.deepcopy(child.gpu_selections)\n",
    "        new_node.children = {key[split_len:]: child}\n",
    "        new_node.parent = child.parent\n",
    "        new_node.ref_counter = child.ref_counter\n",
    "        new_node.context_length = child.parent.context_length + split_len\n",
    "\n",
    "        new_node.value = child.value[:split_len]\n",
    "        child.parent = new_node\n",
    "        child.value = child.value[split_len:]\n",
    "\n",
    "        new_node.parent.children[key[:split_len]] = new_node\n",
    "        del new_node.parent.children[key]\n",
    "        return new_node\n",
    "\n",
    "    def _insert_helper(\n",
    "        self,\n",
    "        node: LPTreeNode,\n",
    "        key,\n",
    "        value,\n",
    "        node_map,\n",
    "        modified_nodes,\n",
    "        depth_limit,\n",
    "        current_depth,\n",
    "        split_nodes,\n",
    "        parent_context_length = 0\n",
    "    ):\n",
    "        node.last_access_time = time.time()\n",
    "        node.ref_counter += 1\n",
    "\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len == len(c_key):\n",
    "                if prefix_len == len(key):\n",
    "                    child.ref_counter += 1\n",
    "                    modified_nodes.add(child)\n",
    "                    return child\n",
    "                else:\n",
    "                    key = key[prefix_len:]\n",
    "                    value = value[prefix_len:]\n",
    "                    return self._insert_helper(\n",
    "                        child,\n",
    "                        key,\n",
    "                        value,\n",
    "                        node_map=node_map,\n",
    "                        modified_nodes=modified_nodes,\n",
    "                        depth_limit=depth_limit,\n",
    "                        current_depth=current_depth + 1,\n",
    "                        split_nodes=split_nodes,\n",
    "                        parent_context_length=parent_context_length + prefix_len,\n",
    "                    )\n",
    "\n",
    "            if prefix_len:\n",
    "                new_node = self._split_node(\n",
    "                    c_key,\n",
    "                    child,\n",
    "                    prefix_len,\n",
    "                    node_map,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                )\n",
    "                # modified_nodes.add(new_node)\n",
    "                # modified_nodes.add(child)\n",
    "                # TODO check if this makes sense to ignore this?\n",
    "                # if child in node_map and current_depth < depth_limit:\n",
    "                split_nodes[child] = new_node\n",
    "                return self._insert_helper(\n",
    "                    new_node,\n",
    "                    key[prefix_len:],\n",
    "                    value[prefix_len:],\n",
    "                    node_map=node_map,\n",
    "                    modified_nodes=modified_nodes,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                    split_nodes=split_nodes,\n",
    "                    parent_context_length=parent_context_length + prefix_len,\n",
    "                )\n",
    "\n",
    "        if len(key):\n",
    "            new_node = LPTreeNode()\n",
    "            new_node.gpu_selections = set()\n",
    "            new_node.parent = node\n",
    "            new_node.value = value\n",
    "            new_node.ref_counter = 1\n",
    "            new_node.context_length = parent_context_length + len(key)\n",
    "\n",
    "            node.children[key] = new_node\n",
    "            self.evictable_size_ += len(value)\n",
    "            # if current_depth < depth_limit:\n",
    "            modified_nodes.add(new_node)\n",
    "            # return new_node\n",
    "            return new_node\n",
    "        return node\n",
    "\n",
    "    def _print_helper(self, node, indent, depth=0):\n",
    "        if depth == 5:\n",
    "            return\n",
    "        for key, child in node.children.items():\n",
    "            print(\" \" * indent, child)\n",
    "            self._print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "\n",
    "    def _delete_leaf(self, node):\n",
    "        for k, v in node.parent.children.items():\n",
    "            if v == node:\n",
    "                break\n",
    "        del node.parent.children[k]\n",
    "        self.evictable_size_ -= len(k)\n",
    "\n",
    "    def _total_size_helper(self, node):\n",
    "        x = len(node.value)\n",
    "        for child in node.children.values():\n",
    "            x += self._total_size_helper(child)\n",
    "        return x\n",
    "\n",
    "    def _collect_leaves(self):\n",
    "        ret_list = []\n",
    "\n",
    "        def dfs_(cur_node):\n",
    "            if len(cur_node.children) == 0:\n",
    "                ret_list.append(cur_node)\n",
    "\n",
    "            for x in cur_node.children.values():\n",
    "                dfs_(x)\n",
    "\n",
    "        dfs_(self.root_node)\n",
    "        return ret_list\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from benchmarks.benchmark_workload_gen import WorkloadPrefixDataLoader, ToolBenchDataLoader, LooGLEDataset, LoadDistribution\n",
    "\n",
    "random_workload = WorkloadPrefixDataLoader(\n",
    "        num_patterns=200,\n",
    "        total_num_requests=400,\n",
    "        tokenizer=tokenizer,\n",
    "        load_dist = LoadDistribution.EVEN,\n",
    "        distribution_of_non_shared = 0.2,\n",
    "        output_len=16,\n",
    "        num_in_context_examples = 3,\n",
    "        random_workload_path=\"benchmarks/datasets/ShareGPT_V3_unfiltered_cleaned_split.json\"\n",
    ")\n",
    "requests = random_workload.generate_workload(k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbench_workload = ToolBenchDataLoader(\n",
    "    num_patterns=200,\n",
    "    total_num_requests=400,\n",
    "    tokenizer=tokenizer,\n",
    "    load_dist = LoadDistribution.EVEN,\n",
    "    data_path=\"benchmarks/datasets/G1_workload_updated_input_output_lengths_4096.json\",\n",
    ")\n",
    "toolbench_requets = toolbench_workload.generate_workload(k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "mem_cost = [0, 0]\n",
    "num_gpus = 2\n",
    "gpu_allocations = defaultdict(set)\n",
    "\n",
    "def get_recomp_cost(node: LPTreeNode, gpu_id):\n",
    "    if not node or gpu_id in gpu_allocations[node]:\n",
    "        return 0\n",
    "    else:\n",
    "        return node.num_tokens + get_recomp_cost(node.parent, gpu_id)\n",
    "\n",
    "def update_gpu_selections_of_parent(node: LPTreeNode, gpu_id):\n",
    "    if not node:\n",
    "        return\n",
    "    node.gpu_selections.add(gpu_id)\n",
    "    update_gpu_selections_of_parent(node.parent, gpu_id)\n",
    "def handle_split_nodes(split_nodes, gpu_allocations):\n",
    "    for k, v in split_nodes.items():\n",
    "        gpu_allocations[k] = gpu_allocations[v].copy()\n",
    "\n",
    "cache = LPRadixCache()\n",
    "for request in requests[:64]:\n",
    "    split_nodes = {}\n",
    "    leaf_node = cache.insert(tuple(request[\"input_ids\"]), split_nodes=split_nodes)\n",
    "    handle_split_nodes(split_nodes, gpu_allocations)\n",
    "    recom_costs = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        recomputation_cost = get_recomp_cost(leaf_node, gpu_id)\n",
    "        recom_costs.append(recomputation_cost)\n",
    "    gpu_selected = np.argmin([recom_costs[gpu_id] + mem_cost[gpu_id] for gpu_id in range(num_gpus)])\n",
    "    mem_cost[gpu_selected] += recom_costs[gpu_selected]\n",
    "    update_gpu_selections_of_parent(leaf_node, gpu_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = []\n",
    "for req in toolbench_requets:\n",
    "    reqs.append(req[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <s> System: You are  {0, 1} 358\n",
      "   icehockeyapi: IceHoc {0} 2216\n",
      "   fluximmo: API de flu {1} 2661\n",
      "   kenyan_news_api: Non {0} 809\n",
      "   valorant_weapons: Pr {0} 752\n",
      "   zodiacapi: Simple Zo {1} 622\n",
      "   instagram_api_2023:  {0} 2585\n",
      "   trackmania: Get Stat {1} 2189\n",
      "   complete_study_bible {1} 2505\n",
      "   world_population_by_ {1} 1346\n",
      "   temp_email: The Temp {1} 570\n",
      "   dcps_project: Gets a {1} 666\n",
      "   reqres: Reqres\n",
      "\n",
      "Spec {1} 456\n",
      "   opensea_v2: Opensea  {0} 2109\n",
      "   evosis_s_game_databa {1} 968\n",
      "   get_58_provinces_of_ {0} 510\n",
      "   spotify_web: Spotify {1} 1651\n",
      "     need to compile a li {1} 474\n",
      "     want to explore the  {1} 668\n",
      "   dimondevosint: It is {0} 1146\n",
      "   financial_statements {0} 1668\n",
      "   horoscopes_ai: Horos {0} 1275\n",
      "   api_video: api.video {0} 2493\n",
      "   tiktok_user: Get pro {1} 514\n",
      "   auth: OAuth2 Authori {0} 953\n",
      "     'm building an appli {0} 526\n",
      "     need to retrieve the {0} 270\n",
      "   oil_thai_price: Pric {1} 1220\n",
      "   rushtranslate: Human {1} 1588\n",
      "   iys_skill_api: With  {0} 1894\n",
      "   lorem_ipsum_api: Gen {1} 844\n",
      "   covid_19_india: COVI {1} 580\n",
      "   fast_email_verifier: {0} 987\n",
      "   corona_virus_world_a {1} 1057\n",
      "   words_of_wisdom_the_ {0} 1049\n",
      "   tank01_nfl_live_in_g {1} 2418\n",
      "   h {0, 1} 1\n",
      "     api_books: HAPI Book {1} 3409\n",
      "     ryvna_today: Exchang {0} 1042\n",
      "   flowers: Information {0} 718\n",
      "   youtube_ {0, 1} 2\n",
      "     video_stream_downloa {0} 1495\n",
      "     search: Introducing  {1} 2588\n",
      "   dreambet: Games and  {0} 3403\n",
      "   consumer_reports: Th {1} 1940\n",
      "   minecraft_forge_opti {0} 1346\n",
      "   numberstoletters: Co {1} 623\n",
      "     want to convert 2500 {1} 37\n",
      "     'm helping my family {1} 64\n",
      "   vat_validation_and_t {0} 1825\n",
      "   transaction: Get tra {1} 879\n",
      "   us_counties: Detaile {1} 1580\n",
      "   ocr: This API proces {0} 771\n",
      "   live_world_futbol_ne {0} 622\n",
      "   bing_web_search: Bin {1} 452\n",
      "     \n",
      "Thought: \n",
      "Action: s {1} 441\n",
      "   mobile_ {0} 2\n",
      "     phone_specs_database {0} 2431\n",
      "     phones: An API that  {0} 1179\n",
      "   investing_financial_ {1} 3151\n",
      "   as {0} 1\n",
      "     n_details: Get detai {0} 879\n",
      "     pose_imaging_cloud:  {0} 3095\n",
      "   opt_nc_public_docker {1} 1016\n",
      "   check_username: Gene {1} 1419\n",
      "   wosti_futbol_tv_spai {1} 926\n",
      "     run a sports blog an {1} 58\n",
      "     'm organizing a foot {1} 72\n",
      "   visual_crossing_weat {0} 2091\n",
      "   groupdocs_ {0} 3\n",
      "     metadata_cloud: Grou {0} 1442\n",
      "     signature_cloud: Gro {0} 1499\n",
      "   judge0_ce: The most  {1} 708\n",
      "   car_data: Use this A {0} 1802\n"
     ]
    }
   ],
   "source": [
    "def _print_helper(node, indent, depth=0):\n",
    "    for key, child in node.children.items():\n",
    "        print(\" \" * indent, tokenizer.decode(child.value)[:20], child.gpu_selections, len(child.value))\n",
    "        _print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "_print_helper(cache.root_node, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2574 2574\n",
      "984 1342\n",
      "2661 3019\n",
      "880 1238\n",
      "809 1167\n",
      "660 1018\n",
      "752 1110\n",
      "622 980\n",
      "2585 2943\n",
      "2189 2547\n",
      "2505 2863\n",
      "1445 1803\n",
      "1479 1837\n",
      "1346 1704\n",
      "2433 2791\n",
      "570 928\n",
      "666 1024\n",
      "456 814\n",
      "1497 1855\n",
      "2125 2483\n",
      "2109 2467\n",
      "968 1326\n",
      "510 868\n",
      "668 2677\n",
      "1146 1504\n",
      "1668 2026\n",
      "3410 3768\n",
      "1275 1633\n",
      "2493 2851\n",
      "514 872\n",
      "270 1581\n",
      "1220 1578\n",
      "1588 1946\n",
      "1894 2252\n",
      "844 1202\n",
      "580 938\n",
      "987 1345\n",
      "1057 1415\n",
      "1049 1407\n",
      "2418 2776\n",
      "1042 1401\n",
      "718 1076\n",
      "2588 2948\n",
      "3403 3761\n",
      "1940 2298\n",
      "1346 1704\n",
      "64 1045\n",
      "1825 2183\n",
      "879 1237\n",
      "1580 1938\n",
      "771 1129\n",
      "622 980\n",
      "452 810\n",
      "1179 1539\n",
      "3151 3509\n",
      "3095 3454\n",
      "1016 1374\n",
      "1419 1777\n",
      "72 1356\n",
      "2091 2449\n",
      "1499 1860\n",
      "441 1251\n",
      "708 1066\n",
      "1802 2160\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "mem_cost = [0, 0]\n",
    "num_gpus = 2\n",
    "gpu_allocations = defaultdict(set)\n",
    "\n",
    "def get_recomp_cost(node: LPTreeNode, gpu_id):\n",
    "    if not node or gpu_id in gpu_allocations[node]:\n",
    "        return 0\n",
    "    else:\n",
    "        return node.num_tokens + get_recomp_cost(node.parent, gpu_id)\n",
    "\n",
    "def update_gpu_selections_of_parent(node: LPTreeNode, gpu_id):\n",
    "    if not node:\n",
    "        return\n",
    "    node.gpu_selections = node.gpu_selections.union(gpu_id)\n",
    "    update_gpu_selections_of_parent(node.parent, gpu_id)\n",
    "def handle_split_nodes(split_nodes, gpu_allocations):\n",
    "    for k, v in split_nodes.items():\n",
    "        gpu_allocations[k] = gpu_allocations[v].copy()\n",
    "\n",
    "def get_parent_gpu_selections(node: LPTreeNode):\n",
    "    if not node:\n",
    "        return set()\n",
    "    if node.gpu_selections:\n",
    "        return node.gpu_selections\n",
    "    return get_parent_gpu_selections(node.parent)\n",
    "\n",
    "cache = LPRadixCache()\n",
    "for request in toolbench_requets[:64]:\n",
    "    split_nodes = {}\n",
    "    leaf_node = cache.insert(tuple(request[\"input_ids\"]), split_nodes=split_nodes)\n",
    "    handle_split_nodes(split_nodes, gpu_allocations)\n",
    "    print(leaf_node.num_tokens, leaf_node.context_length)\n",
    "    \n",
    "    gpu_selected:set\n",
    "    if leaf_node.num_tokens < leaf_node.context_length - leaf_node.num_tokens:\n",
    "        gpu_selected = get_parent_gpu_selections(leaf_node)\n",
    "        for gpu in gpu_selected:\n",
    "            mem_cost[gpu] += get_recomp_cost(leaf_node, gpu)\n",
    "    else:\n",
    "        recom_costs = []\n",
    "        for gpu_id in range(num_gpus):\n",
    "            recomputation_cost = get_recomp_cost(leaf_node, gpu_id)\n",
    "            recom_costs.append(recomputation_cost)\n",
    "        gpu_selected = np.argmin([recom_costs[gpu_id] + mem_cost[gpu_id] for gpu_id in range(num_gpus)])\n",
    "        mem_cost[gpu_selected] += recom_costs[gpu_selected]\n",
    "        gpu_selected = set([gpu_selected])\n",
    "    update_gpu_selections_of_parent(leaf_node, gpu_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/ssd/sglang_env/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for bigainlco/LooGLE contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigainlco/LooGLE\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from benchmarks.benchmark_workload_gen import LooGLEDataset, LooGLEDatasetType\n",
    "dataloader = LooGLEDataset(\n",
    "    num_patterns=24,\n",
    "    total_num_requests=250,\n",
    "    tokenizer=tokenizer,\n",
    "    loogle_dataset_type=LooGLEDatasetType.SHORT_QA\n",
    ")\n",
    "requests = dataloader.generate_workload(max_length=32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <s> System: You are  {0, 1} 358\n",
      "   icehockeyapi: IceHoc {0} 2216\n",
      "   fluximmo: API de flu {1} 2661\n",
      "   kenyan_news_api: Non {0} 809\n",
      "   valorant_weapons: Pr {0} 752\n",
      "   zodiacapi: Simple Zo {1} 622\n",
      "   instagram_api_2023:  {0} 2585\n",
      "   trackmania: Get Stat {1} 2189\n",
      "   complete_study_bible {1} 2505\n",
      "   world_population_by_ {1} 1346\n",
      "   temp_email: The Temp {1} 570\n",
      "   dcps_project: Gets a {1} 666\n",
      "   reqres: Reqres\n",
      "\n",
      "Spec {1} 456\n",
      "   opensea_v2: Opensea  {0} 2109\n",
      "   evosis_s_game_databa {1} 968\n",
      "   get_58_provinces_of_ {0} 510\n",
      "   spotify_web: Spotify {1} 1651\n",
      "     need to compile a li {1} 474\n",
      "     want to explore the  {1} 668\n",
      "   dimondevosint: It is {0} 1146\n",
      "   financial_statements {0} 1668\n",
      "   horoscopes_ai: Horos {0} 1275\n",
      "   api_video: api.video {0} 2493\n",
      "   tiktok_user: Get pro {1} 514\n",
      "   auth: OAuth2 Authori {0} 953\n",
      "     'm building an appli {0} 526\n",
      "     need to retrieve the {0} 270\n",
      "   oil_thai_price: Pric {1} 1220\n",
      "   rushtranslate: Human {1} 1588\n",
      "   iys_skill_api: With  {0} 1894\n",
      "   lorem_ipsum_api: Gen {1} 844\n",
      "   covid_19_india: COVI {1} 580\n",
      "   fast_email_verifier: {0} 987\n",
      "   corona_virus_world_a {1} 1057\n",
      "   words_of_wisdom_the_ {0} 1049\n",
      "   tank01_nfl_live_in_g {1} 2418\n",
      "   h {0, 1} 1\n",
      "     api_books: HAPI Book {1} 3409\n",
      "     ryvna_today: Exchang {0} 1042\n",
      "   flowers: Information {0} 718\n",
      "   youtube_ {0, 1} 2\n",
      "     video_stream_downloa {0} 1495\n",
      "     search: Introducing  {1} 2588\n",
      "   dreambet: Games and  {0} 3403\n",
      "   consumer_reports: Th {1} 1940\n",
      "   minecraft_forge_opti {0} 1346\n",
      "   numberstoletters: Co {1} 623\n",
      "     want to convert 2500 {1} 37\n",
      "     'm helping my family {1} 64\n",
      "   vat_validation_and_t {0} 1825\n",
      "   transaction: Get tra {1} 879\n",
      "   us_counties: Detaile {1} 1580\n",
      "   ocr: This API proces {0} 771\n",
      "   live_world_futbol_ne {0} 622\n",
      "   bing_web_search: Bin {1} 452\n",
      "     \n",
      "Thought: \n",
      "Action: s {1} 441\n",
      "   mobile_ {0} 2\n",
      "     phone_specs_database {0} 2431\n",
      "     phones: An API that  {0} 1179\n",
      "   investing_financial_ {1} 3151\n",
      "   as {0} 1\n",
      "     n_details: Get detai {0} 879\n",
      "     pose_imaging_cloud:  {0} 3095\n",
      "   opt_nc_public_docker {1} 1016\n",
      "   check_username: Gene {1} 1419\n",
      "   wosti_futbol_tv_spai {1} 926\n",
      "     run a sports blog an {1} 58\n",
      "     'm organizing a foot {1} 72\n",
      "   visual_crossing_weat {0} 2091\n",
      "   groupdocs_ {0} 3\n",
      "     metadata_cloud: Grou {0} 1442\n",
      "     signature_cloud: Gro {0} 1499\n",
      "   judge0_ce: The most  {1} 708\n",
      "   car_data: Use this A {0} 1802\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "mem_cost = [0, 0]\n",
    "num_gpus = 2\n",
    "gpu_allocations = defaultdict(set)\n",
    "\n",
    "def get_recomp_cost(node: LPTreeNode, gpu_id):\n",
    "    if not node or gpu_id in gpu_allocations[node]:\n",
    "        return 0\n",
    "    else:\n",
    "        return node.num_tokens + get_recomp_cost(node.parent, gpu_id)\n",
    "\n",
    "def update_gpu_selections_of_parent(node: LPTreeNode, gpu_id):\n",
    "    if not node:\n",
    "        return\n",
    "    node.gpu_selections = node.gpu_selections.union(gpu_id)\n",
    "    update_gpu_selections_of_parent(node.parent, gpu_id)\n",
    "def handle_split_nodes(split_nodes, gpu_allocations):\n",
    "    for k, v in split_nodes.items():\n",
    "        gpu_allocations[k] = gpu_allocations[v].copy()\n",
    "\n",
    "def get_parent_gpu_selections(node: LPTreeNode):\n",
    "    if not node:\n",
    "        return set()\n",
    "    if node.gpu_selections:\n",
    "        return node.gpu_selections\n",
    "    return get_parent_gpu_selections(node.parent)\n",
    "\n",
    "cache = LPRadixCache()\n",
    "for request in toolbench_requets[:64]:\n",
    "    split_nodes = {}\n",
    "    leaf_node = cache.insert(tuple(request[\"input_ids\"]), split_nodes=split_nodes)\n",
    "    handle_split_nodes(split_nodes, gpu_allocations)\n",
    "    print(leaf_node.num_tokens, leaf_node.context_length)\n",
    "    \n",
    "    gpu_selected:set\n",
    "    if leaf_node.num_tokens < leaf_node.context_length - leaf_node.num_tokens:\n",
    "        gpu_selected = get_parent_gpu_selections(leaf_node)\n",
    "        for gpu in gpu_selected:\n",
    "            mem_cost[gpu] += get_recomp_cost(leaf_node, gpu)\n",
    "    else:\n",
    "        recom_costs = []\n",
    "        for gpu_id in range(num_gpus):\n",
    "            recomputation_cost = get_recomp_cost(leaf_node, gpu_id)\n",
    "            recom_costs.append(recomputation_cost)\n",
    "        gpu_selected = np.argmin([recom_costs[gpu_id] + mem_cost[gpu_id] for gpu_id in range(num_gpus)])\n",
    "        mem_cost[gpu_selected] += recom_costs[gpu_selected]\n",
    "        gpu_selected = set([gpu_selected])\n",
    "    update_gpu_selections_of_parent(leaf_node, gpu_selected)\n",
    "_print_helper(cache.root_node, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
