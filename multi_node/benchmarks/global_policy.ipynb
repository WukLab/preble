{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# Add the parent directory of the 'src' directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\".\"), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from benchmark_workload_gen import ToolBenchDataLoader, LoadDistribution\n",
    "from sglang.srt.managers.router.radix_cache import RadixCache\n",
    "\n",
    "num_workloads = 100\n",
    "num_requests = 4096\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "dataloader = ToolBenchDataLoader('G1_workload_updated_input_output_lengths_4096_cropped_to_50.json', num_workloads, num_requests, tokenizer, LoadDistribution.EVEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "workload = dataloader.generate_workload(k=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('surebets', 'manga_scrapper')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def get_tool(workload_item):\n",
    "    text = workload_item[\"text\"]\n",
    "    match = re.search(r\"You have access of the following tools:\\n1.(.+?): \", text)\n",
    "    if match:\n",
    "        tool = match.group(1)\n",
    "        return tool\n",
    "get_tool(workload[0]), get_tool(workload[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5 Hello r=3\n",
      "   6  There r=1\n",
      "   6 _L.A.! r=1\n",
      "#tokens: 17\n",
      "Hello T\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(TreeNode)\n",
    "        self.parent = None\n",
    "        self.value = None\n",
    "        self.ref_counter = 0\n",
    "        self.last_access_time = time.time()\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.value)\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.last_access_time < other.last_access_time\n",
    "\n",
    "\n",
    "def match(key, seq):\n",
    "    i = 0\n",
    "    for k, w in zip(key, seq):\n",
    "        if k != w:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "class RadixCache:\n",
    "    def __init__(self, disable=False):\n",
    "        self.reset()\n",
    "        self.disable = disable\n",
    "\n",
    "    ##### Public API #####\n",
    "\n",
    "    def reset(self):\n",
    "        self.root_node = TreeNode()\n",
    "        self.root_node.value = []\n",
    "        self.root_node.ref_counter = 1\n",
    "        self.evictable_size_ = 0\n",
    "\n",
    "    def match_prefix(self, key):\n",
    "        if self.disable:\n",
    "            return [], self.root_node\n",
    "\n",
    "        value = []\n",
    "        last_node = [self.root_node]\n",
    "        self._match_prefix_helper(self.root_node, key, value, last_node)\n",
    "        if value:\n",
    "            if isinstance(value[0], torch.Tensor):\n",
    "                value = torch.concat(value)\n",
    "            else:\n",
    "                concatenated_value = []\n",
    "                for v in value:\n",
    "                    concatenated_value.extend(v)  # Assuming each element in value is a list itself\n",
    "                value = concatenated_value\n",
    "        return value, last_node[0]\n",
    "\n",
    "    def match_prefix_return_str(self, key):\n",
    "        return \"\".join(self.match_prefix(key)[0])\n",
    "\n",
    "    def insert(self, key, value=None):\n",
    "        if self.disable:\n",
    "            return len(key)\n",
    "\n",
    "        if value is None:\n",
    "            value = [x for x in key]\n",
    "        return self._insert_helper(self.root_node, key, value)\n",
    "\n",
    "    def pretty_print(self):\n",
    "        self._print_helper(self.root_node, 0)\n",
    "        print(f\"#tokens: {self.total_size()}\")\n",
    "\n",
    "    def total_size(self):\n",
    "        return self._total_size_helper(self.root_node)\n",
    "\n",
    "    def evict(self, num_tokens, evict_callback):\n",
    "        if self.disable:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        leaves = self._collect_leaves()\n",
    "        heapq.heapify(leaves)\n",
    "\n",
    "        num_evicted = 0\n",
    "        while num_evicted < num_tokens and len(leaves):\n",
    "            x = heapq.heappop(leaves)\n",
    "\n",
    "            if x == self.root_node:\n",
    "                break\n",
    "            if x.ref_counter > 0:\n",
    "                continue\n",
    "\n",
    "            num_evicted += evict_callback(x.value)\n",
    "            self._delete_leaf(x)\n",
    "\n",
    "            if len(x.parent.children) == 0:\n",
    "                heapq.heappush(leaves, x.parent)\n",
    "\n",
    "    def inc_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 0:\n",
    "                self.evictable_size_ -= len(node.value)\n",
    "                delta -= len(node.value)\n",
    "            node.ref_counter += 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def dec_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 1:\n",
    "                self.evictable_size_ += len(node.value)\n",
    "                delta += len(node.value)\n",
    "            node.ref_counter -= 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def evictable_size(self):\n",
    "        return self.evictable_size_\n",
    "\n",
    "    ##### Internal Helper Functions #####\n",
    "    def _match_prefix_helper(self, node, key, value, last_node):\n",
    "        node.last_access_time = time.time()\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len != 0:\n",
    "                if prefix_len < len(c_key):\n",
    "                    new_node = self._split_node(c_key, child, prefix_len)\n",
    "                    value.append(new_node.value)\n",
    "                    last_node[0] = new_node\n",
    "                else:\n",
    "                    value.append(child.value)\n",
    "                    last_node[0] = child\n",
    "                    self._match_prefix_helper(child, key[prefix_len:], value, last_node)\n",
    "                break\n",
    "\n",
    "    def _split_node(self, key, child, split_len):\n",
    "        # new_node -> child\n",
    "        new_node = TreeNode()\n",
    "        new_node.children = {key[split_len:]: child}\n",
    "        new_node.parent = child.parent\n",
    "        new_node.ref_counter = child.ref_counter\n",
    "        new_node.value = child.value[:split_len]\n",
    "        child.parent = new_node\n",
    "        child.value = child.value[split_len:]\n",
    "        new_node.parent.children[key[:split_len]] = new_node\n",
    "        del new_node.parent.children[key]\n",
    "        return new_node\n",
    "\n",
    "    def _insert_helper(self, node, key, value):\n",
    "        node.last_access_time = time.time()\n",
    "        node.ref_counter += 1\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "\n",
    "            if prefix_len == len(c_key):\n",
    "                if prefix_len == len(key):\n",
    "                    child.ref_counter += 1\n",
    "                    return prefix_len\n",
    "                else:\n",
    "                    key = key[prefix_len:]\n",
    "                    value = value[prefix_len:]\n",
    "                    return prefix_len + self._insert_helper(child, key, value)\n",
    "\n",
    "            if prefix_len:\n",
    "                new_node = self._split_node(c_key, child, prefix_len)\n",
    "                return prefix_len + self._insert_helper(\n",
    "                    new_node, key[prefix_len:], value[prefix_len:]\n",
    "                )\n",
    "\n",
    "        if len(key):\n",
    "            new_node = TreeNode()\n",
    "            new_node.parent = node\n",
    "            new_node.value = value\n",
    "            new_node.ref_counter = 1\n",
    "            node.children[key] = new_node\n",
    "            self.evictable_size_ += len(value)\n",
    "        return 0\n",
    "\n",
    "    def _print_helper(self, node, indent, depth=0):\n",
    "        if depth == 5:\n",
    "            return\n",
    "        for key, child in node.children.items():\n",
    "            print(\" \" * indent, len(key), key[:10], f\"r={child.ref_counter}\")\n",
    "            self._print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "        \n",
    "\n",
    "    def _delete_leaf(self, node):\n",
    "        for k, v in node.parent.children.items():\n",
    "            if v == node:\n",
    "                break\n",
    "        del node.parent.children[k]\n",
    "        self.evictable_size_ -= len(k)\n",
    "\n",
    "    def _total_size_helper(self, node):\n",
    "        x = len(node.value)\n",
    "        for child in node.children.values():\n",
    "            x += self._total_size_helper(child)\n",
    "        return x\n",
    "\n",
    "    def _collect_leaves(self):\n",
    "        ret_list = []\n",
    "\n",
    "        def dfs_(cur_node):\n",
    "            if len(cur_node.children) == 0:\n",
    "                ret_list.append(cur_node)\n",
    "\n",
    "            for x in cur_node.children.values():\n",
    "                dfs_(x)\n",
    "\n",
    "        dfs_(self.root_node)\n",
    "        return ret_list\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tree = RadixCache(disable=False)\n",
    "\n",
    "    tree.insert(\"Hello\")\n",
    "    tree.insert(\"Hello There\")\n",
    "    tree.insert(\"Hello_L.A.!\")\n",
    "    # tree.insert(\"Hello_world! Happy\")\n",
    "    # tree.insert(\"I love you!\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "    print(tree.match_prefix_return_str(\"Hello T\"))\n",
    "\n",
    "    # def evict_callback(x):\n",
    "    #    print(\"evict\", x)\n",
    "    #    return len(x)\n",
    "\n",
    "    # tree.evict(5, evict_callback)\n",
    "    # tree.evict(10, evict_callback)\n",
    "    # tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 (1,) r=4\n",
      "   357 (2135, 28747, 995, 460, 12191, 28777, 6316, 28725, 368, 541) r=3\n",
      "     443 (17989, 28726, 1468, 28747, 7086, 354, 12875, 28726, 1468, 297) r=1\n",
      "     3250 (28719, 14836, 28730, 824, 6131, 28747, 2483, 4686, 532, 266) r=1\n",
      "     1141 (17064, 28730, 28713, 2729, 28730, 350, 3673, 28747, 451, 1036) r=1\n",
      "   8 (287, 1036, 28730, 14908, 28730, 20228, 28746, 4365) r=1\n",
      "#tokens: 5200\n"
     ]
    }
   ],
   "source": [
    "cache = RadixCache()\n",
    "for item in workload[:3]:\n",
    "    cache.insert(tuple(item[\"input_ids\"][\"input_ids\"]))\n",
    "# for i in range(100):\n",
    "#     cache.insert(tuple(workload[0][\"input_ids\"][\"input_ids\"]))\n",
    "# for j in range(100):\n",
    "#     cache.insert(tuple(workload[1][\"input_ids\"][\"input_ids\"]))\n",
    "cache.insert(tuple(tokenizer.encode(\"bdd_special_tokens=True\")))\n",
    "cache.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "total_tokens_available = [33077, 33077, 33077]\n",
    "nodes = {0: 0, 1: 0, 2:0}\n",
    "\n",
    "def cost_fn(nodes, total_tokens):\n",
    "    # how close are the loads of the two nodes\n",
    "    # how many tokens are being used\n",
    "    total_load = sum(nodes)\n",
    "\n",
    "    return {\n",
    "        \"total_load\": total_load,\n",
    "        \"total_tokens\": sum(total_tokens),\n",
    "        \"load_ratio\": np.std(nodes),\n",
    "        \"token_ratio\": np.std(total_tokens)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1 (Prefix: []): GPUs [0, 1, 2]\n",
      "  Node 2 (Prefix: [1]): GPUs [0, 1, 2]\n",
      "    Node 3 (Prefix: [2135, 28747, 995, 460, 12191, 28777, 6316, 28725, 368, 541, 938, 1287, 7040, 28732, 19659]): GPUs [0, 1, 2]\n",
      "      Node 4 (Prefix: [17989, 28726, 1468, 28747, 7086, 354, 12875, 28726, 1468, 297, 8657, 22567, 13, 13, 4947]): GPUs [2]\n",
      "      Node 5 (Prefix: [28719, 14836, 28730, 824, 6131, 28747, 2483, 4686, 532, 266, 732, 19607, 1178, 477, 16020]): GPUs [2]\n",
      "      Node 6 (Prefix: [17064, 28730, 28713, 2729, 28730, 350, 3673, 28747, 451, 1036, 28713, 1178, 7086, 354, 17868]): GPUs [2]\n",
      "    Node 7 (Prefix: [287, 1036, 28730, 14908, 28730, 20228, 28746, 4365]): GPUs [2]\n",
      "Combo based\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "class LPTreeTraversal:\n",
    "    def __init__(self, num_gpus):\n",
    "        self.num_gpus = num_gpus\n",
    "        self.num_gpus_param = cp.Parameter(nonneg=True, value=num_gpus)\n",
    "        self.counter = 0\n",
    "        self.constraints = []\n",
    "        self.node_map = {}  # Maps PrefixTreeNode to LpNode\n",
    "\n",
    "        self.time_per_token_fixed_cost = cp.Parameter(nonneg=True, value=4.59)\n",
    "        self.time_per_token_variable_cost = cp.Parameter(nonneg=True, value=0.1)\n",
    "        self.load_cost_per_token = cp.Parameter(nonneg=True, value=0.1)\n",
    "\n",
    "        self.num_tokens_params = {}\n",
    "        self.ref_count_params = {}\n",
    "\n",
    "    def _traverse_tree(self, current_prefix_node, parent_lp_node=None):\n",
    "        self.counter += 1\n",
    "        current_lp_node = LpNode(self.counter, self.num_gpus)\n",
    "        self.node_map[current_prefix_node] = current_lp_node\n",
    "\n",
    "        self.constraints += [cp.sum(current_lp_node.variables) >= 1] # At least one GPU must be allocated for a prefix\n",
    "        if parent_lp_node:\n",
    "            # Add constraints based on the relationship between the current node and its parent\n",
    "            # If the child takes a node, then the parent must also take a node\n",
    "            for gpu in range(self.num_gpus):\n",
    "                self.constraints.append(current_lp_node.variables[gpu] <= parent_lp_node.variables[gpu])\n",
    "\n",
    "        for child_prefix_node in current_prefix_node.children.values():\n",
    "            self._traverse_tree(child_prefix_node, current_lp_node)\n",
    "\n",
    "    def solve(self, objective):\n",
    "        # Create problem and solve\n",
    "        problem = cp.Problem(objective, self.constraints)\n",
    "        problem.solve()\n",
    "\n",
    "    def traverse_and_optimize(self, prefix_tree_root):\n",
    "        self._traverse_tree(prefix_tree_root)\n",
    "\n",
    "        memory_cost_terms = []\n",
    "        load_cost_params = []\n",
    "        for prefix_node, lp_node in self.node_map.items():\n",
    "            num_tokens_param = self.num_tokens_params.get(prefix_node, cp.Parameter(nonneg=True))\n",
    "            num_tokens_param.value = prefix_node.num_tokens  # Update parameter value\n",
    "            \n",
    "            load_param = self.ref_count_params.get(prefix_node, cp.Parameter(nonneg=True))\n",
    "            load_param.value = prefix_node.ref_counter  # Update parameter value\n",
    "\n",
    "            # TODO use existing matrix to calculate the difference of new tokens\n",
    "            memory_cost_terms += [num_tokens_param * gpu_var * self.time_per_token_variable_cost + self.time_per_token_fixed_cost * gpu_var for gpu_var in lp_node.variables]\n",
    "            # Load cost param \n",
    "            for gpu_var in lp_node.variables:\n",
    "                # Directly penalize the non-selection of a GPU. This represents 1/x and is used to simplify the cost.\n",
    "                penalty = (1 - gpu_var) * self.load_cost_per_token * num_tokens_param * load_param\n",
    "                load_cost_params.append(penalty)\n",
    "\n",
    "        # Define your objective function here, for example:\n",
    "        objective = cp.Minimize(cp.sum(load_cost_params) + cp.sum(memory_cost_terms))  # Adjust according to your specific problem\n",
    "\n",
    "        # Solve the problem\n",
    "        self._solve(objective)\n",
    "\n",
    "    def _solve(self, objective):\n",
    "        problem = cp.Problem(objective, self.constraints)\n",
    "        problem.solve()\n",
    "\n",
    "        # After solving, you can access and print the values of your variables like so:\n",
    "        # for prefix_node, lp_node in self.node_map.items():\n",
    "        #     print(f\"Node {lp_node.node_id}: {[var.value for var in lp_node.variables]}\")\n",
    "        #     print(f\"Node {prefix_node.value[:15]}\")\n",
    "    \n",
    "    def pretty_print(self, prefix_node, indent=\"\"):\n",
    "        lp_node = self.node_map.get(prefix_node)\n",
    "        if lp_node:\n",
    "            # Assuming each lp_node.variables[i].value gives whether GPU i is selected\n",
    "            selected_gpus = [i for i, var in enumerate(lp_node.variables) if var.value >= 0.99]  # Adjust threshold as needed\n",
    "            print(f\"{indent}Node {lp_node.node_id} (Prefix: {prefix_node.value[:15]}): GPUs {selected_gpus}\")\n",
    "        else:\n",
    "            print(f\"{indent}Node (Prefix: {prefix_node.value[:15]}) has no LP Node mapping\")\n",
    "\n",
    "        for child in prefix_node.children.values():\n",
    "            self.pretty_print(child, indent + \"  \")\n",
    "\n",
    "# Assuming a Node class with attributes 'children' and 'node_id'\n",
    "class LpNode:\n",
    "    def __init__(self, node_id, num_gpus):\n",
    "        self.node_id = node_id\n",
    "        self.variables = [cp.Variable(name=f\"node_{node_id}_gpu_{gpu}\", boolean=True) for gpu in range(num_gpus)]\n",
    "\n",
    "\n",
    "lp_tree_traversal = LPTreeTraversal(3)\n",
    "lp_tree_traversal.traverse_and_optimize(cache.root_node)\n",
    "lp_tree_traversal.pretty_print(cache.root_node)\n",
    "print(\"Combo based\")\n",
    "# import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
