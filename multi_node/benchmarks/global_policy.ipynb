{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# Add the parent directory of the 'src' directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\".\"), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from benchmark_workload_gen import ToolBenchDataLoader, LoadDistribution\n",
    "\n",
    "num_workloads = 100\n",
    "num_requests = 4096\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "dataloader = ToolBenchDataLoader('datasets/G1_workload_updated_input_output_lengths_4096_cropped_to_50.json', num_workloads, num_requests, tokenizer, LoadDistribution.EVEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "workload = dataloader.generate_workload(k=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vod_app', 'iotvas')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def get_tool(workload_item):\n",
    "    text = workload_item[\"text\"]\n",
    "    match = re.search(r\"You have access of the following tools:\\n1.(.+?): \", text)\n",
    "    if match:\n",
    "        tool = match.group(1)\n",
    "        return tool\n",
    "get_tool(workload[0]), get_tool(workload[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "from uuid import uuid4  \n",
    "\n",
    "import torch\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.id = uuid4()  \n",
    "        self.children = defaultdict(TreeNode)\n",
    "        self.parent = None\n",
    "        self.value = None\n",
    "        self.ref_counter = 0\n",
    "        self.last_access_time = time.time()\n",
    "        self.gpu_selections = set()\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.value)\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.last_access_time < other.last_access_time\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, TreeNode):\n",
    "            return self.id == other.id  # Compare nodes based on their unique ID\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)  # Use the unique ID for hashing\n",
    "\n",
    "def match(key, seq):\n",
    "    i = 0\n",
    "    for k, w in zip(key, seq):\n",
    "        if k != w:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "class RadixCache:\n",
    "    def __init__(self, disable=False):\n",
    "        self.reset()\n",
    "        self.disable = disable\n",
    "\n",
    "    ##### Public API #####\n",
    "\n",
    "    def reset(self):\n",
    "        self.root_node = TreeNode()\n",
    "        self.root_node.value = []\n",
    "        self.root_node.ref_counter = 1\n",
    "        self.evictable_size_ = 0\n",
    "\n",
    "    def match_prefix_get_gpu_selection(self, key):\n",
    "        if self.disable:\n",
    "            return [], self.root_node\n",
    "\n",
    "        value = []\n",
    "        current_gpu_selection = self.root_node.gpu_selections\n",
    "        current_gpu_selection, node = self._match_prefix_helper_gpu_selection(self.root_node, key, value, current_gpu_selection)\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def _match_prefix_helper_gpu_selection(self, node, key, value, current_gpu_selection):\n",
    "        node.last_access_time = time.time()\n",
    "        child: TreeNode\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len != 0:\n",
    "                text = tokenizer.decode(child.value)\n",
    "                if child.gpu_selections:\n",
    "                    current_gpu_selection = child.gpu_selections\n",
    "                if prefix_len < len(c_key):\n",
    "                    assert False\n",
    "                    new_node = self._split_node(c_key, child, prefix_len, new_nodes_created=new_nodes_created)\n",
    "                    value.append(new_node.value)\n",
    "                    # last_node[0] = new_node\n",
    "                else:\n",
    "                    value.append(child.value)\n",
    "                    # last_node[0] = child\n",
    "                    return self._match_prefix_helper_gpu_selection(child, key[prefix_len:], value, current_gpu_selection)\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def match_prefix_return_str(self, key):\n",
    "        return \"\".join(self.match_prefix(key)[0])\n",
    "\n",
    "    def insert(self, key, value=None, node_map=None, all_modified_nodes=None, depth_limit=0):\n",
    "        if node_map is None:\n",
    "            node_map = {}\n",
    "        if self.disable:\n",
    "            return len(key)\n",
    "\n",
    "        if value is None:\n",
    "            value = [x for x in key]\n",
    "        modified_nodes = set()\n",
    "        total_tokens_added = self._insert_helper(\n",
    "            self.root_node,\n",
    "            key, \n",
    "            value, \n",
    "            node_map=node_map, \n",
    "            modified_nodes=modified_nodes, \n",
    "            depth_limit=depth_limit, \n",
    "            current_depth=0\n",
    "        )\n",
    "\n",
    "        node: TreeNode\n",
    "        for node in modified_nodes:\n",
    "            # Add all parents till parent is none to all_modified_nodes\n",
    "            while node is not None:\n",
    "                all_modified_nodes.add(node)\n",
    "                node = node.parent\n",
    "        return total_tokens_added\n",
    "\n",
    "    def pretty_print(self):\n",
    "        self._print_helper(self.root_node, 0)\n",
    "        print(f\"#tokens: {self.total_size()}\")\n",
    "\n",
    "    def total_size(self):\n",
    "        return self._total_size_helper(self.root_node)\n",
    "\n",
    "    def evict(self, num_tokens, evict_callback):\n",
    "        if self.disable:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        leaves = self._collect_leaves()\n",
    "        heapq.heapify(leaves)\n",
    "\n",
    "        num_evicted = 0\n",
    "        while num_evicted < num_tokens and len(leaves):\n",
    "            x = heapq.heappop(leaves)\n",
    "\n",
    "            if x == self.root_node:\n",
    "                break\n",
    "            if x.ref_counter > 0:\n",
    "                continue\n",
    "\n",
    "            num_evicted += evict_callback(x.value)\n",
    "            self._delete_leaf(x)\n",
    "\n",
    "            if len(x.parent.children) == 0:\n",
    "                heapq.heappush(leaves, x.parent)\n",
    "\n",
    "    def inc_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 0:\n",
    "                self.evictable_size_ -= len(node.value)\n",
    "                delta -= len(node.value)\n",
    "            node.ref_counter += 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def dec_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 1:\n",
    "                self.evictable_size_ += len(node.value)\n",
    "                delta += len(node.value)\n",
    "            node.ref_counter -= 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def evictable_size(self):\n",
    "        return self.evictable_size_\n",
    "\n",
    "    def _split_node(self, key, child, split_len, node_map, depth_limit, current_depth):\n",
    "        # new_node -> child\n",
    "        is_child_in_node_map = child in node_map\n",
    "        if is_child_in_node_map:\n",
    "            lp_node = node_map[child]\n",
    "\n",
    "        new_node = TreeNode()\n",
    "        new_node.gpu_selections = copy.deepcopy(child.gpu_selections)\n",
    "        new_node.children = {\n",
    "            key[split_len:]: child\n",
    "        }\n",
    "        new_node.parent = child.parent\n",
    "        new_node.ref_counter = child.ref_counter\n",
    "\n",
    "        new_node.value = child.value[:split_len]\n",
    "        child.parent = new_node\n",
    "        child.value = child.value[split_len:]\n",
    "\n",
    "        new_node.parent.children[key[:split_len]] = new_node\n",
    "        del new_node.parent.children[key]\n",
    "\n",
    "        if is_child_in_node_map and current_depth < depth_limit:\n",
    "            node_map[child] = lp_node\n",
    "            node_map[new_node] = lp_node\n",
    "\n",
    "        return new_node\n",
    "\n",
    "    def _insert_helper(self, node, key, value, node_map, modified_nodes, depth_limit, current_depth):\n",
    "        node.last_access_time = time.time()\n",
    "        node.ref_counter += 1\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "\n",
    "            if prefix_len == len(c_key):\n",
    "                if prefix_len == len(key):\n",
    "                    child.ref_counter += 1\n",
    "                    return prefix_len\n",
    "                else:\n",
    "                    key = key[prefix_len:]\n",
    "                    value = value[prefix_len:]\n",
    "                    return prefix_len + self._insert_helper(child, key, value, node_map=node_map, modified_nodes=modified_nodes, depth_limit=depth_limit, current_depth=current_depth + 1)\n",
    "\n",
    "            if prefix_len:\n",
    "                new_node = self._split_node(c_key, child, prefix_len, node_map, depth_limit=depth_limit, current_depth=current_depth + 1)\n",
    "                modified_nodes.add(new_node)\n",
    "                modified_nodes.add(child)\n",
    "                return prefix_len + self._insert_helper(\n",
    "                    new_node, key[prefix_len:], value[prefix_len:], node_map=node_map, modified_nodes=modified_nodes, depth_limit=depth_limit, current_depth=current_depth + 1\n",
    "                )\n",
    "\n",
    "        if len(key):\n",
    "            new_node = TreeNode()\n",
    "            new_node.gpu_selections = copy.deepcopy(node.gpu_selections)\n",
    "            new_node.parent = node\n",
    "            new_node.value = value\n",
    "            new_node.ref_counter = 1\n",
    "            node.children[key] = new_node\n",
    "            self.evictable_size_ += len(value)\n",
    "            if current_depth < depth_limit:\n",
    "                modified_nodes.add(new_node)\n",
    "        return 0\n",
    "\n",
    "    def _print_helper(self, node, indent, depth=0):\n",
    "        if depth == 5:\n",
    "            return\n",
    "        for key, child in node.children.items():\n",
    "            print(\" \" * indent, len(key), key[:10], f\"r={child.ref_counter}\")\n",
    "            self._print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "        \n",
    "\n",
    "    def _delete_leaf(self, node):\n",
    "        for k, v in node.parent.children.items():\n",
    "            if v == node:\n",
    "                break\n",
    "        del node.parent.children[k]\n",
    "        self.evictable_size_ -= len(k)\n",
    "\n",
    "    def _total_size_helper(self, node):\n",
    "        x = len(node.value)\n",
    "        for child in node.children.values():\n",
    "            x += self._total_size_helper(child)\n",
    "        return x\n",
    "\n",
    "    def _collect_leaves(self):\n",
    "        ret_list = []\n",
    "\n",
    "        def dfs_(cur_node):\n",
    "            if len(cur_node.children) == 0:\n",
    "                ret_list.append(cur_node)\n",
    "\n",
    "            for x in cur_node.children.values():\n",
    "                dfs_(x)\n",
    "\n",
    "        dfs_(self.root_node)\n",
    "        return ret_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mip import Model, xsum, BINARY, MINIMIZE, OptimizationStatus, minimize, INTEGER, GUROBI\n",
    "import random\n",
    "\n",
    "class LpNode:\n",
    "    def __init__(self, node_id, num_gpus):\n",
    "        self.node_id = node_id\n",
    "        self.variables = [None] * num_gpus  # Will be initialized as binary variables in the model\n",
    "        self.children_token_cost_at_max_depth = 0 # Issue is that depth_limit will cut off the tokens for children and that will treat it as free\n",
    "        self.randomly_selected_gpu = None\n",
    "\n",
    "class LPTreeTraversal:\n",
    "    def __init__(self, num_gpus):\n",
    "        self.num_gpus = num_gpus\n",
    "        self.node_map = {}  # Maps PrefixTreeNode to LpNode\n",
    "        self.depth_limit = 5\n",
    "        self.model = Model(sense=MINIMIZE, solver_name=GUROBI)\n",
    "        self.model.verbose = 1\n",
    "        self.counter = 0\n",
    "        self.total_cost_var = None\n",
    "        self.per_gpu_constraints = []\n",
    "        self.dynamic_constraints = []  # To keep track of dynamic constraints\n",
    "        self.iteration_counter = 0 \n",
    "\n",
    "    def _traverse_tree(self, current_prefix_node: TreeNode, parent_lp_node=None, depth=0, modified_nodes=None):\n",
    "        if modified_nodes is not None and current_prefix_node not in modified_nodes:\n",
    "            print(f\"SKIPPING\", current_prefix_node.id, \"Not in modified nodes\", modified_nodes)\n",
    "            return # Skip nodes that have not been modified\n",
    "\n",
    "        if depth == self.depth_limit:\n",
    "            assert parent_lp_node is not None\n",
    "            parent_lp_node.children_token_cost_at_max_depth = self._calculate_children_token_cost(current_prefix_node)\n",
    "            return\n",
    "        self.counter += 1\n",
    "        current_lp_node = LpNode(current_prefix_node.id, self.num_gpus)\n",
    "        self.node_map[current_prefix_node] = current_lp_node\n",
    "        # Initialize binary variables for the LP node\n",
    "        for gpu in range(self.num_gpus):\n",
    "            current_lp_node.variables[gpu] = self.model.add_var(f\"node_{self.counter}_{gpu}\",var_type=BINARY)\n",
    "\n",
    "        # At least one GPU must be allocated for a prefix\n",
    "        self.model += xsum(current_lp_node.variables) >= 1\n",
    "\n",
    "        if parent_lp_node:\n",
    "            # If the child takes a node, then the parent must also take a node\n",
    "            for gpu in range(self.num_gpus):\n",
    "                self.model += current_lp_node.variables[gpu] <= parent_lp_node.variables[gpu]\n",
    "\n",
    "        for child_prefix_node in current_prefix_node.children.values():\n",
    "            self._traverse_tree(current_prefix_node=child_prefix_node, parent_lp_node=current_lp_node, depth=depth + 1, modified_nodes=modified_nodes)\n",
    "\n",
    "    def _calculate_children_token_cost(self, node):\n",
    "        \"\"\"\n",
    "        Recursively calculate the total number of tokens for all children of a given node,\n",
    "        effectively aggregating the tokens for nodes that are beyond the depth limit.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        total_tokens = node.num_tokens\n",
    "        for child in node.children.values():\n",
    "            total_tokens += self._calculate_children_token_cost(child)\n",
    "        return total_tokens\n",
    "\n",
    "    def add_parent_child_gpu_constraints(self, modified_nodes=None):\n",
    "        for parent_prefix_node, parent_lp_node in self.node_map.items():\n",
    "            if modified_nodes is not None and parent_prefix_node not in modified_nodes:\n",
    "                continue\n",
    "            if not parent_prefix_node.children:  # Skip leaf nodes\n",
    "                continue\n",
    "            for gpu_index in range(self.num_gpus):\n",
    "                children_gpu_selections = []\n",
    "                for child_prefix_node in parent_prefix_node.children.values():\n",
    "                    if child_prefix_node in self.node_map:\n",
    "                        child_lp_node = self.node_map[child_prefix_node]\n",
    "                        children_gpu_selections.append(child_lp_node.variables[gpu_index])\n",
    "                if children_gpu_selections:\n",
    "                    children_selections_total = xsum(children_gpu_selections)\n",
    "                    self.model += parent_lp_node.variables[gpu_index] <= children_selections_total\n",
    "    \n",
    "    def traverse_and_optimize(self, prefix_tree_root, existing_cost={}, modified_nodes=None, objective_only=False):\n",
    "        if modified_nodes is None:\n",
    "            modified_nodes = set()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # self.model.reset()  # Re-initialize the model for a new optimization problem\n",
    "        # self.model = Model(sense=MINIMIZE, solver_name=GUROBI)\n",
    "\n",
    "        self.model.verbose = 1\n",
    "        # if self.iteration_counter % 100 == 0:\n",
    "        self.model = Model(sense=MINIMIZE, solver_name=GUROBI)\n",
    "        modified_nodes = None\n",
    "        self.model.verbose = 1\n",
    "        self.iteration_counter += 1\n",
    "\n",
    "        self._traverse_tree(prefix_tree_root, modified_nodes=modified_nodes)  # Set up variables and base constraints\n",
    "        self.add_parent_child_gpu_constraints(modified_nodes=modified_nodes)  # Add parent-child constraints\n",
    "\n",
    "        # Objective components: Let's assume we're trying to minimize the total cost adjusted for existing costs\n",
    "        total_cost = []\n",
    "        per_gpu_mem_cost = [[] for _ in range(self.num_gpus)]\n",
    "        per_gpu_load_cost = [[] for _ in range(self.num_gpus)]\n",
    "        initial_solution = []\n",
    "        load_costs = []\n",
    "        for prefix_node, lp_node in self.node_map.items():\n",
    "            node_costs = existing_cost.get(prefix_node, {})\n",
    "            # children token cost is to account for depth cutoff\n",
    "            # depth 3: tool 5, examples\n",
    "            num_tokens_total = prefix_node.num_tokens + lp_node.children_token_cost_at_max_depth\n",
    "            for gpu_index, var in enumerate(lp_node.variables):\n",
    "                previous_gpu_selected = existing_cost.get(prefix_node, {}).get(gpu_index, 0) \n",
    "                if previous_gpu_selected:\n",
    "                    initial_solution.append((var, previous_gpu_selected))\n",
    "                mistral_tokens_to_prefill_time = lambda x: 0.148 * x + 22.7\n",
    "                num_tokens_time = mistral_tokens_to_prefill_time(num_tokens_total)\n",
    "                \n",
    "                decode_length  = 16 # for now assume decoding occurs for 20 tokens\n",
    "                decoding_time = lambda x: 6.7 * x\n",
    "                total_cost.append(var * num_tokens_time - var * previous_gpu_selected * num_tokens_time)\n",
    "                per_gpu_mem_cost[gpu_index].append(var * num_tokens_time)\n",
    "            load_cost = prefix_node.ref_counter * decoding_time(num_tokens_total) * decode_length * (- xsum(lp_node.variables)) / self.num_gpus\n",
    "            per_gpu_load_cost[gpu_index].append(load_cost)\n",
    "\n",
    "        self.max_per_gpu_cost = self.model.add_var(var_type=INTEGER)\n",
    "        self.total_gpu_load_cost = [xsum(load_cost) for load_cost in per_gpu_load_cost]\n",
    "        for mem_cost in per_gpu_mem_cost:\n",
    "            current_gpu_cost = xsum(mem_cost)\n",
    "            constraint = self.model.add_constr(current_gpu_cost <= self.max_per_gpu_cost)\n",
    "\n",
    "        # self.per_gpu_constraints = per_gpu_constraints\n",
    "        self.model.start = initial_solution\n",
    "        self.model.threads = -1\n",
    "        # self.model.max_mip_gap = 0.02\n",
    "        self.model.max_seconds = 0.2\n",
    "        setup_time = time.time() - start_time\n",
    "        solving_time = time.time()\n",
    "\n",
    "        self.debug_total_cost_var = self.model.add_var(var_type=INTEGER)\n",
    "        self.model += self.debug_total_cost_var >= xsum(total_cost)\n",
    "\n",
    "        self.debug_gpu_load_cost = self.model.add_var(var_type=INTEGER, lb=-10000)\n",
    "        self.model += self.debug_gpu_load_cost >= xsum(self.total_gpu_load_cost)\n",
    "        \n",
    "\n",
    "        self.model.objective = minimize(self.debug_total_cost_var + self.debug_gpu_load_cost)\n",
    "\n",
    "        status = self.model.optimize()\n",
    "\n",
    "        # print(f\"max cost per gpu {self.max_per_gpu_cost.x} {self.total_cost_var.x}\")\n",
    "        tokens_per_gpu, load_to_gpu = self.calculate_tokens_per_gpu()\n",
    "        print(f\"Tokens per GPU: {tokens_per_gpu} {load_to_gpu}\")\n",
    "        print(f\"Objective value: {self.model.objective_value}\")\n",
    "        print(f\"Total GPU Cost: {self.debug_gpu_load_cost.x} Memory Cost {self.debug_total_cost_var.x}\")\n",
    "\n",
    "        if status == OptimizationStatus.OPTIMAL:\n",
    "            pass\n",
    "            # print('Optimal solution found.')\n",
    "        elif status == OptimizationStatus.FEASIBLE:\n",
    "            print('Feasible solution found, but not necessarily optimal.')\n",
    "        else:\n",
    "            print('No feasible solution found.')\n",
    "        print(f\"Solving time: {time.time() - solving_time}s Setup Time {setup_time}s Tota\")\n",
    "\n",
    "    def get_exisiting_cost(self):\n",
    "        existing_cost = {}\n",
    "        lp_node: LpNode\n",
    "        for prefix_node, lp_node in self.node_map.items():\n",
    "            for gpu_id, var in enumerate(lp_node.variables):\n",
    "                if prefix_node not in existing_cost:\n",
    "                    existing_cost[prefix_node] = {}\n",
    "                solver_selection = var and var.x >= 0.99\n",
    "                random_selection = lp_node.randomly_selected_gpu and lp_node.randomly_selected_gpu == gpu_id\n",
    "                if solver_selection or random_selection:\n",
    "                    existing_cost[prefix_node][gpu_id] = 1\n",
    "                else:\n",
    "                    existing_cost[prefix_node][gpu_id] = 0\n",
    "        return existing_cost\n",
    "\n",
    "    def calculate_tokens_per_gpu(self):\n",
    "        tokens_per_gpu = {gpu: 0 for gpu in range(self.num_gpus)}  # Reset/initialize\n",
    "        load_to_gpu = {gpu: 0 for gpu in range(self.num_gpus)}\n",
    "        lp_node:LpNode\n",
    "        for prefix_node, lp_node in self.node_map.items():\n",
    "            for i, var in enumerate(lp_node.variables):\n",
    "                solved_var = var.x if var.x >= 0.99 else 0\n",
    "                if solved_var:  # If GPU i is selected by this node, using .x for variable value in MIP\n",
    "                    tokens_per_gpu[i] += prefix_node.num_tokens  # Accumulate tokens\n",
    "                    load_to_gpu[i] += prefix_node.ref_counter\n",
    "        return tokens_per_gpu, load_to_gpu\n",
    "\n",
    "\n",
    "    def pretty_print(self, prefix_node):\n",
    "        # This method will call pretty_print_helper and then print additional information\n",
    "        # Adjustments are mainly in handling variable values using .x in MIP\n",
    "        self.pretty_print_helper(prefix_node)\n",
    "        # tokens_per_gpu, load_to_gpu = self.calculate_tokens_per_gpu()\n",
    "        # print(f\"Tokens per GPU: {tokens_per_gpu} {load_to_gpu}\")\n",
    "        # print(f\"Objective value: {self.model.objective_value}\")\n",
    "\n",
    "    def pretty_print_helper(self, prefix_node, indent=\"\", depth=0):\n",
    "        if depth == self.depth_limit:\n",
    "            return\n",
    "        lp_node = self.node_map.get(prefix_node)\n",
    "        if lp_node:\n",
    "            if lp_node.randomly_selected_gpu:\n",
    "                selected_gpus = [lp_node.randomly_selected_gpu]\n",
    "            else:\n",
    "                selected_gpus = [i for i, var in enumerate(lp_node.variables) if var and var.x >= 0.99]  # Adjust threshold as needed, using .x for variable value\n",
    "            # if lp_node.node_id == 4 or True:\n",
    "\n",
    "            def get_tool(workload_item):\n",
    "                text = tokenizer.decode(workload_item)\n",
    "                if \":\" in text:\n",
    "                    return text.split(\":\")[0].strip().replace(\"\\n\", \" \")\n",
    "                else:\n",
    "                    return text[:60].strip().replace(\"\\n\", \"\")\n",
    "            print(f\"{indent}Node {lp_node.node_id} (Tokens: {get_tool(prefix_node.value)}, {len(prefix_node.value)}): GPUs {selected_gpus}\")\n",
    "        else:\n",
    "            print(f\"{indent}Node (Prefix: {len(prefix_node.value)}) has no LP Node mapping\")\n",
    "\n",
    "        for child in prefix_node.children.values():\n",
    "            self.pretty_print_helper(child, indent + \"  \", depth=depth + 1)\n",
    "\n",
    "    def update_nodes_with_solution(self, modified_nodes=None):\n",
    "        for prefix_node, lp_node in self.node_map.items():\n",
    "            prefix_node.gpu_selections = set()\n",
    "            for gpu_id, var in enumerate(lp_node.variables):\n",
    "                if var.x >= 0.99:\n",
    "                    prefix_node.gpu_selections.add(gpu_id)\n",
    "\n",
    "class LPScheduler:\n",
    "    def __init__(self, num_nodes: int, depth_limit=4, update_interval=5):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.tree_cache = RadixCache()\n",
    "        self.shadow_cache = RadixCache()\n",
    "        self.lp_tree_traversal = LPTreeTraversal(num_nodes)\n",
    "        self.lp_tree_traversal.depth_limit = depth_limit\n",
    "        self.metrics_dict = []\n",
    "        self.counter = 0\n",
    "        self.update_interval=update_interval\n",
    "        self.load = {\n",
    "\n",
    "        }\n",
    "        self.modified_nodes = set()\n",
    "\n",
    "    def runtime_selector(self, text: str=None, request_id: str=None, input_ids=None, ):\n",
    "        # Tokenize the text\n",
    "        start_time = time.time()\n",
    "\n",
    "        node_map = self.lp_tree_traversal.node_map\n",
    "        self.tree_cache.insert(tuple(input_ids), node_map=node_map, all_modified_nodes=self.modified_nodes, depth_limit=self.lp_tree_traversal.depth_limit)\n",
    "        if self.counter % self.update_interval == 0:\n",
    "            existing_cost = self.lp_tree_traversal.get_exisiting_cost()\n",
    "            self.lp_tree_traversal.traverse_and_optimize(self.tree_cache.root_node, existing_cost=existing_cost, modified_nodes=self.modified_nodes)\n",
    "            self.lp_tree_traversal.update_nodes_with_solution()\n",
    "            self.modified_nodes = set()\n",
    "\n",
    "        self.counter += 1\n",
    "        gpu_selections, node = self.tree_cache.match_prefix_get_gpu_selection(input_ids)\n",
    "        # Randomly select a node from gpu selections\n",
    "        mode = \"not_random\"\n",
    "        if len(gpu_selections) == 0 or len(gpu_selections) == self.num_nodes:\n",
    "            print(\"Random selection\", gpu_selections)\n",
    "            gpu_selections = set(range(self.num_nodes))\n",
    "            mode = \"random\"\n",
    "\n",
    "        runtime_selected = random.choice(list(gpu_selections))\n",
    "        self.load[runtime_selected] = self.load.get(runtime_selected, 0) + 1\n",
    "        self.metrics_dict.append({\n",
    "            \"text\": text,\n",
    "            \"rid\": request_id,\n",
    "            \"selected_runtime\": runtime_selected,\n",
    "            \"overhead\": time.time() - start_time,\n",
    "            \"mode\": mode\n",
    "        })\n",
    "        return runtime_selected\n",
    "\n",
    "# Example usage (you would need to define the structure of PrefixTreeNode and provide a valid prefix_tree_root):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_ipsum = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sit amet purus gravida quis blandit turpis cursus. Sagittis nisl rhoncus mattis rhoncus urna neque viverra justo. Sapien et ligula ullamcorper malesuada proin libero nunc consequat. Sed velit dignissim sodales ut eu sem integer. Nulla aliquet enim tortor at auctor urna nunc id. Nulla aliquet enim tortor at auctor urna nunc id cursus. Tortor at risus viverra adipiscing at in tellus. Consequat nisl vel pretium lectus quam id. Rutrum quisque non tellus orci ac auctor. Natoque penatibus et magnis dis parturient.\n",
    "\n",
    "Nunc sed velit dignissim sodales ut eu sem integer. Sit amet consectetur adipiscing elit pellentesque. Lectus nulla at volutpat diam ut venenatis tellus in. Rhoncus mattis rhoncus urna neque viverra. A lacus vestibulum sed arcu non odio euismod lacinia at. Sem fringilla ut morbi tincidunt augue interdum velit. Massa ultricies mi quis hendrerit dolor magna eget. Mus mauris vitae ultricies leo. Aliquam purus sit amet luctus venenatis lectus magna. Ac feugiat sed lectus vestibulum mattis ullamcorper velit.\n",
    "\n",
    "Fringilla ut morbi tincidunt augue interdum. Tincidunt nunc pulvinar sapien et ligula ullamcorper. Consequat mauris nunc congue nisi vitae suscipit tellus mauris. Erat nam at lectus urna duis convallis convallis. Mauris nunc congue nisi vitae suscipit tellus mauris. Blandit massa enim nec dui nunc mattis enim. Velit dignissim sodales ut eu sem integer vitae. Semper viverra nam libero justo laoreet sit amet cursus sit. Urna condimentum mattis pellentesque id nibh tortor id aliquet. Dui nunc mattis enim ut tellus elementum sagittis. Facilisi nullam vehicula ipsum a arcu cursus vitae congue mauris. Aenean vel elit scelerisque mauris. Amet nulla facilisi morbi tempus iaculis. Nec feugiat nisl pretium fusce id velit ut tortor. Amet justo donec enim diam vulputate ut pharetra. Turpis cursus in hac habitasse platea. At auctor urna nunc id cursus metus.\n",
    "\n",
    "Auctor elit sed vulputate mi sit amet mauris. Rhoncus mattis rhoncus urna neque viverra justo. Cursus euismod quis viverra nibh cras pulvinar mattis nunc sed. Aliquam faucibus purus in massa tempor. Felis eget nunc lobortis mattis aliquam faucibus. Ullamcorper malesuada proin libero nunc consequat interdum varius. Vulputate eu scelerisque felis imperdiet proin fermentum leo vel. Mi ipsum faucibus vitae aliquet nec ullamcorper sit amet risus. Habitasse platea dictumst quisque sagittis purus sit amet. Felis eget velit aliquet sagittis id consectetur purus. Odio eu feugiat pretium nibh ipsum consequat. Laoreet non curabitur gravida arcu ac tortor dignissim. Mi in nulla posuere sollicitudin aliquam ultrices sagittis orci a. Molestie a iaculis at erat. Enim diam vulputate ut pharetra. Eu non diam phasellus vestibulum lorem sed.\n",
    "\n",
    "Scelerisque fermentum dui faucibus in ornare. Ut tellus elementum sagittis vitae et leo. Felis eget nunc lobortis mattis aliquam faucibus purus in massa. Amet tellus cras adipiscing enim eu turpis egestas. Dignissim enim sit amet venenatis urna cursus eget nunc scelerisque. Nisi lacus sed viverra tellus in hac. Nulla malesuada pellentesque elit eget. Purus in massa tempor nec feugiat nisl pretium fusce. Lectus nulla at volutpat diam ut venenatis tellus in. Neque ornare aenean euismod elementum nisi quis eleifend. Enim praesent elementum facilisis leo vel.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workload length: 4000\n",
      "Set parameter Username\n",
      "Request 0\n",
      " 1 (1,) r=20\n",
      "   1231 (330, 27588, 5066, 1768, 22673, 28757, 13, 28758, 5382, 8465) r=10\n",
      "   6 (5066, 1768, 259, 28740, 22673, 28757) r=10\n",
      "     1226 (28705, 13, 28758, 5382, 8465, 1801, 13824, 271, 1943, 837) r=1\n",
      "     1225 (13, 28758, 5382, 8465, 1801, 13824, 271, 1943, 837, 299) r=9\n",
      "#tokens: 3689\n",
      "MODIFIED NODES [UUID('d16c12bc-8c9f-47ae-84a4-4e328b99557c'), UUID('2bc90401-876f-4efb-a64d-34483d8d3dbe'), UUID('b36e0e24-3e89-4273-ba53-b1093a04e2fd'), UUID('e9b12677-0783-4b80-8f37-0b07af225fb7'), UUID('19d23218-26d4-4c75-ab45-c5d2aec33d84'), UUID('cfed5aee-81cf-443e-98dd-b1d75421a870')]\n",
      "Set parameter Username\n",
      "Tokens per GPU: {0: 3689, 1: 0} {0: 71, 1: 0}\n",
      "Objective value: -9317.0\n",
      "Total GPU Cost: -10000.0 Memory Cost 683.0\n",
      "Solving time: 0.002572774887084961s Setup Time 0.00754857063293457s Tota\n",
      "Tokens per GPU: {0: 3689, 1: 0} {0: 71, 1: 0}\n",
      "Node cfed5aee-81cf-443e-98dd-b1d75421a870 (Tokens: , 0): GPUs [0]\n",
      "  Node d16c12bc-8c9f-47ae-84a4-4e328b99557c (Tokens: <s>, 1): GPUs [0]\n",
      "    Node 2bc90401-876f-4efb-a64d-34483d8d3dbe (Tokens: A Different Workload ABCDLorem ipsum dolor sit amet, consec, 1231): GPUs [0]\n",
      "    Node 19d23218-26d4-4c75-ab45-c5d2aec33d84 (Tokens: Workload  1 ABCD, 6): GPUs [0]\n",
      "      Node e9b12677-0783-4b80-8f37-0b07af225fb7 (Tokens: Lorem ipsum dolor sit amet, consectetur adipiscing elit, se, 1226): GPUs [0]\n",
      "      Node b36e0e24-3e89-4273-ba53-b1093a04e2fd (Tokens: Lorem ipsum dolor sit amet, consectetur adipiscing elit, se, 1225): GPUs [0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(f\"Workload length: {len(workload)}\")\n",
    "lp_tree_traversal = LPTreeTraversal(2)\n",
    "lp_tree_traversal.depth_limit = 4\n",
    "runtime_selected = []\n",
    "texts = [\"Workload  1 ABCD \" + lorem_ipsum, \"A Different Workload ABCD\" + lorem_ipsum]\n",
    "for i in range(20):\n",
    "    texts += [\"Workload  1 ABCD\" + lorem_ipsum, \"A Different Workload ABCD\" + lorem_ipsum]\n",
    "# texts = [\"1 sentence. A B C D\", \"3 sentence. A B C D\", \"4 sentencee. A B C D\", \"2 sentence. A B C D\", \"1 sentence. A B C D example 1\", \"1 sentence example 2\", \"2 sentence. A B C D E\"]\n",
    "input_ids = [tokenizer.encode(text) for text in texts]\n",
    "cache = RadixCache()\n",
    "modified_nodes = set()\n",
    "for i in range(len(texts)):\n",
    "    print(f\"Request {i}\")\n",
    "    node_map = lp_tree_traversal.node_map\n",
    "    for i in range(20):\n",
    "        cache.insert(tuple(input_ids[i]), node_map=node_map, all_modified_nodes=modified_nodes, depth_limit=lp_tree_traversal.depth_limit)\n",
    "    cache.pretty_print()\n",
    "    print(\"MODIFIED NODES\", [x.id for x in modified_nodes])\n",
    "    existing_cost = lp_tree_traversal.get_exisiting_cost()\n",
    "    runtime = lp_tree_traversal.traverse_and_optimize(cache.root_node, existing_cost=existing_cost, modified_nodes=modified_nodes)\n",
    "    tokens_per_gpu, load_to_gpu = lp_tree_traversal.calculate_tokens_per_gpu()\n",
    "    print(f\"Tokens per GPU: {tokens_per_gpu} {load_to_gpu}\")\n",
    "    lp_tree_traversal.update_nodes_with_solution()\n",
    "    lp_tree_traversal.pretty_print(cache.root_node)\n",
    "    modified_nodes = set()\n",
    "    break\n",
    "\n",
    "# scheduler = LPScheduler(2)\n",
    "# for i in range(4):\n",
    "#     print(f\"Request {i}\")\n",
    "#     cache.insert(tuple(input_ids[i]))\n",
    "#     runtime_selected = scheduler.runtime_selector(text=texts[i], request_id=i, input_ids=input_ids[i])\n",
    "#     print(texts[i],runtime_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(f\"Workload length: {len(workload)}\")\n",
    "scheduler = LPScheduler(2, depth_limit=3, update_interval=1)\n",
    "runtime_selected = []\n",
    "\n",
    "# input_ids = [tokenizer.encode(text) for text in texts]\n",
    "# for i in range(len(texts)):\n",
    "#     runtime = scheduler.runtime_selector(input_ids=input_ids[i], text=texts[i])\n",
    "#     scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node)\n",
    "#     runtime_selected.append(runtime)\n",
    "for i in workload[:50]:\n",
    "    runtime = scheduler.runtime_selector(input_ids=i[\"input_ids\"], text=i[\"text\"])\n",
    "    # scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node)\n",
    "    runtime_selected.append(runtime)\n",
    "\n",
    "print(pd.DataFrame(scheduler.metrics_dict))\n",
    "# Write metrics dict to json\n",
    "import json\n",
    "with open(\"metrics_lp_scheduler_2_200.json\", \"w\") as f:\n",
    "    json.dump(scheduler.metrics_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Random workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from benchmark_workload_gen import RandomDataLoader\n",
    "\n",
    "num_workloads = 100\n",
    "num_requests = 4096\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "dataloader = RandomDataLoader(4, total_num_requests=num_requests, tokenizer=tokenizer, load_dist=LoadDistribution.EVEN, random_workload_path=\"datasets/ShareGPT_V3_unfiltered_cleaned_split.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload = dataloader.generate_workload(k=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(f\"Workload length: {len(workload)}\")\n",
    "scheduler = LPScheduler(2, depth_limit=3, update_interval=1)\n",
    "runtime_selected = []\n",
    "\n",
    "for i in workload[:200]:\n",
    "    runtime = scheduler.runtime_selector(input_ids=i[\"input_ids\"], text=i[\"text\"])\n",
    "    # scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node)\n",
    "    runtime_selected.append(runtime)\n",
    "\n",
    "print(pd.DataFrame(scheduler.metrics_dict))\n",
    "# Write metrics dict to json\n",
    "import json\n",
    "with open(\"metrics_lp_scheduler_2_200.json\", \"w\") as f:\n",
    "    json.dump(scheduler.metrics_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "df = pd.DataFrame(scheduler.metrics_dict)\n",
    "def get_tool(x):\n",
    "    text = x\n",
    "    match = re.search(r\"You have access of the following tools:\\n1.(.+?): \", text)\n",
    "    if match:\n",
    "        tool = match.group(1)\n",
    "        return tool\n",
    "    return text\n",
    "print(df)\n",
    "df[\"text\"] = df[\"text\"].map(lambda x: get_tool(x))\n",
    "\n",
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def measure_acc(df, num_nodes=4):\n",
    "    # Create a pivot table from the dataframe\n",
    "    pivot_table = df.pivot_table(index='text', columns='selected_runtime', aggfunc='size', fill_value=0)\n",
    "\n",
    "    # Dynamically rename columns based on num_nodes\n",
    "    pivot_table.columns = [f'selected_runtime_{i}' for i in range(num_nodes)]\n",
    "\n",
    "    # Print the pivot table for verification\n",
    "    print(pivot_table)\n",
    "\n",
    "    # Save the pivot table to a CSV file\n",
    "    pivot_table.to_csv(\"pivot_table.csv\")\n",
    "\n",
    "    correct = 0\n",
    "    # Iterate over each row to count 'correct' selections\n",
    "    for index, row in pivot_table.iterrows():\n",
    "        # Print the index for verification\n",
    "        # print(f\"Index: {index}\")\n",
    "        \n",
    "        # Count the number of columns with non-zero values (indicating a selection was made)\n",
    "        num_selected = sum(row[f'selected_runtime_{i}'] != 0 for i in range(num_nodes))\n",
    "        \n",
    "        # If exactly one runtime was selected, consider it 'correct'\n",
    "        if num_selected == 1:\n",
    "            correct += 1\n",
    "\n",
    "    total = len(pivot_table)\n",
    "    accuracy = correct / total if total > 0 else 0  # Compute accuracy, guarding against division by zero\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Correct: {correct} Total: {total}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Assuming 'df' is your DataFrame containing the job scheduling results\n",
    "# measure_acc(df\n",
    "\n",
    "measure_acc(df, num_nodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "# interval = 100\n",
    "depth_limit = 3\n",
    "lp_tree_traversal = LPTreeTraversal(2)\n",
    "lp_tree_traversal.depth_limit = depth_limit\n",
    "cache = RadixCache()\n",
    "random.shuffle(workload)\n",
    "modified_nodes = set()\n",
    "prev_existing_cost = {}\n",
    "for item in workload[:1000]:\n",
    "    cache.insert(tuple(item[\"input_ids\"]), all_modified_nodes=modified_nodes, depth_limit=lp_tree_traversal.depth_limit)\n",
    "lp_tree_traversal.traverse_and_optimize(cache.root_node, existing_cost=lp_tree_traversal.get_exisiting_cost(), modified_nodes=modified_nodes)\n",
    "lp_tree_traversal.update_nodes_with_solution()\n",
    "# lp_tree_traversal.pretty_print(cache.root_node)\n",
    "modified_nodes = set()\n",
    "\n",
    "for i in range(10000):\n",
    "    # for item in workload[i*interval:(i+1)*interval]:\n",
    "    #     cache.insert(tuple(item[\"input_ids\"]), all_modified_nodes=modified_nodes, depth_limit=lp_tree_traversal.depth_limit)\n",
    "    start_time = time.time()\n",
    "    time_after_inserting = time.time()\n",
    "    lp_tree_traversal.traverse_and_optimize(cache.root_node, existing_cost=lp_tree_traversal.get_exisiting_cost(), modified_nodes=modified_nodes, objective_only=True)\n",
    "    # lp_tree_traversal.update_nodes_with_solution()\n",
    "    existing_cost = lp_tree_traversal.get_exisiting_cost()\n",
    "    total_new_nodes_in_existing_cost = len(existing_cost) - len(prev_existing_cost)\n",
    "    prev_existing_cost = existing_cost\n",
    "    times.append({\"num_nodes\": len(lp_tree_traversal.node_map), \"time_after_inserting\": time.time() - time_after_inserting, \"total\": i, \"depth_limit\": depth_limit, \"num_modified\": len(modified_nodes)})\n",
    "    modified_nodes = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lp_tree_traversal.node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame(times)\n",
    "time_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "time_df = pd.DataFrame(times)\n",
    "\n",
    "# Assuming time_df is your DataFrame\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns for subplots\n",
    "\n",
    "# Plot on the first subplot\n",
    "time_df.plot(ax=axs[0], x=\"total\", y=\"time_after_inserting\", title=\"Time taken vs Total\", marker='o', linestyle='-')\n",
    "axs[0].set_xlabel(\"Total\")\n",
    "axs[0].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "# Plot on the second subplot\n",
    "time_df.plot(ax=axs[1], x=\"num_nodes\", y=\"time_after_inserting\", title=\"Time taken vs Number of Nodes\", marker='o', linestyle='-')\n",
    "axs[1].set_xlabel(\"Number of Nodes\")\n",
    "axs[1].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "plt.tight_layout()  # Adjusts subplot params so that subplots are nicely fit in the figure area.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scheduler.metrics_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Load JSON data\n",
    "file_name = 'metrics_lp_scheduler_2_200.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract overhead values\n",
    "overhead_values = [entry['overhead'] for entry in data]\n",
    "max_index = np.argmax(overhead_values)\n",
    "max_value = overhead_values[max_index]\n",
    "print(\"Index with max overhead:\", max_index)\n",
    "print(\"Value with max overhead:\", max_value)\n",
    "\n",
    "std_dev = np.std(overhead_values) / 4  # Reduced for visualization purposes\n",
    "error = [std_dev] * len(overhead_values)\n",
    "# Initial Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(overhead_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Overhead over Time for depth = 3')\n",
    "plt.xlabel('Time (sequential order of metrics)')\n",
    "plt.ylabel('Overhead (seconds)')\n",
    "plt.grid(True)\n",
    "# plt.savefig('initial_overhead_plot.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "# More Smoothed Plot\n",
    "more_smoothed_overhead = savgol_filter(overhead_values, 21, 3)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(range(len(overhead_values)), more_smoothed_overhead, yerr=error, marker='o', linestyle='-', color='g', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "plt.title('More Smoothed Overhead over Time with Error Bars')\n",
    "plt.xlabel('Time (sequential order of metrics)')\n",
    "plt.ylabel('Overhead (seconds)')\n",
    "plt.grid(True)\n",
    "# plt.savefig('more_smoothed_overhead_with_error_bars.png')\n",
    "# plt.close()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Initialize a dictionary to map the first 10 characters to the selected runtime\n",
    "runtime_selection = {}\n",
    "correct_predictions = 0\n",
    "ignore_items = 0\n",
    "import re\n",
    "\n",
    "for entry in data:\n",
    "    text = entry['text']\n",
    "    matched_tool = re.search(r\"You have access of the following tools:\\n1.(.+?): \", text)\n",
    "    if matched_tool:\n",
    "        tool = matched_tool.group(1)\n",
    "    first_10_chars = tool\n",
    "    selected_runtime = entry['selected_runtime']\n",
    "    # If the first 10 characters have been seen before\n",
    "    if first_10_chars in runtime_selection:\n",
    "        # Check if the selected runtime matches the previously recorded runtime\n",
    "        if runtime_selection[first_10_chars] == selected_runtime:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            print(f\"Incorrect prediction for tool: {first_10_chars}\")\n",
    "    else:\n",
    "        # Record the runtime selection for the first 10 characters\n",
    "        runtime_selection[first_10_chars] = selected_runtime\n",
    "        # Assuming the first selection is always correct as there's no precedent\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_percentage = (correct_predictions / (len(data) - ignore_items)) * 100\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_selection = {}\n",
    "first_occurrences = set()  # Keep track of tools that have been seen once\n",
    "correct_predictions = 0\n",
    "ignore_items = 0\n",
    "import re\n",
    "\n",
    "for entry in data:\n",
    "    text = entry['text']\n",
    "    matched_tool = re.search(r\"You have access of the following tools:\\n1.(.+?): \", text)\n",
    "    if not matched_tool:\n",
    "        ignore_items += 1\n",
    "        continue  # If no tool is matched, skip this entry\n",
    "    \n",
    "    tool = matched_tool.group(1)[:10]  # Extract first 10 chars of the tool name\n",
    "    selected_runtime = entry['selected_runtime']\n",
    "    \n",
    "    # Check if it's the first occurrence of the tool\n",
    "    if tool not in first_occurrences and tool not in runtime_selection:\n",
    "        first_occurrences.add(tool)  # Mark it as seen for the first time\n",
    "        ignore_items += 1\n",
    "        continue  # Move to the next entry without doing anything else\n",
    "    \n",
    "    # If it's the second time we see the tool, record its runtime\n",
    "    if tool in first_occurrences and tool not in runtime_selection:\n",
    "        runtime_selection[tool] = selected_runtime\n",
    "        ignore_items += 1\n",
    "        continue  # Move to the next entry after recording the second runtime\n",
    "    \n",
    "    # From the third occurrence onwards, we check predictions\n",
    "    if tool in runtime_selection:\n",
    "        if runtime_selection[tool] == selected_runtime:\n",
    "            correct_predictions += 1\n",
    "        else:\n",
    "            print(f\"Incorrect prediction for tool: {tool}\")\n",
    "\n",
    "print(f\"Correct predictions (excluding first occurrences): {correct_predictions}\")\n",
    "accuracy_percentage = (correct_predictions / (len(data) - ignore_items)) * 100\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_workload_gen import LooGLEDataset, LooGLEDatasetType\n",
    "dataloader_short = LooGLEDataset(\n",
    "    loogle_dataset_type=LooGLEDatasetType.SHORT_QA, \n",
    "    num_patterns=4, \n",
    "    total_num_requests=100, \n",
    "    tokenizer=tokenizer, \n",
    "    load_dist=LoadDistribution.ALL, \n",
    "    crop_max_decode=True)\n",
    "workload_short = dataloader_short.generate_workload(max_length=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LPScheduler(3, depth_limit=3, update_interval=1)\n",
    "runtime_selected = []\n",
    "\n",
    "# input_ids = [tokenizer.encode(text) for text in texts]\n",
    "# for i in range(len(texts)):\n",
    "#     runtime = scheduler.runtime_selector(input_ids=input_ids[i], text=texts[i])\n",
    "#     scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node)\n",
    "#     runtime_selected.append(runtime)\n",
    "\n",
    "for i in workload_short:\n",
    "    runtime = scheduler.runtime_selector(input_ids=i[\"input_ids\"], text=i[\"text\"])\n",
    "    # scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node)\n",
    "    scheduler.tree_cache.pretty_print()\n",
    "    runtime_selected.append(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_random = pd.DataFrame(scheduler.metrics_dict)\n",
    "def get_tool(x):\n",
    "    text = x\n",
    "    \n",
    "    match = re.search(r\"Workload (.+?) Solve\", text)\n",
    "    if match:\n",
    "        tool = match.group(1)\n",
    "        return tool\n",
    "    return text\n",
    "df_random[\"text\"] = df_random[\"text\"].map(lambda x: get_tool(x))\n",
    "df_random.to_csv(\"test_random.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_acc(df_random, num_nodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "interval = 20\n",
    "depth_limit = 5\n",
    "lp_tree_traversal = LPTreeTraversal(3)\n",
    "lp_tree_traversal.depth_limit = depth_limit\n",
    "cache = RadixCache()\n",
    "random.shuffle(workload)\n",
    "modified_nodes = set()\n",
    "for item in workload:\n",
    "    cache.insert(tuple(item[\"input_ids\"]), modified_nodes=modified_nodes)\n",
    "\n",
    "for i in range(1000):\n",
    "    start_time = time.time()\n",
    "    time_after_inserting = time.time()\n",
    "    lp_tree_traversal.traverse_and_optimize(cache.root_node, existing_cost=lp_tree_traversal.get_exisiting_cost(), modified_nodes=modified_nodes)\n",
    "    modified_nodes = set()\n",
    "    lp_tree_traversal.update_nodes_with_solution()\n",
    "    # lp_tree_traversal.pretty_print(cache.root_node)\n",
    "    times.append({\"num_nodes\": len(lp_tree_traversal.node_map), \"time_after_inserting\": time.time() - time_after_inserting, \"total\": (i+1)*interval, \"depth_limit\": depth_limit})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lp_scheduler_metrics.csv\")\n",
    "df.drop(\"text\", axis=1, inplace=True)\n",
    "df['overhead'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
