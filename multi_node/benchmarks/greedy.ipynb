{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import GRB\n",
    "import gurobipy as gp\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from uuid import uuid4\n",
    "import copy\n",
    "import random\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "class LpNode:\n",
    "    def __init__(self, node_id, num_gpus):\n",
    "        self.node_id = node_id\n",
    "        self.variables = [\n",
    "            None for _ in range(num_gpus)\n",
    "        ]  # Will be initialized as binary variables in the model\n",
    "        self.children_token_cost_at_max_depth = 0  # Issue is that depth_limit will cut off the tokens for children and that will treat it as free\n",
    "        self.randomly_selected_gpu = None\n",
    "        self.load_variables = [None for _ in range(num_gpus)]\n",
    "        self.common_load = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        variable_values = [var.x if var else None for var in self.variables]\n",
    "        load_variable_values = [var.x if var else None for var in self.load_variables]\n",
    "        common_load = self.common_load.x if self.common_load else None\n",
    "        # ignore printing laod variables if None\n",
    "        if any(load_variable_values):\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values}, load_variables={load_variable_values}, common_load={common_load})\"\n",
    "        else:\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values})\"\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.id = uuid4()\n",
    "        self.children = defaultdict(TreeNode)\n",
    "        self.parent: Optional[TreeNode] = None\n",
    "        self.value = None\n",
    "        self.ref_counter = 0\n",
    "        self.last_access_time = time.time()\n",
    "        self.gpu_selections = set()\n",
    "        self.is_leaf = False\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.value)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.last_access_time < other.last_access_time\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, TreeNode):\n",
    "            return self.id == other.id  # Compare nodes based on their unique ID\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)  # Use the unique ID for hashing\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"TreeNode(id={self.id}, ref_counter={self.ref_counter})\"\n",
    "\n",
    "\n",
    "def match(key, seq):\n",
    "    i = 0\n",
    "    for k, w in zip(key, seq):\n",
    "        if k != w:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "class RadixCache:\n",
    "    def __init__(self, disable=False):\n",
    "        self.reset()\n",
    "        self.disable = disable\n",
    "\n",
    "    ##### Public API #####\n",
    "\n",
    "    def reset(self):\n",
    "        self.root_node = TreeNode()\n",
    "        self.root_node.value = []\n",
    "        self.root_node.ref_counter = 1\n",
    "        self.evictable_size_ = 0\n",
    "\n",
    "    def find_node(self, key):\n",
    "        if self.disable:\n",
    "            return None\n",
    "        current_gpu_selection, node = self.match_prefix_get_gpu_selection(key)\n",
    "        return node\n",
    "\n",
    "    def match_prefix_get_gpu_selection(self, key, path_to_node=[]):\n",
    "        if self.disable:\n",
    "            return [], self.root_node\n",
    "\n",
    "        value = []\n",
    "        current_gpu_selection = self.root_node.gpu_selections\n",
    "        current_gpu_selection, node = self._match_prefix_helper_gpu_selection(\n",
    "            self.root_node, key, value, current_gpu_selection\n",
    "        )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def _match_prefix_helper_gpu_selection(\n",
    "        self, node, key, value, current_gpu_selection\n",
    "    ):\n",
    "        node.last_access_time = time.time()\n",
    "        child: TreeNode\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len != 0:\n",
    "                if child.gpu_selections:\n",
    "                    current_gpu_selection = child.gpu_selections\n",
    "                if prefix_len < len(c_key):\n",
    "                    assert False\n",
    "                    new_node = self._split_node(\n",
    "                        c_key, child, prefix_len, new_nodes_created=new_nodes_created\n",
    "                    )\n",
    "                    value.append(new_node.value)\n",
    "                    # last_node[0] = new_node\n",
    "                else:\n",
    "                    value.append(child.value)\n",
    "                    # last_node[0] = child\n",
    "                    return self._match_prefix_helper_gpu_selection(\n",
    "                        child, key[prefix_len:], value, current_gpu_selection\n",
    "                    )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def match_prefix_return_str(self, key):\n",
    "        return \"\".join(self.match_prefix(key)[0])\n",
    "\n",
    "    def insert(\n",
    "        self,\n",
    "        key,\n",
    "        value=None,\n",
    "        node_map=None,\n",
    "        all_modified_nodes=None,\n",
    "        split_nodes=None,\n",
    "        depth_limit=0,\n",
    "    ):\n",
    "        if node_map is None:\n",
    "            node_map = {}\n",
    "            print(\"Node map is None\")\n",
    "        if split_nodes is None:\n",
    "            split_nodes = {}  # key -> node\n",
    "        if self.disable:\n",
    "            return len(key)\n",
    "\n",
    "        if value is None:\n",
    "            value = [x for x in key]\n",
    "        modified_nodes = set()\n",
    "        created_node = self._insert_helper(\n",
    "            self.root_node,\n",
    "            key,\n",
    "            value,\n",
    "            node_map=node_map,\n",
    "            modified_nodes=modified_nodes,\n",
    "            depth_limit=depth_limit,\n",
    "            current_depth=0,\n",
    "            split_nodes=split_nodes,\n",
    "        )\n",
    "\n",
    "        node: TreeNode = created_node\n",
    "        while node is not None:\n",
    "            if node in all_modified_nodes:\n",
    "                break\n",
    "            all_modified_nodes.add(node)\n",
    "            node = node.parent\n",
    "        return created_node\n",
    "\n",
    "    def pretty_print(self):\n",
    "        self._print_helper(self.root_node, 0)\n",
    "        print(f\"#tokens: {self.total_size()}\")\n",
    "\n",
    "    def total_size(self):\n",
    "        return self._total_size_helper(self.root_node)\n",
    "\n",
    "    def evict(self, num_tokens, evict_callback):\n",
    "        if self.disable:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        leaves = self._collect_leaves()\n",
    "        heapq.heapify(leaves)\n",
    "\n",
    "        num_evicted = 0\n",
    "        while num_evicted < num_tokens and len(leaves):\n",
    "            x = heapq.heappop(leaves)\n",
    "\n",
    "            if x == self.root_node:\n",
    "                break\n",
    "            if x.ref_counter > 0:\n",
    "                continue\n",
    "\n",
    "            num_evicted += evict_callback(x.value)\n",
    "            self._delete_leaf(x)\n",
    "\n",
    "            if len(x.parent.children) == 0:\n",
    "                heapq.heappush(leaves, x.parent)\n",
    "\n",
    "    def inc_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 0:\n",
    "                self.evictable_size_ -= len(node.value)\n",
    "                delta -= len(node.value)\n",
    "            node.ref_counter += 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def dec_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 1:\n",
    "                self.evictable_size_ += len(node.value)\n",
    "                delta += len(node.value)\n",
    "            node.ref_counter -= 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def remove_completed_input_ids(self, input_ids):\n",
    "        node = self.find_node(input_ids)\n",
    "        self.dec_ref_counter(node)  # remove reference counter up to parent\n",
    "\n",
    "    def evictable_size(self):\n",
    "        return self.evictable_size_\n",
    "\n",
    "    def _split_node(\n",
    "        self, key, child: TreeNode, split_len, node_map, depth_limit, current_depth\n",
    "    ):\n",
    "        # new_node -> child\n",
    "        new_node = TreeNode()\n",
    "        new_node.gpu_selections = copy.deepcopy(child.gpu_selections)\n",
    "        new_node.children = {key[split_len:]: child}\n",
    "        new_node.parent = child.parent\n",
    "        new_node.ref_counter = child.ref_counter\n",
    "\n",
    "        new_node.value = child.value[:split_len]\n",
    "        child.parent = new_node\n",
    "        child.value = child.value[split_len:]\n",
    "\n",
    "        new_node.parent.children[key[:split_len]] = new_node\n",
    "        del new_node.parent.children[key]\n",
    "        return new_node\n",
    "\n",
    "    def _insert_helper(\n",
    "        self,\n",
    "        node: TreeNode,\n",
    "        key,\n",
    "        value,\n",
    "        node_map,\n",
    "        modified_nodes,\n",
    "        depth_limit,\n",
    "        current_depth,\n",
    "        split_nodes,\n",
    "    ):\n",
    "        node.last_access_time = time.time()\n",
    "        node.ref_counter += 1\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "\n",
    "            if prefix_len == len(c_key):\n",
    "                if prefix_len == len(key):\n",
    "                    child.ref_counter += 1\n",
    "                    modified_nodes.add(child)\n",
    "                    return child\n",
    "                else:\n",
    "                    key = key[prefix_len:]\n",
    "                    value = value[prefix_len:]\n",
    "                    return self._insert_helper(\n",
    "                        child,\n",
    "                        key,\n",
    "                        value,\n",
    "                        node_map=node_map,\n",
    "                        modified_nodes=modified_nodes,\n",
    "                        depth_limit=depth_limit,\n",
    "                        current_depth=current_depth + 1,\n",
    "                        split_nodes=split_nodes,\n",
    "                    )\n",
    "\n",
    "            if prefix_len:\n",
    "                new_node = self._split_node(\n",
    "                    c_key,\n",
    "                    child,\n",
    "                    prefix_len,\n",
    "                    node_map,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                )\n",
    "                # modified_nodes.add(new_node)\n",
    "                # modified_nodes.add(child)\n",
    "                # TODO check if this makes sense to ignore this?\n",
    "                if child in node_map and current_depth < depth_limit:\n",
    "                    split_nodes[child] = new_node\n",
    "                return self._insert_helper(\n",
    "                    new_node,\n",
    "                    key[prefix_len:],\n",
    "                    value[prefix_len:],\n",
    "                    node_map=node_map,\n",
    "                    modified_nodes=modified_nodes,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                    split_nodes=split_nodes,\n",
    "                )\n",
    "\n",
    "        if len(key):\n",
    "            new_node = TreeNode()\n",
    "            new_node.gpu_selections = copy.deepcopy(node.gpu_selections)\n",
    "            new_node.parent = node\n",
    "            new_node.value = value\n",
    "            new_node.ref_counter = 1\n",
    "            node.children[key] = new_node\n",
    "            self.evictable_size_ += len(value)\n",
    "            # if current_depth < depth_limit:\n",
    "            modified_nodes.add(new_node)\n",
    "            # return new_node\n",
    "            return new_node\n",
    "        return node\n",
    "\n",
    "    def _print_helper(self, node, indent, depth=0):\n",
    "        if depth == 5:\n",
    "            return\n",
    "        for key, child in node.children.items():\n",
    "            print(\" \" * indent, len(key), key[:10], f\"r={child.ref_counter}\")\n",
    "            self._print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "\n",
    "    def _delete_leaf(self, node):\n",
    "        for k, v in node.parent.children.items():\n",
    "            if v == node:\n",
    "                break\n",
    "        del node.parent.children[k]\n",
    "        self.evictable_size_ -= len(k)\n",
    "\n",
    "    def _total_size_helper(self, node):\n",
    "        x = len(node.value)\n",
    "        for child in node.children.values():\n",
    "            x += self._total_size_helper(child)\n",
    "        return x\n",
    "\n",
    "    def _collect_leaves(self):\n",
    "        ret_list = []\n",
    "\n",
    "        def dfs_(cur_node):\n",
    "            if len(cur_node.children) == 0:\n",
    "                ret_list.append(cur_node)\n",
    "\n",
    "            for x in cur_node.children.values():\n",
    "                dfs_(x)\n",
    "\n",
    "        dfs_(self.root_node)\n",
    "        return ret_list\n",
    "\n",
    "\n",
    "class LPGurobiGreedyTraversal:\n",
    "    def __init__(self, num_gpus):\n",
    "        self.num_gpus = num_gpus\n",
    "        self.node_map = defaultdict(set)\n",
    "        self.depth_limit = 3\n",
    "        self.current_load_cost = [0 for _ in range(num_gpus)]\n",
    "        self.current_memory_cost = [0 for _ in range(num_gpus)]\n",
    "\n",
    "    def _calculate_children_token_cost(self, node: TreeNode):\n",
    "        \"\"\"\n",
    "        Recursively calculate the total number of tokens for all children of a given node,\n",
    "        effectively aggregating the tokens for nodes that are beyond the depth limit.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        total_tokens = node.num_tokens\n",
    "        for child in node.children.values():\n",
    "            total_tokens += self._calculate_children_token_cost(child)\n",
    "        return total_tokens\n",
    "\n",
    "    def traverse_and_optimize(\n",
    "        self, leaf_node: TreeNode, modified_nodes: set[TreeNode] = None, split_nodes={}\n",
    "    ):\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model = gp.Model(\"LPTreeTraversal\")\n",
    "        self.model.setParam(\"OutputFlag\", 0)  # Equivalent to verbose = 1 in python-mip\n",
    "        self.model.setParam(\"LogToConsole\", 0)\n",
    "\n",
    "        for key, value in split_nodes.items():\n",
    "            self.node_map[value] = self.node_map[key]\n",
    "\n",
    "        self.max_per_gpu_cost_constr = []\n",
    "\n",
    "        lp_node = LpNode(\"main\", self.num_gpus)\n",
    "        for gpu in range(self.num_gpus):\n",
    "            lp_node.variables[gpu] = self.model.addVar(\n",
    "                vtype=GRB.BINARY, name=f\"x_{gpu}\"\n",
    "            )\n",
    "\n",
    "        self.model.update()\n",
    "\n",
    "        self.model.addConstr(\n",
    "            gp.quicksum(lp_node.variables) >= 1\n",
    "        )  # at least 1 variable should be one\n",
    "        # Objective components: Let's assume we're trying to minimize the total cost adjusted for existing costs\n",
    "        total_cost = gp.LinExpr()\n",
    "        per_gpu_load_cost = [gp.LinExpr() for _ in range(self.num_gpus)]\n",
    "        per_gpu_mem_load_cost = [gp.LinExpr() for _ in range(self.num_gpus)]\n",
    "\n",
    "        new_total_memory_cost = [0 for _ in range(self.num_gpus)]\n",
    "\n",
    "        decode_length = 16  # Assume decoding occurs for 20 tokens\n",
    "        decoding_time = lambda x: 6.7 * x\n",
    "        total_decode_time = decoding_time(decode_length)\n",
    "\n",
    "        for prefix_node in modified_nodes:\n",
    "            num_tokens_total = 0\n",
    "            if prefix_node == leaf_node:\n",
    "                num_tokens_total += self._calculate_children_token_cost(leaf_node)\n",
    "            else:\n",
    "                num_tokens_total += prefix_node.num_tokens\n",
    "\n",
    "            mistral_tokens_to_prefill_time = lambda x: 0.148 * x + 22.7\n",
    "            num_tokens_time = mistral_tokens_to_prefill_time(num_tokens_total)\n",
    "\n",
    "            for gpu_index, var in enumerate(lp_node.variables):\n",
    "                previous_gpu_selected = gpu_index in self.node_map[prefix_node]\n",
    "                recomp_cost = var * (\n",
    "                    num_tokens_time - previous_gpu_selected * num_tokens_time\n",
    "                )\n",
    "                total_cost += recomp_cost\n",
    "                new_total_memory_cost[gpu_index] += (\n",
    "                    num_tokens_time - previous_gpu_selected * num_tokens_time\n",
    "                )\n",
    "\n",
    "        for gpu_index in range(self.num_gpus):\n",
    "            # Increment load by decoding time each time\n",
    "            per_gpu_load_cost[gpu_index] += (\n",
    "                lp_node.variables[gpu_index] * total_decode_time\n",
    "            )\n",
    "\n",
    "        max_per_gpu_cost = self.model.addVar(name=\"max_per_gpu_cost\", vtype=GRB.INTEGER)\n",
    "        for gpu_index in range(self.num_gpus):\n",
    "            per_gpu_mem_load_cost[gpu_index] += self.current_memory_cost[gpu_index]\n",
    "            # per_gpu_load_cost[gpu_index] += self.current_load_cost[gpu_index]\n",
    "            self.model.addConstr(\n",
    "                per_gpu_mem_load_cost[gpu_index] + per_gpu_load_cost[gpu_index]\n",
    "                <= max_per_gpu_cost,\n",
    "                name=f\"max_per_gpu_cost_constr_{gpu_index}\",\n",
    "            )\n",
    "            # updated load cost\n",
    "        # Set objective\n",
    "        self.model.setObjective(max_per_gpu_cost + total_cost, GRB.MINIMIZE)\n",
    "\n",
    "        # Model parameters\n",
    "        self.model.setParam(\"Threads\", 0)\n",
    "        self.model.setParam(\"TimeLimit\", 0.005)\n",
    "        self.model.setParam(\"MIPGap\", 0.02)\n",
    "\n",
    "        self.model.optimize()\n",
    "        self.model.update()\n",
    "        if self.model.Status == GRB.OPTIMAL:\n",
    "            # print('Optimal solution found.')\n",
    "            pass\n",
    "        elif self.model.Status == GRB.INFEASIBLE:\n",
    "            logging.warn(\"Infeasable solution found\")\n",
    "        else:\n",
    "            pass\n",
    "            # print('Feasible solution found.')\n",
    "\n",
    "        # todo find placement\n",
    "        selected_gpus = [\n",
    "            gpu_id for gpu_id, var in enumerate(lp_node.variables) if var.X >= 0.99\n",
    "        ]\n",
    "        leaf_node.gpu_selections = set(selected_gpus)\n",
    "        self.node_map[leaf_node] = leaf_node.gpu_selections\n",
    "\n",
    "        node: TreeNode = leaf_node.parent\n",
    "        while node != None:\n",
    "            parent_gpu_selection = set()\n",
    "            for key, children in node.children.items():\n",
    "                parent_gpu_selection.update(children.gpu_selections)\n",
    "            node.gpu_selections = parent_gpu_selection\n",
    "            self.node_map[node] = parent_gpu_selection\n",
    "            node = node.parent\n",
    "\n",
    "        for gpu in selected_gpus:\n",
    "            self.current_load_cost[\n",
    "                gpu\n",
    "            ] += total_decode_time  # Increase by decoding time to each gpu\n",
    "            self.current_memory_cost[gpu] += new_total_memory_cost[gpu]\n",
    "\n",
    "        # To get the total number of variables in the model\n",
    "        num_vars = self.model.numVars\n",
    "\n",
    "        # To get the total number of constraints in the model\n",
    "        num_constraints = self.model.numConstrs\n",
    "\n",
    "        # Print total number of parameters (variables and constraints)\n",
    "        # print(f\"Total number of variables: {num_vars}\")\n",
    "        # print(f\"Total number of constraints: {num_constraints}\")\n",
    "\n",
    "        # print(f\"Solving time: {(time.time() - start_time) * 1000}ms\")\n",
    "        return time.time() - start_time\n",
    "\n",
    "    def pretty_print(self, prefix_node, depth_limit=4, tokenizer=None):\n",
    "        self.pretty_print_helper(\n",
    "            prefix_node, depth_limit=depth_limit, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    def pretty_print_helper(\n",
    "        self, prefix_node: TreeNode, indent=\"\", depth=0, depth_limit=4, tokenizer=None\n",
    "    ):\n",
    "        if depth == depth_limit:\n",
    "            return\n",
    "        selected_gpus = self.node_map.get(prefix_node)\n",
    "\n",
    "        def get_tool(workload_item):\n",
    "            text = tokenizer.decode(workload_item)\n",
    "            if \":\" in text:\n",
    "                return text.split(\":\")[0].strip().replace(\"\\n\", \" \")\n",
    "            else:\n",
    "                return text[:16].strip().replace(\"\\n\", \"\")\n",
    "\n",
    "        print(\n",
    "            f\"{indent}Node {prefix_node.id} (Tokens: {get_tool(prefix_node.value)}, {len(prefix_node.value)}, {(prefix_node.ref_counter)}): GPUs {selected_gpus}\"\n",
    "        )\n",
    "\n",
    "        for child in prefix_node.children.values():\n",
    "            self.pretty_print_helper(\n",
    "                child,\n",
    "                indent + \"  \",\n",
    "                depth=depth + 1,\n",
    "                depth_limit=depth_limit,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "    def update_nodes_with_solution(self, modified_nodes=None):\n",
    "        for prefix_node, lp_node in self.node_map.items():\n",
    "            prefix_node.gpu_selections = set()\n",
    "            for gpu_id, var in enumerate(lp_node.variables):\n",
    "                if var.X >= 0.99:\n",
    "                    prefix_node.gpu_selections.add(gpu_id)\n",
    "\n",
    "    def completed_request(self, tree_cache, input_ids):\n",
    "        decode_length = 16  # Assume decoding occurs for 20 tokens\n",
    "        decoding_time = lambda x: 6.7 * x\n",
    "        total_decode_time = decoding_time(decode_length)\n",
    "        node: TreeNode = tree_cache.find_node(input_ids)\n",
    "        tree_cache.remove_completed_input_ids(input_ids)\n",
    "        for selection in node.gpu_selections:\n",
    "            self.current_load_cost[selection] -= total_decode_time\n",
    "\n",
    "\n",
    "class GurobiGreedyLPScheduler:\n",
    "    def __init__(self, num_nodes: int):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.tree_cache = RadixCache()\n",
    "        self.lp_tree_traversal = LPGurobiGreedyTraversal(num_nodes)\n",
    "        self.lp_tree_traversal.depth_limit = 64\n",
    "        self.metrics_dict = []\n",
    "        self.counter = 0\n",
    "        self.load = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.modified_nodes = set()\n",
    "\n",
    "    def runtime_selector(\n",
    "        self,\n",
    "        text: str = None,\n",
    "        request_id: str = None,\n",
    "        input_ids=None,\n",
    "    ):\n",
    "        # Tokenize the text\n",
    "        start_time = time.time()\n",
    "        with self.lock:\n",
    "            node_map = self.lp_tree_traversal.node_map\n",
    "            split_nodes = {}\n",
    "            node = self.tree_cache.insert(\n",
    "                tuple(input_ids),\n",
    "                node_map=node_map,\n",
    "                all_modified_nodes=self.modified_nodes,\n",
    "                depth_limit=self.lp_tree_traversal.depth_limit,\n",
    "                split_nodes=split_nodes,\n",
    "            )\n",
    "            self.lp_tree_traversal.traverse_and_optimize(\n",
    "                node, modified_nodes=self.modified_nodes, split_nodes=split_nodes\n",
    "            )\n",
    "            gpu_selections = node.gpu_selections\n",
    "            self.modified_nodes = set()\n",
    "\n",
    "        self.counter += 1\n",
    "        # Randomly select a node from gpu selections\n",
    "        mode = \"not_random\"\n",
    "        if len(gpu_selections) == 0 or len(gpu_selections) == self.num_nodes:\n",
    "            gpu_selections = set(range(self.num_nodes))\n",
    "            mode = \"random\"\n",
    "\n",
    "        runtime_selected = random.choice(list(gpu_selections))\n",
    "        self.load[runtime_selected] = self.load.get(runtime_selected, 0) + 1\n",
    "        self.metrics_dict.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"rid\": request_id,\n",
    "                \"selected_runtime\": runtime_selected,\n",
    "                \"overhead\": time.time() - start_time,\n",
    "                \"mode\": mode,\n",
    "            }\n",
    "        )\n",
    "        return runtime_selected\n",
    "\n",
    "    def finish_request(\n",
    "        self, text: str = None, request_id: str = None, input_ids=None, func_output=None\n",
    "    ):\n",
    "        with self.lock:\n",
    "            self.lp_tree_traversal.completed_request(self.tree_cache, input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# Add the parent directory of the 'src' directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\".\"), \"..\")))\n",
    "from transformers import AutoTokenizer\n",
    "from benchmarks.benchmark_workload_gen import ToolBenchDataLoader, LoadDistribution\n",
    "\n",
    "cache = RadixCache()\n",
    "\n",
    "num_workloads = 100\n",
    "num_requests = 4096\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "random.seed(5)\n",
    "dataloader = ToolBenchDataLoader(\n",
    "    \"datasets/G1_workload_updated_input_output_lengths_4096.json\",\n",
    "    num_workloads,\n",
    "    num_requests,\n",
    "    tokenizer,\n",
    "    LoadDistribution.EVEN,\n",
    ")\n",
    "workload = dataloader.generate_workload(k=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node dabb80a8-5fc7-4ac5-9749-38858022c8d4 (Tokens: , 0, 129): GPUs {0, 1}\n",
      "  Node d0463f20-d312-4e4b-afc7-fdcc5cf5fb9b (Tokens: <s> System, 358, 128): GPUs {0, 1}\n",
      "    Node 1311f80f-c70e-413c-a972-ab93f246a056 (Tokens: league_of_legends_galore, 1460, 2): GPUs {0}\n",
      "    Node f107d7f1-c0ce-4177-b320-2eec957a9ce6 (Tokens: review_generator_ai, 457, 2): GPUs {0}\n",
      "    Node 110f2a82-f9b7-44e7-aaad-8c64de1d80a9 (Tokens: epic_store_games, 748, 2): GPUs {1}\n",
      "    Node 50a43b1a-82ec-4e28-bc3b-02f20ab7517d (Tokens: quotes_api, 671, 2): GPUs {0}\n",
      "    Node 9da593d5-a697-40c2-9cc7-9c96c0ba34ae (Tokens: contextoguess, 687, 2): GPUs {1}\n",
      "    Node a6ccca98-6bf4-46d7-b617-0972d6b9dbff (Tokens: latest_ipl_news, 519, 2): GPUs {1}\n",
      "    Node 9659f5b5-54da-4ae3-a5a8-07cbc12bdd46 (Tokens: futures, 1481, 2): GPUs {1}\n",
      "    Node 89058100-f739-4ec4-8d47-962ce9f685bc (Tokens: web_search, 1497, 4): GPUs {0}\n",
      "    Node c52bf9da-ea4d-434b-9ae7-c75b31b94070 (Tokens: random_chunk_api, 1391, 2): GPUs {1}\n",
      "    Node 7144eacb-7b78-4457-8e95-fea3a7e47592 (Tokens: social_media_data_tt, 3429, 2): GPUs {0}\n",
      "    Node a93404d4-6e75-4574-97e0-2b2a0748a119 (Tokens: odesk_apis, 3668, 2): GPUs {1}\n",
      "    Node 74ddd7ab-f028-431c-98f4-138ca234f083 (Tokens: tank01_nfl_live_in_game_real_time_statistics_nfl, 2464, 2): GPUs {1}\n",
      "    Node 40fe314c-7656-472e-a1ab-32ca2774a282 (Tokens: tiktok_bulletproof, 899, 2): GPUs {0}\n",
      "    Node 0bbe3869-677d-414b-9983-2008214ae538 (Tokens: route_precedence_test_1, 454, 6): GPUs {0, 1}\n",
      "    Node 959a2797-d215-49fa-8083-ae858da0b8c3 (Tokens: world_scuba_diving_sites_api, 2056, 2): GPUs {1}\n",
      "    Node 08cd168e-0783-413b-a608-d211fb624b27 (Tokens: ai_news_v2, 1681, 2): GPUs {0}\n",
      "    Node a5bf0663-a508-4014-9fcb-86911820804b (Tokens: bayut, 2728, 2): GPUs {0}\n",
      "    Node a5b842c4-b3c9-4c72-8ec8-9ab8c7fd5b65 (Tokens: dns_lookup_v2, 1765, 2): GPUs {1}\n",
      "    Node 9e4bea3c-063d-45f4-9b6c-dc3ca0a53d2a (Tokens: india_pincode_api, 921, 6): GPUs {0, 1}\n",
      "    Node 02144d7f-900a-4baf-b98b-99359cb18c58 (Tokens: colegiosantaana, 1056, 4): GPUs {0}\n",
      "    Node 38248ad0-3397-4489-968e-8a6d9705f8c1 (Tokens: un, 1, 10): GPUs {0, 1}\n",
      "    Node e38e0dd6-a6be-4a6c-9abc-ffc7cf85a3a4 (Tokens: n, 1, 6): GPUs {0, 1}\n",
      "    Node a7573fd6-4196-42a0-8c5d-1e7c9944c507 (Tokens: direct_wines, 1067, 2): GPUs {1}\n",
      "    Node dcdf09e5-8241-4fc4-9a5f-10116ea84b90 (Tokens: poker_cards_cartomancy, 479, 2): GPUs {0}\n",
      "    Node 606a5392-14e1-41a4-8aea-24a887c1074d (Tokens: link_preview, 1266, 2): GPUs {1}\n",
      "    Node 6f7aa56c-4e72-4908-87a8-c940efe06292 (Tokens: mantis_object_detection, 1151, 2): GPUs {1}\n",
      "    Node a9d9b3f5-ee8e-45aa-a402-b5648c1fa0da (Tokens: inst, 1, 4): GPUs {0}\n",
      "    Node c4025786-5867-40fd-9036-8a3bdcb6fff8 (Tokens: get_4bro_1337x, 529, 4): GPUs {0}\n",
      "    Node 8f0fdc7c-d47c-4d12-8ade-9108b60e1340 (Tokens: dafabet, 766, 2): GPUs {1}\n",
      "    Node 018cba71-3323-46df-87db-21bc7b9ea7cc (Tokens: ice_hockey_data, 1901, 2): GPUs {1}\n",
      "    Node 91cc352a-c29c-44dd-9b00-3d12306b66e3 (Tokens: hapi_books, 1425, 4): GPUs {1}\n",
      "    Node 8e7c778b-43da-45d8-9f4c-94b7f927629a (Tokens: burning_series_api, 1083, 2): GPUs {0}\n",
      "    Node a450c1c4-cd5e-4c7c-b05a-f6f0f7fc6882 (Tokens: translate_all_languages, 1182, 2): GPUs {1}\n",
      "    Node fb2c2576-a7a1-47c7-960e-16910c041960 (Tokens: teas, 404, 4): GPUs {0}\n",
      "    Node e06721b5-f1e4-447c-abc8-903d2578b1c7 (Tokens: finance_social_sentiment_for_twitter_and_stocktwits, 2791, 2): GPUs {1}\n",
      "    Node 8cb6f64a-9736-49db-944c-a0e4788f1e76 (Tokens: easy_weather, 1726, 4): GPUs {0}\n",
      "    Node 61c28682-7ede-4e81-9575-b06c29eb976e (Tokens: thai_lotto_new_api, 600, 4): GPUs {0}\n",
      "    Node 1d0fe841-9975-4dbc-b4b9-b9ffd0736fd1 (Tokens: business_, 2, 4): GPUs {0}\n",
      "    Node 587ab9a4-6d4b-474f-bf26-82b49778f6ab (Tokens: whatsapp_private_api, 835, 2): GPUs {1}\n",
      "    Node 503d5e56-db5f-4d88-90c6-0df236ec0d45 (Tokens: investing_cryptocurrency_markets, 2981, 4): GPUs {1}\n",
      "    Node 5df4c2df-ab9b-4577-8776-d09365dee008 (Tokens: lemurbot, 1671, 2): GPUs {0}\n",
      "    Node 1161ddcc-39ea-4aae-b2a0-f51e30346049 (Tokens: argentina_movie_theatres, 625, 4): GPUs {1}\n",
      "    Node b2086e8c-fb7f-41f4-97ee-16a29fb0b3e7 (Tokens: teamriverbubbles_random_utilities, 1393, 2): GPUs {0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scheduler = GurobiGreedyLPScheduler(2)\n",
    "for i in range(2):\n",
    "    for i, item in enumerate(workload[:64]):\n",
    "        runtime_selected = scheduler.runtime_selector(\n",
    "            text=item[\"text\"], request_id=i, input_ids=item[\"input_ids\"]\n",
    "        )\n",
    "    # print(item[\"text\"], runtime_selected)\n",
    "# print(pd.DataFrame(scheduler.metrics_dict))\n",
    "scheduler.lp_tree_traversal.pretty_print(\n",
    "    scheduler.tree_cache.root_node, depth_limit=3, tokenizer=tokenizer\n",
    ")\n",
    "    # breakpoint()\n",
    "    # scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node, depth_limit=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
