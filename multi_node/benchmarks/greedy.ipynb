{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import GRB\n",
    "import gurobipy as gp\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from uuid import uuid4\n",
    "import copy\n",
    "import random\n",
    "import threading\n",
    "from enum import Enum, auto\n",
    "import logging\n",
    "from benchmarks.benchmark_utils import RequestFuncOutput\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "class LpNode:\n",
    "    def __init__(self, node_id, num_gpus):\n",
    "        self.node_id = node_id\n",
    "        self.variables = [\n",
    "            None for _ in range(num_gpus)\n",
    "        ]  # Will be initialized as binary variables in the model\n",
    "        self.children_token_cost_at_max_depth = 0  # Issue is that depth_limit will cut off the tokens for children and that will treat it as free\n",
    "        self.randomly_selected_gpu = None\n",
    "        self.load_variables = [None for _ in range(num_gpus)]\n",
    "        self.common_load = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        variable_values = [var.x if var else None for var in self.variables]\n",
    "        load_variable_values = [var.x if var else None for var in self.load_variables]\n",
    "        common_load = self.common_load.x if self.common_load else None\n",
    "        # ignore printing laod variables if None\n",
    "        if any(load_variable_values):\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values}, load_variables={load_variable_values}, common_load={common_load})\"\n",
    "        else:\n",
    "            return f\"LpNode(node_id={self.node_id}, variables={variable_values})\"\n",
    "\n",
    "\n",
    "class LPTreeNode:\n",
    "    def __init__(self):\n",
    "        self.id = uuid4()\n",
    "        self.children = defaultdict(LPTreeNode)\n",
    "        self.parent: Optional[LPTreeNode] = None\n",
    "        self.value = None\n",
    "        self.ref_counter = 0\n",
    "        self.last_access_time = time.time()\n",
    "        self.gpu_selections = set()\n",
    "        self.is_leaf = False\n",
    "        self.decode_length = 0\n",
    "        self.context_length = 0\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.value)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.last_access_time < other.last_access_time\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, LPTreeNode):\n",
    "            return self.id == other.id  # Compare nodes based on their unique ID\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)  # Use the unique ID for hashing\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"LPTreeNode(id={self.id}, ref_counter={self.ref_counter})\"\n",
    "\n",
    "\n",
    "def match(key, seq):\n",
    "    i = 0\n",
    "    for k, w in zip(key, seq):\n",
    "        if k != w:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "class LPRadixCache:\n",
    "    def __init__(self, disable=False):\n",
    "        self.reset()\n",
    "        self.disable = disable\n",
    "\n",
    "    ##### Public API #####\n",
    "\n",
    "    def reset(self):\n",
    "        self.root_node = LPTreeNode()\n",
    "        self.root_node.value = []\n",
    "        self.root_node.ref_counter = 1\n",
    "        self.evictable_size_ = 0\n",
    "\n",
    "    def find_node(self, key):\n",
    "        if self.disable:\n",
    "            return None\n",
    "        current_gpu_selection, node = self.match_prefix_get_gpu_selection(key)\n",
    "        return node\n",
    "\n",
    "    def match_prefix_get_gpu_selection(self, key, path_to_node=[]):\n",
    "        if self.disable:\n",
    "            return [], self.root_node\n",
    "\n",
    "        value = []\n",
    "        current_gpu_selection = self.root_node.gpu_selections\n",
    "        current_gpu_selection, node = self._match_prefix_helper_gpu_selection(\n",
    "            self.root_node, key, value, current_gpu_selection\n",
    "        )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def _match_prefix_helper_gpu_selection(\n",
    "        self, node, key, value, current_gpu_selection\n",
    "    ):\n",
    "        child: LPTreeNode\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "            if prefix_len != 0:\n",
    "                if child.gpu_selections:\n",
    "                    current_gpu_selection = child.gpu_selections\n",
    "                if prefix_len < len(c_key):\n",
    "                    print(prefix_len, len(c_key))\n",
    "                    assert False\n",
    "                    new_node = self._split_node(\n",
    "                        c_key, child, prefix_len, new_nodes_created=new_nodes_created\n",
    "                    )\n",
    "                    value.append(new_node.value)\n",
    "                    # last_node[0] = new_node\n",
    "                else:\n",
    "                    value.append(child.value)\n",
    "                    # last_node[0] = child\n",
    "                    return self._match_prefix_helper_gpu_selection(\n",
    "                        child, key[prefix_len:], value, current_gpu_selection\n",
    "                    )\n",
    "        return current_gpu_selection, node\n",
    "\n",
    "    def match_prefix_return_str(self, key):\n",
    "        return \"\".join(self.match_prefix(key)[0])\n",
    "\n",
    "    def insert(\n",
    "        self,\n",
    "        key,\n",
    "        value=None,\n",
    "        node_map=None,\n",
    "        all_modified_nodes=None,\n",
    "        split_nodes=None,\n",
    "        depth_limit=0,\n",
    "    ):\n",
    "        if node_map is None:\n",
    "            node_map = {}\n",
    "        if all_modified_nodes is None:\n",
    "            all_modified_nodes = set()\n",
    "        if split_nodes is None:\n",
    "            split_nodes = {}  # key -> node\n",
    "        if self.disable:\n",
    "            return len(key)\n",
    "\n",
    "        if value is None:\n",
    "            value = [x for x in key]\n",
    "        modified_nodes = set()\n",
    "        created_node = self._insert_helper(\n",
    "            self.root_node,\n",
    "            key,\n",
    "            value,\n",
    "            node_map=node_map,\n",
    "            modified_nodes=modified_nodes,\n",
    "            depth_limit=depth_limit,\n",
    "            current_depth=0,\n",
    "            split_nodes=split_nodes,\n",
    "        )\n",
    "\n",
    "        node: LPTreeNode = created_node\n",
    "        while node is not None:\n",
    "            if node in all_modified_nodes:\n",
    "                break\n",
    "            all_modified_nodes.add(node)\n",
    "            node = node.parent\n",
    "        return created_node\n",
    "\n",
    "    def pretty_print(self):\n",
    "        self._print_helper(self.root_node, 0)\n",
    "        print(f\"#tokens: {self.total_size()}\")\n",
    "\n",
    "    def total_size(self):\n",
    "        return self._total_size_helper(self.root_node)\n",
    "\n",
    "    def evict(self, num_tokens, evict_callback):\n",
    "        if self.disable:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        leaves = self._collect_leaves()\n",
    "        heapq.heapify(leaves)\n",
    "\n",
    "        num_evicted = 0\n",
    "        while num_evicted < num_tokens and len(leaves):\n",
    "            x = heapq.heappop(leaves)\n",
    "\n",
    "            if x == self.root_node:\n",
    "                break\n",
    "            if x.ref_counter > 0:\n",
    "                continue\n",
    "\n",
    "            num_evicted += evict_callback(x)\n",
    "            self._delete_leaf(x)\n",
    "\n",
    "            if len(x.parent.children) == 0:\n",
    "                heapq.heappush(leaves, x.parent)\n",
    "\n",
    "    def inc_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            if node.ref_counter == 0:\n",
    "                self.evictable_size_ -= len(node.value)\n",
    "                delta -= len(node.value)\n",
    "            node.ref_counter += 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def dec_ref_counter(self, node):\n",
    "        delta = 0\n",
    "        while node != self.root_node:\n",
    "            # if node.ref_counter == 1: TODO why does this exist?\n",
    "            #     self.evictable_size_ += len(node.value)\n",
    "            #     delta += len(node.value)\n",
    "            node.ref_counter -= 1\n",
    "            node = node.parent\n",
    "        return delta\n",
    "\n",
    "    def remove_completed_input_ids(self, input_ids):\n",
    "        node = self.find_node(input_ids)\n",
    "        self.dec_ref_counter(node)  # remove reference counter up to parent\n",
    "    \n",
    "    def evictable_size(self):\n",
    "        return self.evictable_size_\n",
    "\n",
    "    def _split_node(\n",
    "        self, key, child: LPTreeNode, split_len, node_map, depth_limit, current_depth\n",
    "    ):\n",
    "        # new_node -> child\n",
    "        new_node = LPTreeNode()\n",
    "        new_node.gpu_selections = copy.deepcopy(child.gpu_selections)\n",
    "        new_node.children = {key[split_len:]: child}\n",
    "        new_node.parent = child.parent\n",
    "        new_node.ref_counter = child.ref_counter\n",
    "\n",
    "        new_node.value = child.value[:split_len]\n",
    "        child.parent = new_node\n",
    "        child.value = child.value[split_len:]\n",
    "\n",
    "        new_node.parent.children[key[:split_len]] = new_node\n",
    "        del new_node.parent.children[key]\n",
    "        return new_node\n",
    "\n",
    "    def _insert_helper(\n",
    "        self,\n",
    "        node: LPTreeNode,\n",
    "        key,\n",
    "        value,\n",
    "        node_map,\n",
    "        modified_nodes,\n",
    "        depth_limit,\n",
    "        current_depth,\n",
    "        split_nodes,\n",
    "        parent_context_length = 0\n",
    "    ):\n",
    "        node.last_access_time = time.time()\n",
    "        node.ref_counter += 1\n",
    "        for c_key, child in node.children.items():\n",
    "            prefix_len = match(c_key, key)\n",
    "\n",
    "            if prefix_len == len(c_key):\n",
    "                if prefix_len == len(key):\n",
    "                    child.ref_counter += 1\n",
    "                    modified_nodes.add(child)\n",
    "                    return child\n",
    "                else:\n",
    "                    key = key[prefix_len:]\n",
    "                    value = value[prefix_len:]\n",
    "                    return self._insert_helper(\n",
    "                        child,\n",
    "                        key,\n",
    "                        value,\n",
    "                        node_map=node_map,\n",
    "                        modified_nodes=modified_nodes,\n",
    "                        depth_limit=depth_limit,\n",
    "                        current_depth=current_depth + 1,\n",
    "                        split_nodes=split_nodes,\n",
    "                        parent_context_length=node.context_length + len(child.value),\n",
    "                    )\n",
    "\n",
    "            if prefix_len:\n",
    "                new_node = self._split_node(\n",
    "                    c_key,\n",
    "                    child,\n",
    "                    prefix_len,\n",
    "                    node_map,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                )\n",
    "                # modified_nodes.add(new_node)\n",
    "                # modified_nodes.add(child)\n",
    "                # TODO check if this makes sense to ignore this?\n",
    "                if child in node_map and current_depth < depth_limit:\n",
    "                    split_nodes[child] = new_node\n",
    "                return self._insert_helper(\n",
    "                    new_node,\n",
    "                    key[prefix_len:],\n",
    "                    value[prefix_len:],\n",
    "                    node_map=node_map,\n",
    "                    modified_nodes=modified_nodes,\n",
    "                    depth_limit=depth_limit,\n",
    "                    current_depth=current_depth + 1,\n",
    "                    split_nodes=split_nodes,\n",
    "                    parent_context_length=node.context_length + len(child.value),\n",
    "                )\n",
    "\n",
    "        if len(key):\n",
    "            new_node = LPTreeNode()\n",
    "            new_node.gpu_selections = copy.deepcopy(node.gpu_selections)\n",
    "            new_node.parent = node\n",
    "            new_node.value = value\n",
    "            new_node.ref_counter = 1\n",
    "            node.children[key] = new_node\n",
    "            node.context_length = parent_context_length + len(value) \n",
    "            self.evictable_size_ += len(value)\n",
    "            # if current_depth < depth_limit:\n",
    "            modified_nodes.add(new_node)\n",
    "            # return new_node\n",
    "            return new_node\n",
    "        return node\n",
    "\n",
    "    def _print_helper(self, node, indent, depth=0):\n",
    "        if depth == 5:\n",
    "            return\n",
    "        for key, child in node.children.items():\n",
    "            print(\" \" * indent, len(key), key[:10], f\"r={child.ref_counter}\")\n",
    "            self._print_helper(child, indent=indent + 2, depth=depth + 1)\n",
    "\n",
    "    def _delete_leaf(self, node):\n",
    "        for k, v in node.parent.children.items():\n",
    "            if v == node:\n",
    "                break\n",
    "        del node.parent.children[k]\n",
    "        self.evictable_size_ -= len(k)\n",
    "\n",
    "    def _total_size_helper(self, node):\n",
    "        x = len(node.value)\n",
    "        for child in node.children.values():\n",
    "            x += self._total_size_helper(child)\n",
    "        return x\n",
    "\n",
    "    def _collect_leaves(self):\n",
    "        ret_list = []\n",
    "\n",
    "        def dfs_(cur_node):\n",
    "            if len(cur_node.children) == 0:\n",
    "                ret_list.append(cur_node)\n",
    "\n",
    "            for x in cur_node.children.values():\n",
    "                dfs_(x)\n",
    "\n",
    "        dfs_(self.root_node)\n",
    "        return ret_list\n",
    "\n",
    "class LPGurobiGreedyTraversal:\n",
    "    def __init__(self, num_gpus, gpu_configs=None):\n",
    "        self.lp_forward_simulation = gpu_configs[0].lp_forward_simulation if gpu_configs else None\n",
    "        if self.lp_forward_simulation is None:\n",
    "            self.lp_forward_simulation = lambda num_extend_tokens, is_leaf: num_extend_tokens * 0.148 + (22.7 if is_leaf else 0)\n",
    "\n",
    "        self.num_gpus = num_gpus\n",
    "        self.node_to_gpu_selections = defaultdict(set)\n",
    "        self.depth_limit = 3\n",
    "        self.current_load_cost = [0 for _ in range(num_gpus)]\n",
    "        self.current_memory_cost = [0 for _ in range(num_gpus)]\n",
    "        self.model = gp.Model()\n",
    "        self.model.setParam(\"OutputFlag\", 0)\n",
    "        self.model.setParam(\"LogToConsole\", 0)\n",
    "        self.model.setParam('Seed', 0)\n",
    "        self.model.setParam(\"Threads\", 0)\n",
    "        # self.model.setParam(\"WorkLimit\", 0.005)\n",
    "        self.model.setParam(\"MIPGap\", 0.02)\n",
    "        # self.model.setParam(\"Method\", 4)\n",
    "        # self.model.setParam(\"TimeLimit\", 0.005)\n",
    "\n",
    "        self.variables_initialized = False\n",
    "        self.initialize_or_update_variables()\n",
    "        # self.init_cache() # for performance reasons, Presolve an LP. This reduces the cost of the first LP\n",
    "\n",
    "    def initialize_or_update_variables(self):\n",
    "        if not self.variables_initialized:\n",
    "            # First run: add variables\n",
    "            self.max_per_gpu_cost = self.model.addVar(name=\"max_per_gpu_cost\", vtype=GRB.INTEGER)\n",
    "            self.lp_node = LpNode(\"main\", self.num_gpus)\n",
    "            for gpu in range(self.num_gpus):\n",
    "                self.lp_node.variables[gpu] = self.model.addVar(\n",
    "                    vtype=GRB.BINARY, name=f\"x_{gpu}\"\n",
    "                )\n",
    "            self.variables_initialized = True\n",
    "        else:\n",
    "            # Subsequent runs: reset the model but keep the same structure\n",
    "            self.model.reset()\n",
    "\n",
    "\n",
    "    def _calculate_children_token_cost(self, node: LPTreeNode):\n",
    "        \"\"\"\n",
    "        Recursively calculate the total number of tokens for all children of a given node,\n",
    "        effectively aggregating the tokens for nodes that are beyond the depth limit.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return 0\n",
    "        total_tokens = node.num_tokens\n",
    "        for child in node.children.values():\n",
    "            total_tokens += self._calculate_children_token_cost(child)\n",
    "        return total_tokens\n",
    "\n",
    "    def update_constraints(self, leaf_node: LPTreeNode, modified_nodes: set[LPTreeNode], decode_cost):\n",
    "        self.model.remove(self.model.getConstrs())\n",
    "        self.model.addConstr(gp.quicksum(self.lp_node.variables) >= 1, \"min_one_gpu\")\n",
    "\n",
    "        total_cost = gp.LinExpr()\n",
    "        per_gpu_load_cost = [gp.LinExpr() for _ in range(self.num_gpus)]\n",
    "        per_gpu_mem_load_cost = [gp.LinExpr() for _ in range(self.num_gpus)]\n",
    "        new_total_memory_cost = [0 for _ in range(self.num_gpus)]\n",
    "\n",
    "        decoding_time = decode_cost\n",
    "        node: LPTreeNode = leaf_node\n",
    "        for gpu_index, var in enumerate(self.lp_node.variables):\n",
    "            node = leaf_node\n",
    "            total_recomputation_tokens = 0\n",
    "            while node != None:\n",
    "                if gpu_index in node.gpu_selections:\n",
    "                    break\n",
    "                total_recomputation_tokens += node.num_tokens\n",
    "                node = node.parent\n",
    "\n",
    "            recomputation_time = 0\n",
    "            if total_recomputation_tokens != 0:\n",
    "                recomputation_time = self.lp_forward_simulation(total_recomputation_tokens, leaf_node.context_length)\n",
    "            total_cost += var * recomputation_time\n",
    "            new_total_memory_cost[gpu_index] += recomputation_time\n",
    "\n",
    "        for gpu_index in range(self.num_gpus):\n",
    "            per_gpu_load_cost[gpu_index] += self.lp_node.variables[gpu_index] * decoding_time\n",
    "            per_gpu_mem_load_cost[gpu_index] += self.current_memory_cost[gpu_index]  # Assuming current_memory_cost is tracked\n",
    "            per_gpu_load_cost[gpu_index] += self.current_load_cost[gpu_index]\n",
    "\n",
    "            self.model.addConstr(\n",
    "                per_gpu_mem_load_cost[gpu_index] + per_gpu_load_cost[gpu_index] <= self.max_per_gpu_cost,\n",
    "                name=f\"max_per_gpu_cost_constr_{gpu_index}\"\n",
    "            )\n",
    "\n",
    "        self.model.setObjective(self.max_per_gpu_cost + total_cost, GRB.MINIMIZE)\n",
    "        return new_total_memory_cost, decoding_time\n",
    "\n",
    "    def traverse_and_optimize(\n",
    "        self, leaf_node: LPTreeNode, modified_nodes: set[LPTreeNode] = None, split_nodes={}, decode_cost=0\n",
    "    ):\n",
    "        start_time = time.time()\n",
    "        leaf_node.is_leaf = True\n",
    "\n",
    "        self.initialize_or_update_variables()\n",
    "        for key, value in split_nodes.items():\n",
    "            self.node_to_gpu_selections[value] = self.node_to_gpu_selections[key]\n",
    "\n",
    "        new_total_memory_cost, total_decode_time = self.update_constraints(leaf_node, modified_nodes, decode_cost)\n",
    "        self.model.optimize()\n",
    "\n",
    "        if self.model.Status == GRB.OPTIMAL:\n",
    "            pass\n",
    "        elif self.model.Status == GRB.INFEASIBLE:\n",
    "            print(\"Infeasable solution found\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        selected_gpus = self.update_gpu_selections(self.lp_node, leaf_node)\n",
    "        for gpu in selected_gpus:\n",
    "            self.current_load_cost[gpu] += total_decode_time\n",
    "            self.current_memory_cost[gpu] += new_total_memory_cost[gpu]\n",
    "        return time.time() - start_time\n",
    "\n",
    "\n",
    "    def update_gpu_selections(self, lp_node, leaf_node):\n",
    "        selected_gpus = [\n",
    "            gpu_id for gpu_id, var in enumerate(self.lp_node.variables) if var.X >= 0.99\n",
    "        ]\n",
    "        leaf_node.gpu_selections = set(selected_gpus)\n",
    "        self.node_to_gpu_selections[leaf_node] = leaf_node.gpu_selections\n",
    "\n",
    "        node: LPTreeNode = leaf_node.parent\n",
    "        while node != None:\n",
    "            parent_gpu_selection = set()\n",
    "            for key, children in node.children.items():\n",
    "                parent_gpu_selection.update(children.gpu_selections)\n",
    "            node.gpu_selections = parent_gpu_selection\n",
    "            self.node_to_gpu_selections[node] = parent_gpu_selection\n",
    "            node = node.parent\n",
    "        return selected_gpus\n",
    "        \n",
    "    def pretty_print(self, prefix_node, depth_limit=4, tokenizer=None):\n",
    "        self.pretty_print_helper(\n",
    "            prefix_node, depth_limit=depth_limit, tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    def pretty_print_helper(\n",
    "        self, prefix_node: LPTreeNode, indent=\"\", depth=0, depth_limit=4, tokenizer=None\n",
    "    ):\n",
    "        if depth == depth_limit:\n",
    "            return\n",
    "        selected_gpus = self.node_to_gpu_selections.get(prefix_node)\n",
    "\n",
    "        def get_tool(workload_item):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "            text = tokenizer.decode(workload_item)\n",
    "            if \":\" in text:\n",
    "                return text.split(\":\")[0].strip().replace(\"\\n\", \" \")\n",
    "            else:\n",
    "                return text[:16].strip().replace(\"\\n\", \"\")\n",
    "\n",
    "        print(\n",
    "            f\"{indent}Node {prefix_node.id} (Tokens: {get_tool(prefix_node.value)}, {len(prefix_node.value)}, {(prefix_node.ref_counter)}): GPUs {selected_gpus}\"\n",
    "        )\n",
    "\n",
    "        for child in prefix_node.children.values():\n",
    "            self.pretty_print_helper(\n",
    "                child,\n",
    "                indent + \"  \",\n",
    "                depth=depth + 1,\n",
    "                depth_limit=depth_limit,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "    def update_nodes_with_solution(self, modified_nodes=None):\n",
    "        for prefix_node, lp_node in self.node_to_gpu_selections.items():\n",
    "            prefix_node.gpu_selections = set()\n",
    "            for gpu_id, var in enumerate(lp_node.variables):\n",
    "                if var.X >= 0.99:\n",
    "                    prefix_node.gpu_selections.add(gpu_id)\n",
    "\n",
    "    def completed_request(self, tree_cache, input_ids, decode_cost):\n",
    "        # decoding_time = lambda x: 47 * x\n",
    "        total_decode_time = decode_cost\n",
    "        node: LPTreeNode = tree_cache.find_node(input_ids)\n",
    "        tree_cache.remove_completed_input_ids(input_ids)\n",
    "        for selection in node.gpu_selections:\n",
    "            self.current_load_cost[selection] -= total_decode_time\n",
    "\n",
    "    def insert_into_cache_and_solve(self, input_ids, tree_cache, decode_cost):\n",
    "        node_map = self.node_to_gpu_selections\n",
    "        split_nodes = {}\n",
    "        modified_nodes = set()\n",
    "        node = tree_cache.insert(\n",
    "            tuple(input_ids),\n",
    "            node_map=node_map,\n",
    "            all_modified_nodes=modified_nodes,\n",
    "            depth_limit=self.depth_limit,\n",
    "            split_nodes=split_nodes,\n",
    "        )\n",
    "        self.traverse_and_optimize(\n",
    "            node, modified_nodes=modified_nodes, split_nodes=split_nodes, decode_cost=decode_cost\n",
    "        )\n",
    "        return node\n",
    "\n",
    "class GurobiGreedyLPScheduler:\n",
    "    class RuntimeSelectionType(Enum):\n",
    "        RANDOM = auto()\n",
    "        NOT_RANDOM = auto()\n",
    "\n",
    "    def __init__(self, num_nodes: int, gpu_configs = None):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.tree_cache = LPRadixCache()\n",
    "        self.lp_tree_traversal = LPGurobiGreedyTraversal(num_nodes, gpu_configs)\n",
    "        self.lp_tree_traversal.depth_limit = 64\n",
    "        self.metrics_dict = []\n",
    "        self.load = {}\n",
    "        self.lock = threading.Lock()\n",
    "        self.gpu_configs = gpu_configs\n",
    "\n",
    "        self.runtime_caches = [LPRadixCache() for _ in range(num_nodes)]\n",
    "        self.max_tokens_gpu = [198466, 198466]\n",
    "        self.average_tpot = 0.041\n",
    "        self.counter = 0\n",
    "        self.rid_to_deocde_cost = {}\n",
    "        self.average_tpot_queue = collections.deque(maxlen=50)\n",
    "        self.average_tpot_queue.append(0.041)\n",
    "        # self.average_tpot_queue.append(0.041)\n",
    "        # self.average_tpot_queue.append(0.041)\n",
    "        # self.average_tpot_queue.append(0.041)\n",
    "        # self.average_tpot_queue.append(0.041)\n",
    "\n",
    "    def evict_callback(self, node: LPTreeNode, runtime_selected: int):\n",
    "        \"\"\"Method to handle eviction logic.\"\"\"\n",
    "        updated_node = self.lp_tree_traversal.node_to_gpu_selections.get(node)\n",
    "        if updated_node:\n",
    "            updated_node.remove(runtime_selected)\n",
    "            if len(updated_node) == 0:\n",
    "                del self.lp_tree_traversal.node_to_gpu_selections[node]\n",
    "        num_tokens = len(node.value)\n",
    "        # accumlation\n",
    "        mistral_tokens_to_prefill_time = self.lp_tree_traversal.lp_forward_simulation(num_tokens, node.context_length)\n",
    "\n",
    "        # mistral_tokens_to_prefill_time = 0.148 * num_tokens\n",
    "        # if node.is_leaf:\n",
    "        #     mistral_tokens_to_prefill_time += 22.7\n",
    "\n",
    "        self.lp_tree_traversal.current_memory_cost[runtime_selected] -= mistral_tokens_to_prefill_time\n",
    "        return len(node.value)\n",
    "\n",
    "    def select_runtime_from_gpu_selections(self, gpu_selections) -> tuple[int, RuntimeSelectionType]:\n",
    "        mode = GurobiGreedyLPScheduler.RuntimeSelectionType.NOT_RANDOM\n",
    "        if len(gpu_selections) == 0 or len(gpu_selections) == self.num_nodes:\n",
    "            gpu_selections = set(range(self.num_nodes))\n",
    "            mode = GurobiGreedyLPScheduler.RuntimeSelectionType.RANDOM\n",
    "        runtime_selected = random.choice(list(gpu_selections))\n",
    "        return runtime_selected, mode\n",
    "\n",
    "    def insert_then_evict_from_runtime_cache(self, input_ids, runtime_selected):\n",
    "        runtime_cache = self.runtime_caches[runtime_selected]\n",
    "        node = runtime_cache.insert(tuple(input_ids))\n",
    "        current_max_tokens = self.max_tokens_gpu[runtime_selected]\n",
    "        if runtime_cache.evictable_size() > current_max_tokens:\n",
    "            num_tokens = runtime_cache.evictable_size() - current_max_tokens\n",
    "            runtime_cache.evict(num_tokens, lambda node: self.evict_callback(node, runtime_selected))\n",
    "            # print(f\"GPU {runtime_selected} Evictable size: \", runtime_cache.evictable_size(), current_max_tokens)\n",
    "\n",
    "    def runtime_selector(\n",
    "        self,\n",
    "        text: str = None,\n",
    "        request_id: str = None,\n",
    "        input_ids=None,\n",
    "        sampling_params=None\n",
    "    ):\n",
    "        # Tokenize the text\n",
    "        start_time = time.time()\n",
    "        with self.lock:\n",
    "            st = time.time()\n",
    "            average_tpot_queue_ms = np.median(self.average_tpot_queue)\n",
    "            # decode_cost =  sampling_params.get(\"max_new_tokens\") * average_tpot_queue_ms\n",
    "            decode_cost = average_tpot_queue_ms * sampling_params.get(\"max_new_tokens\") * 1000 # 1000 bc converting seconds to miliseconds\n",
    "            self.rid_to_deocde_cost[request_id] = decode_cost \n",
    "            # request_id -> memory_cost\n",
    "            node = self.lp_tree_traversal.insert_into_cache_and_solve(input_ids, self.tree_cache, decode_cost)\n",
    "            solving_time = time.time() - st\n",
    "\n",
    "            gpu_selections: set[int] = node.gpu_selections\n",
    "            runtime_selected, selection_type = self.select_runtime_from_gpu_selections(gpu_selections)\n",
    "            self.load[runtime_selected] = self.load.get(runtime_selected, 0) + 1\n",
    "            self.insert_then_evict_from_runtime_cache(input_ids, runtime_selected)\n",
    "        if time.time() - start_time > 0.03:\n",
    "            print(f\"Overall time\", time.time() - start_time, solving_time)\n",
    "        self.metrics_dict.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"rid\": request_id,\n",
    "                \"selected_runtime\": runtime_selected,\n",
    "                \"overhead\": time.time() - start_time,\n",
    "                \"mode\": selection_type,\n",
    "            }\n",
    "        )\n",
    "        return runtime_selected\n",
    "\n",
    "    def finish_request(\n",
    "        self, text: str = None, request_id: str = None, input_ids=None, func_output: RequestFuncOutput=None\n",
    "    ):\n",
    "        with self.lock:\n",
    "            self.average_tpot_queue.append(func_output.tpot)\n",
    "            if request_id not in self.rid_to_deocde_cost:\n",
    "                decode_cost = 0\n",
    "                assert False\n",
    "            else:\n",
    "                decode_cost = self.rid_to_deocde_cost.pop(request_id)\n",
    "            self.lp_tree_traversal.completed_request(self.tree_cache, input_ids, decode_cost)\n",
    "            self.runtime_caches[func_output.runtime_selected].remove_completed_input_ids(input_ids)\n",
    "            tpot = func_output.tpot\n",
    "            self.average_tpot = (self.average_tpot * self.counter + tpot) / (self.counter + 1)\n",
    "            self.counter += 1\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    from transformers import AutoTokenizer\n",
    "    import sys\n",
    "    import os\n",
    "    import copy\n",
    "    import random\n",
    "\n",
    "    # Add the parent directory of the 'src' directory to the Python path\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\".\"), \"..\")))\n",
    "    from transformers import AutoTokenizer\n",
    "    from benchmarks.benchmark_workload_gen import ToolBenchDataLoader, LoadDistribution\n",
    "\n",
    "    cache = LPRadixCache()\n",
    "\n",
    "    num_workloads = 100\n",
    "    num_requests = 4096\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "    random.seed(5)\n",
    "    dataloader = ToolBenchDataLoader(\n",
    "        \"benchmarks/datasets/G1_workload_updated_input_output_lengths_4096_cropped_to_50.json\",\n",
    "        num_workloads,\n",
    "        num_requests,\n",
    "        tokenizer,\n",
    "        LoadDistribution.ZIPF,\n",
    "    )\n",
    "    workload = dataloader.generate_workload(k=1.1)\n",
    "\n",
    "    scheduler = GurobiGreedyLPScheduler(2)\n",
    "    for i, item in enumerate(workload[:64]):\n",
    "        runtime_selected = scheduler.runtime_selector(\n",
    "            text=item[\"text\"], request_id=i, input_ids=item[\"input_ids\"]\n",
    "        )\n",
    "        # print(item[\"text\"], runtime_selected)\n",
    "    # print(pd.DataFrame(scheduler.metrics_dict))\n",
    "    scheduler.lp_tree_traversal.pretty_print(\n",
    "        scheduler.tree_cache.root_node, depth_limit=3, tokenizer=tokenizer\n",
    "    )\n",
    "    breakpoint()\n",
    "    scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node, depth_limit=3)\n",
    "\n",
    "\n",
    "bursty load -> tool replicated on every node. Only tool 1 - 50. \n",
    "non bursty load. tool 1 - 50 \n",
    "\n",
    "Toolbench bursty \n",
    "Loogle <- cache of tools is empty\n",
    "toolbench "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# Add the parent directory of the 'src' directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\".\"), \"..\")))\n",
    "from transformers import AutoTokenizer\n",
    "from benchmarks.benchmark_workload_gen import ToolBenchDataLoader, LoadDistribution\n",
    "\n",
    "cache = RadixCache()\n",
    "\n",
    "num_workloads = 100\n",
    "num_requests = 4096\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "random.seed(5)\n",
    "dataloader = ToolBenchDataLoader(\n",
    "    \"datasets/G1_workload_updated_input_output_lengths_4096.json\",\n",
    "    num_workloads,\n",
    "    num_requests,\n",
    "    tokenizer,\n",
    "    LoadDistribution.EVEN,\n",
    ")\n",
    "workload = dataloader.generate_workload(k=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node dabb80a8-5fc7-4ac5-9749-38858022c8d4 (Tokens: , 0, 129): GPUs {0, 1}\n",
      "  Node d0463f20-d312-4e4b-afc7-fdcc5cf5fb9b (Tokens: <s> System, 358, 128): GPUs {0, 1}\n",
      "    Node 1311f80f-c70e-413c-a972-ab93f246a056 (Tokens: league_of_legends_galore, 1460, 2): GPUs {0}\n",
      "    Node f107d7f1-c0ce-4177-b320-2eec957a9ce6 (Tokens: review_generator_ai, 457, 2): GPUs {0}\n",
      "    Node 110f2a82-f9b7-44e7-aaad-8c64de1d80a9 (Tokens: epic_store_games, 748, 2): GPUs {1}\n",
      "    Node 50a43b1a-82ec-4e28-bc3b-02f20ab7517d (Tokens: quotes_api, 671, 2): GPUs {0}\n",
      "    Node 9da593d5-a697-40c2-9cc7-9c96c0ba34ae (Tokens: contextoguess, 687, 2): GPUs {1}\n",
      "    Node a6ccca98-6bf4-46d7-b617-0972d6b9dbff (Tokens: latest_ipl_news, 519, 2): GPUs {1}\n",
      "    Node 9659f5b5-54da-4ae3-a5a8-07cbc12bdd46 (Tokens: futures, 1481, 2): GPUs {1}\n",
      "    Node 89058100-f739-4ec4-8d47-962ce9f685bc (Tokens: web_search, 1497, 4): GPUs {0}\n",
      "    Node c52bf9da-ea4d-434b-9ae7-c75b31b94070 (Tokens: random_chunk_api, 1391, 2): GPUs {1}\n",
      "    Node 7144eacb-7b78-4457-8e95-fea3a7e47592 (Tokens: social_media_data_tt, 3429, 2): GPUs {0}\n",
      "    Node a93404d4-6e75-4574-97e0-2b2a0748a119 (Tokens: odesk_apis, 3668, 2): GPUs {1}\n",
      "    Node 74ddd7ab-f028-431c-98f4-138ca234f083 (Tokens: tank01_nfl_live_in_game_real_time_statistics_nfl, 2464, 2): GPUs {1}\n",
      "    Node 40fe314c-7656-472e-a1ab-32ca2774a282 (Tokens: tiktok_bulletproof, 899, 2): GPUs {0}\n",
      "    Node 0bbe3869-677d-414b-9983-2008214ae538 (Tokens: route_precedence_test_1, 454, 6): GPUs {0, 1}\n",
      "    Node 959a2797-d215-49fa-8083-ae858da0b8c3 (Tokens: world_scuba_diving_sites_api, 2056, 2): GPUs {1}\n",
      "    Node 08cd168e-0783-413b-a608-d211fb624b27 (Tokens: ai_news_v2, 1681, 2): GPUs {0}\n",
      "    Node a5bf0663-a508-4014-9fcb-86911820804b (Tokens: bayut, 2728, 2): GPUs {0}\n",
      "    Node a5b842c4-b3c9-4c72-8ec8-9ab8c7fd5b65 (Tokens: dns_lookup_v2, 1765, 2): GPUs {1}\n",
      "    Node 9e4bea3c-063d-45f4-9b6c-dc3ca0a53d2a (Tokens: india_pincode_api, 921, 6): GPUs {0, 1}\n",
      "    Node 02144d7f-900a-4baf-b98b-99359cb18c58 (Tokens: colegiosantaana, 1056, 4): GPUs {0}\n",
      "    Node 38248ad0-3397-4489-968e-8a6d9705f8c1 (Tokens: un, 1, 10): GPUs {0, 1}\n",
      "    Node e38e0dd6-a6be-4a6c-9abc-ffc7cf85a3a4 (Tokens: n, 1, 6): GPUs {0, 1}\n",
      "    Node a7573fd6-4196-42a0-8c5d-1e7c9944c507 (Tokens: direct_wines, 1067, 2): GPUs {1}\n",
      "    Node dcdf09e5-8241-4fc4-9a5f-10116ea84b90 (Tokens: poker_cards_cartomancy, 479, 2): GPUs {0}\n",
      "    Node 606a5392-14e1-41a4-8aea-24a887c1074d (Tokens: link_preview, 1266, 2): GPUs {1}\n",
      "    Node 6f7aa56c-4e72-4908-87a8-c940efe06292 (Tokens: mantis_object_detection, 1151, 2): GPUs {1}\n",
      "    Node a9d9b3f5-ee8e-45aa-a402-b5648c1fa0da (Tokens: inst, 1, 4): GPUs {0}\n",
      "    Node c4025786-5867-40fd-9036-8a3bdcb6fff8 (Tokens: get_4bro_1337x, 529, 4): GPUs {0}\n",
      "    Node 8f0fdc7c-d47c-4d12-8ade-9108b60e1340 (Tokens: dafabet, 766, 2): GPUs {1}\n",
      "    Node 018cba71-3323-46df-87db-21bc7b9ea7cc (Tokens: ice_hockey_data, 1901, 2): GPUs {1}\n",
      "    Node 91cc352a-c29c-44dd-9b00-3d12306b66e3 (Tokens: hapi_books, 1425, 4): GPUs {1}\n",
      "    Node 8e7c778b-43da-45d8-9f4c-94b7f927629a (Tokens: burning_series_api, 1083, 2): GPUs {0}\n",
      "    Node a450c1c4-cd5e-4c7c-b05a-f6f0f7fc6882 (Tokens: translate_all_languages, 1182, 2): GPUs {1}\n",
      "    Node fb2c2576-a7a1-47c7-960e-16910c041960 (Tokens: teas, 404, 4): GPUs {0}\n",
      "    Node e06721b5-f1e4-447c-abc8-903d2578b1c7 (Tokens: finance_social_sentiment_for_twitter_and_stocktwits, 2791, 2): GPUs {1}\n",
      "    Node 8cb6f64a-9736-49db-944c-a0e4788f1e76 (Tokens: easy_weather, 1726, 4): GPUs {0}\n",
      "    Node 61c28682-7ede-4e81-9575-b06c29eb976e (Tokens: thai_lotto_new_api, 600, 4): GPUs {0}\n",
      "    Node 1d0fe841-9975-4dbc-b4b9-b9ffd0736fd1 (Tokens: business_, 2, 4): GPUs {0}\n",
      "    Node 587ab9a4-6d4b-474f-bf26-82b49778f6ab (Tokens: whatsapp_private_api, 835, 2): GPUs {1}\n",
      "    Node 503d5e56-db5f-4d88-90c6-0df236ec0d45 (Tokens: investing_cryptocurrency_markets, 2981, 4): GPUs {1}\n",
      "    Node 5df4c2df-ab9b-4577-8776-d09365dee008 (Tokens: lemurbot, 1671, 2): GPUs {0}\n",
      "    Node 1161ddcc-39ea-4aae-b2a0-f51e30346049 (Tokens: argentina_movie_theatres, 625, 4): GPUs {1}\n",
      "    Node b2086e8c-fb7f-41f4-97ee-16a29fb0b3e7 (Tokens: teamriverbubbles_random_utilities, 1393, 2): GPUs {0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scheduler = GurobiGreedyLPScheduler(2)\n",
    "for i in range(2):\n",
    "    for i, item in enumerate(workload[:64]):\n",
    "        runtime_selected = scheduler.runtime_selector(\n",
    "            text=item[\"text\"], request_id=i, input_ids=item[\"input_ids\"]\n",
    "        )\n",
    "    # print(item[\"text\"], runtime_selected)\n",
    "# print(pd.DataFrame(scheduler.metrics_dict))\n",
    "scheduler.lp_tree_traversal.pretty_print(\n",
    "    scheduler.tree_cache.root_node, depth_limit=3, tokenizer=tokenizer\n",
    ")\n",
    "    # breakpoint()\n",
    "    # scheduler.lp_tree_traversal.pretty_print(scheduler.tree_cache.root_node, depth_limit=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
