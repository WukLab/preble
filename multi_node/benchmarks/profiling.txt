Timer unit: 1e-09 s

Total time: 0.0365537 s
File: /mnt/data/ssd/sglang_multi_model/multi_node/greedy_lp.py
Function: traverse_and_optimize at line 424

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   424                                               @do_profile
   425                                               def traverse_and_optimize(
   426                                                   self, leaf_node: LPTreeNode, modified_nodes: set[LPTreeNode] = None, split_nodes={}, decode_length=45
   427                                               ):
   428         1       1032.0   1032.0      0.0          start_time = time.time()
   429                                           
   430         1   30830487.0    3e+07     84.3          self.model = gp.Model()
   431         1      27673.0  27673.0      0.1          self.model.setParam("OutputFlag", 0)  # Equivalent to verbose = 1 in python-mip
   432         1       3918.0   3918.0      0.0          self.model.setParam("LogToConsole", 0)
   433                                           
   434         1       1223.0   1223.0      0.0          for key, value in split_nodes.items():
   435                                                       self.node_to_gpu_selections[value] = self.node_to_gpu_selections[key]
   436                                           
   437         1        664.0    664.0      0.0          self.max_per_gpu_cost_constr = []
   438         1      75979.0  75979.0      0.2          max_per_gpu_cost = self.model.addVar(name="max_per_gpu_cost", vtype=GRB.INTEGER)
   439         1      50323.0  50323.0      0.1          lp_node = LpNode("main", self.num_gpus)
   440         3       1278.0    426.0      0.0          for gpu in range(self.num_gpus):
   441         4       8558.0   2139.5      0.0              lp_node.variables[gpu] = self.model.addVar(
   442         2       7238.0   3619.0      0.0                  vtype=GRB.BINARY, name=f"x_{gpu}"
   443                                                       )
   444                                           
   445         2    2730896.0    1e+06      7.5          self.model.addConstr(
   446         1      68011.0  68011.0      0.2              gp.quicksum(lp_node.variables) >= 1
   447                                                   )
   448                                                   
   449                                                    # at least 1 variable should be one
   450         1       1589.0   1589.0      0.0          var_init_time = time.time() - start_time
   451                                                   # Objective components: Let's assume we're trying to minimize the total cost adjusted for existing costs
   452         1       2372.0   2372.0      0.0          total_cost = gp.LinExpr()
   453         1       5725.0   5725.0      0.0          per_gpu_load_cost = [gp.LinExpr() for _ in range(self.num_gpus)]
   454         1       1908.0   1908.0      0.0          per_gpu_mem_load_cost = [gp.LinExpr() for _ in range(self.num_gpus)]
   455                                           
   456         1       1400.0   1400.0      0.0          new_total_memory_cost = [0 for _ in range(self.num_gpus)]
   457                                           
   458         1        465.0    465.0      0.0          decoding_time = lambda x: 6.7 * x
   459         1       1006.0   1006.0      0.0          total_decode_time = decoding_time(decode_length)
   460                                           
   461         3       1475.0    491.7      0.0          for prefix_node in modified_nodes:
   462         2        593.0    296.5      0.0              num_tokens_total = 0
   463                                           
   464         2       8543.0   4271.5      0.0              if prefix_node == leaf_node:
   465         1       4841.0   4841.0      0.0                  num_tokens_total += self._calculate_children_token_cost(leaf_node)
   466                                                       else:
   467         1       8290.0   8290.0      0.0                  num_tokens_total += prefix_node.num_tokens
   468                                           
   469         2        987.0    493.5      0.0              mistral_tokens_to_prefill_time = lambda x: 0.148 * x + 22.7
   470         2       2110.0   1055.0      0.0              num_tokens_time = mistral_tokens_to_prefill_time(num_tokens_total)
   471                                           
   472         6       4287.0    714.5      0.0              for gpu_index, var in enumerate(lp_node.variables):
   473         4      11494.0   2873.5      0.0                  previous_gpu_selected = gpu_index in self.node_to_gpu_selections[prefix_node]
   474         8      10083.0   1260.4      0.0                  recomp_cost = var * (
   475         4       1662.0    415.5      0.0                      num_tokens_time - previous_gpu_selected * num_tokens_time
   476                                                           )
   477         4       4486.0   1121.5      0.0                  total_cost += recomp_cost
   478         8       3576.0    447.0      0.0                  new_total_memory_cost[gpu_index] += (
   479         4       1431.0    357.8      0.0                      num_tokens_time - previous_gpu_selected * num_tokens_time
   480                                                           )
   481                                           
   482         3       1481.0    493.7      0.0          for gpu_index in range(self.num_gpus):
   483                                                       # Increment load by decoding time each time
   484         4       2467.0    616.8      0.0              per_gpu_load_cost[gpu_index] += (
   485         2       1205.0    602.5      0.0                  lp_node.variables[gpu_index] * total_decode_time
   486                                                       )
   487                                           
   488         3       1454.0    484.7      0.0          for gpu_index in range(self.num_gpus):
   489         2       3330.0   1665.0      0.0              per_gpu_mem_load_cost[gpu_index] += self.current_memory_cost[gpu_index]
   490                                                       # per_gpu_load_cost[gpu_index] += self.current_load_cost[gpu_index]
   491         4      40642.0  10160.5      0.1              self.model.addConstr(
   492         4      18214.0   4553.5      0.0                  per_gpu_mem_load_cost[gpu_index] + per_gpu_load_cost[gpu_index]
   493         2        666.0    333.0      0.0                  <= max_per_gpu_cost,
   494         2       2086.0   1043.0      0.0                  name=f"max_per_gpu_cost_constr_{gpu_index}",
   495                                                       )
   496                                                       # updated load cost
   497                                           
   498                                                   # Set objective
   499         1      44312.0  44312.0      0.1          self.model.setObjective(max_per_gpu_cost + total_cost, GRB.MINIMIZE)
   500         1       9987.0   9987.0      0.0          self.model.setParam("Threads", 0)
   501         1      18904.0  18904.0      0.1          self.model.setParam("TimeLimit", 0.005)
   502         1       5329.0   5329.0      0.0          self.model.setParam("MIPGap", 0.02)
   503         1    2426215.0    2e+06      6.6          self.model.optimize()
   504                                           
   505                                                   # Save gurobi to file
   506         1      39119.0  39119.0      0.1          if self.model.Status == GRB.OPTIMAL:
   507                                                       # print('Optimal solution found.')
   508         1        486.0    486.0      0.0              pass
   509                                                   elif self.model.Status == GRB.INFEASIBLE:
   510                                                       print("Infeasable solution found")
   511                                                   else:
   512                                                       pass
   513                                                       # print('Feasible solution found.')
   514                                                   # todo find placement
   515         2      23132.0  11566.0      0.1          selected_gpus = [
   516         1       1827.0   1827.0      0.0              gpu_id for gpu_id, var in enumerate(lp_node.variables) if var.X >= 0.99
   517                                                   ]
   518         1       2565.0   2565.0      0.0          leaf_node.gpu_selections = set(selected_gpus)
   519         1       5088.0   5088.0      0.0          self.node_to_gpu_selections[leaf_node] = leaf_node.gpu_selections
   520                                           
   521         1        574.0    574.0      0.0          node: LPTreeNode = leaf_node.parent
   522         2       4326.0   2163.0      0.0          while node != None:
   523         1        687.0    687.0      0.0              parent_gpu_selection = set()
   524         2       2419.0   1209.5      0.0              for key, children in node.children.items():
   525         1       1532.0   1532.0      0.0                  parent_gpu_selection.update(children.gpu_selections)
   526         1        565.0    565.0      0.0              node.gpu_selections = parent_gpu_selection
   527         1       1723.0   1723.0      0.0              self.node_to_gpu_selections[node] = parent_gpu_selection
   528         1        480.0    480.0      0.0              node = node.parent
   529                                                   
   530         2       1128.0    564.0      0.0          for gpu in selected_gpus:
   531         3       3118.0   1039.3      0.0              self.current_load_cost[
   532         1        335.0    335.0      0.0                  gpu
   533         1        370.0    370.0      0.0              ] += total_decode_time  # Increase by decoding time to each gpu
   534         1       1131.0   1131.0      0.0              self.current_memory_cost[gpu] += new_total_memory_cost[gpu]
   535                                                   # print(f"Time taken: ", time.time() - start_time)
   536         1       1212.0   1212.0      0.0          return time.time() - start_time

