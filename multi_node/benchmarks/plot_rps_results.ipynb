{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def extract_metrics(log_file, param_keys, max_gpus=2):\n",
    "    metrics = {\n",
    "        \"overall_latencies\": [],\n",
    "        \"overall_throughput\": [],\n",
    "        \"request_latencies\": [],\n",
    "        \"counts_all\": [],\n",
    "        \"request_llm_metrics\": [],\n",
    "        \"finished_llm_metrics\": [],\n",
    "        \"prefill_ratio\": [],\n",
    "        \"ttft_pmetrics\": [],\n",
    "        \"tpot_pmetrics\": [],\n",
    "        \"latency_pmetrics\": []\n",
    "    }\n",
    "\n",
    "    param_format = f\"Params=\\({'(.+?), ' * (len(param_keys)-1)}(.+?)\\)\"\n",
    "    patterns = {\n",
    "        \"Overall Latency\": (\"overall_latencies\", f\"{param_format} Overall Latency: (.*)\"),\n",
    "        \"Overall Throughput\": (\"overall_throughput\", f\"{param_format} Overall Throughput: (.*)\"),\n",
    "        \"Overall Request Latency\": (\"request_latencies\", f\"{param_format} Overall Request Latency: (.*), STD: (.*), P90: (.*)\"),\n",
    "        \"Average TTFT\": (\"request_llm_metrics\", f\"{param_format} Average TTFT: (.*), Average TOPT: (.*), Throughput ToksPerSec: (.*)\"),\n",
    "        \"Num Finished Requests\": (\"finished_llm_metrics\", f\"{param_format} Num Finished Requests: (.*), Finished Throughput ToksPerSec: (.*)\"),\n",
    "        \"Counts\": (\"counts_all\", f\"{param_format} Counts: (.*)\"),\n",
    "        'Overall PrefillRatio': (\"prefill_ratio\", f\"{param_format} Overall PrefillRatio: (.*)\"),\n",
    "        \"TTFT\": (\"ttft_pmetrics\", f\"{param_format} TTFT p50, p90, p99: (.*),(.*),(.*)\"),\n",
    "        \"TPOT\": (\"tpot_pmetrics\", f\"{param_format} TPOT p50, p90, p99: (.*),(.*),(.*)\"),\n",
    "        \"Latency\": (\"latency_pmetrics\", f\"{param_format} Latency p50, p90, p99: (.*),(.*),(.*)\")\n",
    "    }\n",
    "    # print(patterns)\n",
    "    \n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            for key, (metric_key, pattern) in patterns.items():\n",
    "                if key in line:\n",
    "                    match = re.search(pattern, line)\n",
    "                    if match:\n",
    "                        data = {param_key: match.group(i+1) for i, param_key in enumerate(param_keys)}\n",
    "                        # Parse specific data based on metric type\n",
    "                        if metric_key == \"overall_latencies\":\n",
    "                            data.update({\"latency\": float(match.group(len(param_keys)+1))})\n",
    "                        elif metric_key == \"overall_throughput\":\n",
    "                            data.update({\"throughput\": float(match.group(len(param_keys)+1))})\n",
    "                        elif metric_key == \"request_llm_metrics\":\n",
    "                            data.update({\"ttft\": float(match.group(len(param_keys)+1)), \"topt\": float(match.group(len(param_keys)+2)), \"throughput_tkns_per_sec\": float(match.group(len(param_keys)+3))})\n",
    "                        elif metric_key == \"request_latencies\":\n",
    "                            data.update({\"avg_latency\": float(match.group(len(param_keys)+1)), \"std\": float(match.group(len(param_keys)+2)), \"p90\": float(match.group(len(param_keys)+3))})\n",
    "                        elif metric_key == \"finished_llm_metrics\":\n",
    "                            data.update({\"num_finished\": int(match.group(len(param_keys)+1)), \"throughput_finished_tkns_per_sec\": float(match.group(len(param_keys)+2))})\n",
    "                        elif metric_key == \"prefill_ratio\":\n",
    "                            data.update({\"prefill_ratio\": float(match.group(len(param_keys)+1))})\n",
    "                        elif metric_key == \"ttft_pmetrics\":\n",
    "                            data.update({\"ttft_p50\": float(match.group(len(param_keys)+1)), \"ttft_p90\": float(match.group(len(param_keys)+2)), \"ttft_p99\": float(match.group(len(param_keys)+3))})\n",
    "                        elif metric_key == \"tpot_pmetrics\":\n",
    "                            data.update({\"tpot_p50\": float(match.group(len(param_keys)+1)), \"tpot_p90\": float(match.group(len(param_keys)+2)), \"tpot_p99\": float(match.group(len(param_keys)+3))})\n",
    "                        elif metric_key == \"latency_pmetrics\":\n",
    "                            data.update({\"latency_p50\": float(match.group(len(param_keys)+1)), \"latency_p90\": float(match.group(len(param_keys)+2)), \"latency_p99\": float(match.group(len(param_keys)+3))})\n",
    "                        elif metric_key == \"counts_all\":\n",
    "                            # Note: Using eval() can be risky and is generally not recommended\n",
    "                            count_ratio = eval(match.group(len(param_keys)+1))\n",
    "                            ratios = {int(k): float(v) for k, v in count_ratio.items()}\n",
    "                            ratios[1] = ratios.get(1, 0)\n",
    "                            sum_ratios = sum(ratios.values())\n",
    "                            data.update({\"counts_all\": ratios, \"ratio\": ratios[0]/(sum_ratios)})\n",
    "                        metrics[metric_key].append(data)\n",
    "    policy_data = collect_policy_data(log_file, max_gpus=max_gpus)\n",
    "    metrics[\"scheduling_dynamics\"] = policy_data\n",
    "    return metrics\n",
    "\n",
    "# Usage example (ensure you replace `log_file` with the actual log file path and adjust `param_keys` as needed)\n",
    "\n",
    "def prepare_data_for_plotting(metric_data, metric_key, title, policy_replacements=None, group_by_key=None,baseline_key=\"RANDOM\", speedup_direction=\"lower\", global_metric_columns=None):\n",
    "    df = pd.DataFrame(metric_data)\n",
    "    if not df.empty:\n",
    "        df = df.drop([\"model_name\", \"exp_time\"], axis=1, errors='ignore')\n",
    "        # df = df.drop([\"model_name\", \"percent_random_prefixes\", \"exp_time\"], axis=1, errors='ignore')\n",
    "        if policy_replacements:\n",
    "            # print(df[\"policy\"])\n",
    "            for original, replacement in policy_replacements.items():\n",
    "                df['policy'] = df['policy'].str.replace(original, replacement)\n",
    "        df = df.groupby(['num_prefix', \"percent_random_prefixes\", 'num_requests', 'rps', 'policy'])[group_by_key].mean().unstack()\n",
    "        if baseline_key:\n",
    "            df = calculate_speedup(df, baseline_policy=baseline_key, speedup_direction=speedup_direction, global_metric_columns=global_metric_columns)\n",
    "        return df, title\n",
    "    return None, None\n",
    "\n",
    "def collect_policy_data(log_file, max_gpus=2):\n",
    "    policy_data = []\n",
    "    current_policy_info = {}\n",
    "    max_hit_rate_gpu = {}\n",
    "    waiting_queue_lengths = []\n",
    "\n",
    "    def append_policy_data():\n",
    "        \"\"\"Helper function to append the current policy's data.\"\"\"\n",
    "        if current_policy_info:  # Check if there's data to append\n",
    "            # Update with GPU hit rates and average waiting queue length\n",
    "            for gpu_id, hit_rate in max_hit_rate_gpu.items():\n",
    "                current_policy_info[f'max_tree_cache_hit_rate_gpu{gpu_id}'] = hit_rate\n",
    "            current_policy_info['avg_waiting_queue_len'] = (\n",
    "                sum(waiting_queue_lengths) / len(waiting_queue_lengths)\n",
    "                if waiting_queue_lengths else 0\n",
    "            )\n",
    "            policy_data.append(current_policy_info)\n",
    "\n",
    "    with open(log_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if \"=====STARTING Policy\" in line:\n",
    "                append_policy_data()  # Append data for the previous policy\n",
    "                \n",
    "                # Reset for new policy\n",
    "                current_policy_info = {}\n",
    "                max_hit_rate_gpu = {i: 0.0 for i in range(max_gpus)}  # Prepare for up to 8 GPUs\n",
    "                waiting_queue_lengths = []\n",
    "\n",
    "                # Extract policy and other properties\n",
    "                match = re.search(r\"=====STARTING Policy (.+?), (\\d+) WORKLOADS, (.*?) NON-SHARED, (\\d+) REQUESTS, (.*) REQ/s\", line)\n",
    "                if match:\n",
    "                    current_policy_info = {\n",
    "                        'policy': match.group(1),\n",
    "                        'num_prefix': (match.group(2)),\n",
    "                        'percent_random_prefixes': (match.group(3)),\n",
    "                        'num_requests': (match.group(4)),\n",
    "                        'rps': (match.group(5)),\n",
    "                    }\n",
    "            elif \"tree_cache_hit_rate\" in line:\n",
    "                gpu_id_match = re.search(r\"GPU: (\\d+)\", line)\n",
    "                hit_rate_match = re.search(r\"tree_cache_hit_rate: ([\\d.]+)%\", line)\n",
    "                waiting_queue_length_match = re.search(r\"#remaining_req: (\\d+)\", line)\n",
    "\n",
    "                if waiting_queue_length_match:\n",
    "                    waiting_queue_lengths.append(int(waiting_queue_length_match.group(1)))\n",
    "\n",
    "                if gpu_id_match and hit_rate_match:\n",
    "                    gpu_id = int(gpu_id_match.group(1))\n",
    "                    hit_rate = float(hit_rate_match.group(1))\n",
    "                    if gpu_id in max_hit_rate_gpu:\n",
    "                        max_hit_rate_gpu[gpu_id] = max(max_hit_rate_gpu[gpu_id], hit_rate)\n",
    "\n",
    "    append_policy_data()  # Append data for the last policy\n",
    "    # for policy in policy_data:\n",
    "    #     print(policy)\n",
    "    return policy_data\n",
    "\n",
    "def calculate_speedup(df, baseline_policy='RANDOM', speedup_column_name = f\"speedup_over_random\", speedup_direction=\"lower\", global_metric_columns=None):\n",
    "    \"\"\"\n",
    "    Adds speedup columns to the DataFrame based on metrics compared to a baseline policy.\n",
    "    \n",
    "    :param df: DataFrame after groupby().mean().unstack(), where columns are policies.\n",
    "    :param baseline_policy: The column name for the baseline policy to compare against.\n",
    "    :param new_column_suffix: Suffix for the new speedup columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if baseline_policy in df.columns:\n",
    "        first_policy = global_metric_columns[0]\n",
    "        # for policy in df.columns:\n",
    "            # if policy != baseline_policy:\n",
    "                # print(f\"Calculating speedup for {policy} and {baseline_policy}\")\n",
    "        if speedup_direction == \"lower\":\n",
    "            df[speedup_column_name] = df[baseline_policy] / df[first_policy]\n",
    "        else:\n",
    "            df[speedup_column_name] = df[first_policy] / df[baseline_policy]\n",
    "            # break\n",
    "    return df\n",
    "\n",
    "def compare_two_plots(exp1, \n",
    "                      exp2, \n",
    "                      indices_to_compare, \n",
    "                      title_prefix=\"Node 3 vs Node 4\", \n",
    "                      labels=['ORACLE_B', 'RANDOM', 'speedup_over_random'], \n",
    "                      exp1_title=\"Node 3\",\n",
    "                      exp2_title=\"Node 4\"):\n",
    "    # Creating subplots\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plot_configs = []\n",
    "    for metric_key in exp1.keys():\n",
    "        for group_by_key in exp1[metric_key].keys():\n",
    "            plot_configs.append({\n",
    "                \"metric_key\": metric_key,\n",
    "                \"group_by_key\": group_by_key,\n",
    "                \"ylabel\": \"Value\",\n",
    "                \"title\": f\"{group_by_key}\",\n",
    "            })\n",
    "\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        metric_key = config['metric_key']\n",
    "        group_by_key = config['group_by_key']\n",
    "        ylabel = config['ylabel']\n",
    "        title = config['title']\n",
    "\n",
    "        for idx in indices_to_compare:\n",
    "            df_exp1 = exp1[metric_key][group_by_key].loc[idx]\n",
    "            df_exp2 = exp2[metric_key][group_by_key].loc[idx]\n",
    "            # Data for plotting\n",
    "            x = np.arange(len(labels))  # the label locations\n",
    "\n",
    "            value_exp1 = df_exp1[labels]\n",
    "            value_exp2 = df_exp2[labels]\n",
    "\n",
    "            # Plotting\n",
    "            width = 0.35  # the width of the bars\n",
    "            rects1 = axes[i].bar(x - width/2, value_exp1, width, label=exp1_title)\n",
    "            rects2 = axes[i].bar(x + width/2, value_exp2, width, label=exp2_title)\n",
    "\n",
    "            # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "            axes[i].set_ylabel(ylabel)\n",
    "            axes[i].set_title(f\"{title_prefix} {title}\")\n",
    "            axes[i].set_xticks(x)\n",
    "            axes[i].set_xticklabels(labels)\n",
    "            axes[i].legend()\n",
    "\n",
    "            # Optional: Add autolabel function here to display the height of bars\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "def compare_n_plots(\n",
    "        experiments, \n",
    "        exp_titles, \n",
    "        indices_to_compare, \n",
    "        title_prefix=\"Comparison\", \n",
    "        labels=['ORACLE_B', 'RANDOM', 'speedup_over_random']):\n",
    "    # Assuming all experiments have the same structure and metrics\n",
    "    num_experiments = len(experiments)\n",
    "    \n",
    "    # Calculating the width of the bars so that all bars fit within each group\n",
    "    total_width = 0.8  # Total width of the group of bars for each label\n",
    "    width = total_width / num_experiments  # Width of each individual bar\n",
    "    \n",
    "    # Creating subplots\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plot_configs = []\n",
    "    for metric_key in experiments[0].keys():\n",
    "        for group_by_key in experiments[0][metric_key].keys():\n",
    "            plot_configs.append({\n",
    "                \"metric_key\": metric_key,\n",
    "                \"group_by_key\": group_by_key,\n",
    "                \"ylabel\": \"Value\",\n",
    "                \"title\": f\"{group_by_key}\",\n",
    "            })\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        metric_key = config['metric_key']\n",
    "        group_by_key = config['group_by_key']\n",
    "        ylabel = config['ylabel']\n",
    "        title = config['title']\n",
    "\n",
    "        x = np.arange(len(labels))  # the label locations\n",
    "        offset = -total_width / 2 + width / 2  # Starting offset for the first bar\n",
    "        \n",
    "        for exp_idx, experiment in enumerate(experiments):\n",
    "            idx = indices_to_compare[exp_idx]\n",
    "            df = experiment[metric_key][group_by_key].loc[idx]\n",
    "            value = df[labels]\n",
    "            # Plotting\n",
    "            rects = axes[i].bar(x + offset + exp_idx * width, value, width, label=exp_titles[exp_idx])\n",
    "\n",
    "        # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "        axes[i].set_ylabel(ylabel)\n",
    "        axes[i].set_title(f\"{title_prefix} {title}\")\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(labels)\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metrics(metrics, title_prefix, ax_dims=(6, 3), figsize=(28, 14), global_metric_dfs={}, global_metric_columns=None, baseline_key = 'RANDOM'):\n",
    "    fig, axes = plt.subplots(*ax_dims, figsize=figsize)\n",
    "    plt.tight_layout(pad=6.0)\n",
    "    \n",
    "    # Ensure axes is always treated as an array, even when there's only one subplot\n",
    "    axes = np.array(axes).reshape(-1)  # This handles both single and multiple Axes objects\n",
    "\n",
    "    plot_configs = [\n",
    "        # {\"metric_key\": \"overall_latencies\", \"title\": \"End to End Latency\", \"group_by_key\": \"latency\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        # {\"metric_key\": \"overall_throughput\", \"title\": \"Overall Throughput\", \"group_by_key\": \"throughput\", 'speedup_direction': 'higher', \"ylabel\": \"reqs/sec\"},\n",
    "        {\"metric_key\": \"request_latencies\", \"title\": \"Average per request latency\", \"group_by_key\": \"avg_latency\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"request_latencies\", \"title\": \"P90 per request latency\", \"group_by_key\": \"p90\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        # TTFT\n",
    "        {\"metric_key\": \"request_llm_metrics\", \"title\": \"TTFT\", \"group_by_key\": \"ttft\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        # TOPT\n",
    "        {\"metric_key\": \"request_llm_metrics\", \"title\": \"TOPT sec/tkn\", \"group_by_key\": \"topt\", 'speedup_direction': 'lower', \"ylabel\": \"sec/tkn\"},\n",
    "        # throughput_tkns_per_sec\n",
    "        {\"metric_key\": \"request_llm_metrics\", \"title\": \"Total Tokens/sec\", \"group_by_key\": \"throughput_tkns_per_sec\", 'speedup_direction': 'higher', \"ylabel\": \"tkns/sec\"},\n",
    "        {\"metric_key\": \"finished_llm_metrics\", \"title\": \"Num finished 100s\", \"group_by_key\": \"num_finished\", 'speedup_direction': 'higher', \"ylabel\": \"# of requests\"},\n",
    "        {\"metric_key\": \"counts_all\", \"title\": \"Ratio of GPU 0 /(GPU 0 + GPU 1) calls\", \"group_by_key\": \"ratio\", 'speedup_direction': 'higher', \"ylabel\": \"%\"},\n",
    "\n",
    "        # prefill ratio\n",
    "        {\"metric_key\": \"prefill_ratio\", \"title\": \"Prefill Ratio\", \"group_by_key\": \"prefill_ratio\", 'speedup_direction': 'lower', \"ylabel\": \"%\"},\n",
    "\n",
    "        # scheduling_dynamics\n",
    "        {\"metric_key\": \"scheduling_dynamics\", \"title\": \"Max Tree Cache Hit Rate GPU 0\", \"group_by_key\": \"max_tree_cache_hit_rate_gpu0\", 'speedup_direction': 'higher', \"ylabel\": \"%hit\"},\n",
    "        {\"metric_key\": \"scheduling_dynamics\", \"title\": \"Avg Waiting Queue Length\", \"group_by_key\": \"avg_waiting_queue_len\", 'speedup_direction': 'lower', \"ylabel\": \"# requests\"},\n",
    "        # Define additional metric configurations as needed\n",
    "        {\"metric_key\": \"ttft_pmetrics\", \"title\": \"TTFT p50\", \"group_by_key\": \"ttft_p50\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"ttft_pmetrics\", \"title\": \"TTFT p90\", \"group_by_key\": \"ttft_p90\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"ttft_pmetrics\", \"title\": \"TTFT p99\", \"group_by_key\": \"ttft_p99\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"tpot_pmetrics\", \"title\": \"TPOT p50\", \"group_by_key\": \"tpot_p50\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"tpot_pmetrics\", \"title\": \"TPOT p90\", \"group_by_key\": \"tpot_p90\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"tpot_pmetrics\", \"title\": \"TPOT p99\", \"group_by_key\": \"tpot_p99\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"latency_pmetrics\", \"title\": \"Latency p50\", \"group_by_key\": \"latency_p50\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"latency_pmetrics\", \"title\": \"Latency p90\", \"group_by_key\": \"latency_p90\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        {\"metric_key\": \"latency_pmetrics\", \"title\": \"Latency p99\", \"group_by_key\": \"latency_p99\", 'speedup_direction': 'lower', \"ylabel\": \"sec\"},\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    policy_replacements = {\n",
    "        'DataParallelRuntimeSelectionPolicy.': '', \n",
    "        'CUSTOM-CustomPolicyType.': '', \n",
    "        \"-\":'', \n",
    "        \"DataParallelRuntimeSelectionPolicy.RANDOM-:\": \"\",\n",
    "        \"MemSchedulerWithGlobalEviction:\": \"\",\n",
    "        \"TB_DOMAIN_ORACLE:\": \"\",\n",
    "        \"BASIC_MEM_SCHEDULERV2:\": \"\",\n",
    "        \"RANDOM:\": \"\",\n",
    "        \"TB_DOMAIN_ORACLE:\": \"\",\n",
    "        \"TBORACLE_B:\": \"\",\n",
    "        \"HiostgramBasedRecompLoadWithEviction:\": \"\"\n",
    "    }  # Example replacement\n",
    "    \n",
    "\n",
    "    ax_index = 0\n",
    "    for i, config in enumerate(plot_configs):\n",
    "        if ax_index >= len(axes):  # Prevent index out of range\n",
    "            break\n",
    "        metric_data = metrics.get(config[\"metric_key\"], [])\n",
    "        df, title = prepare_data_for_plotting(\n",
    "            metric_data, \n",
    "            config[\"metric_key\"], \n",
    "            config[\"title\"], \n",
    "            policy_replacements, \n",
    "            speedup_direction=config.get('speedup_direction', 'lower'),\n",
    "            group_by_key=config.get(\"group_by_key\"),\n",
    "            baseline_key=baseline_key,\n",
    "            global_metric_columns=global_metric_columns) \n",
    "        if df is not None:\n",
    "            if config[\"metric_key\"] not in global_metric_dfs:\n",
    "                global_metric_dfs[config[\"metric_key\"]] = {}\n",
    "            global_metric_dfs[config[\"metric_key\"]][config[\"group_by_key\"]] = df  # Store dataframe globally\n",
    "            ax = axes[ax_index]\n",
    "            if global_metric_columns:\n",
    "                metric_columns = global_metric_columns\n",
    "            else:\n",
    "                metric_columns = [col for col in df.columns if 'speedup_over_random' != col]\n",
    "            print(df.columns)\n",
    "            bars = df[metric_columns].plot(kind='bar', ax=ax, title=f\"{title_prefix} {title}\", rot=0, ylabel={config[\"ylabel\"]}, legend=(i == 0))\n",
    "            if 'speedup_over_random' in df.columns:\n",
    "                for bar, speedup in zip(bars.get_children(), df['speedup_over_random']):\n",
    "                    if pd.notna(speedup):  # Check if speedup value exists and is not NaN\n",
    "                        height = bar.get_height()\n",
    "                        ax.annotate(f'{speedup:.2f}x',\n",
    "                                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                                    textcoords=\"offset points\",\n",
    "                                    ha='center', va='bottom')\n",
    "            x_labels = []\n",
    "            for idx in df[metric_columns].index:\n",
    "                label = '\\n'.join(str(val) for val in idx)\n",
    "                x_labels.append(label)\n",
    "            bars.set_xticklabels(x_labels)\n",
    "            ax_index += 1\n",
    "\n",
    "            # Customize the plot as needed\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(ax_index, len(axes)):\n",
    "        fig.delaxes(axes[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"eviction_logs_for_load_based_histogram/eviction_load_based_histogram_v5.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Load + Histogram with round robin and histogram\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"recomp_scheduler_without_eviction\",\n",
    "                  \"ROUND_ROBIN:round_robin\"\n",
    "                  ], baseline_key=\"ROUND_ROBIN:round_robin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"prefix_stealing/prefix_stealing_24x.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=9)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Prefix Stealing with Prefix N + 1 GPUs React 24 Examples(longer context)\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"global_evict\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"max_tokens_override/without_chunked_prefill/max_token_override_45.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Loogle max tokens 45 w/o chunked prefill\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"max_tokens_override/without_chunked_prefill/max_token_override_128.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Loogle max tokens 128 w/o chunked prefill\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"max_tokens_override/without_chunked_prefill/max_token_override_256.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Loogle 256 tokens max tokens w/o chunked prefill\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"max_tokens_override/with_chunked_prefill/max_token_override_256.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Loogle 128 tokens max tokens w chunked prefill\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                #   \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"max_tokens_override/2048_chunk.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Loogle 128 tokens max tokens w chunked prefill 2048\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"max_tokens_override/run_to_completion_v2.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Run to completion loogle \", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                #   \"random\"\n",
    "                  ], baseline_key=\"mem_basic_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"toolbench/toolbench_all.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=2)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Toolbench All\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  # \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                #   \"tb_oracle_b\", \n",
    "                  # \"domain_oracle\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"recomp_vs_load/core_vs_round_robin.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=16)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"Multi Domain Toolbench with longer context length 16 GPU\", global_metric_dfs=lp_scheduler_exp,\n",
    "              global_metric_columns=[\n",
    "                  # \"global_evict\",\n",
    "                  \"mem_basic_v2\",\n",
    "                #   \"TBORACLE_B:tb_oracle_b\", \n",
    "                  # \"domain_oracle\",\n",
    "                  \"ROUND_ROBIN:round_robin\",\n",
    "                  \"random\"\n",
    "                  ], baseline_key=\"ROUND_ROBIN:round_robin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_file = '3_node_experiments_verify2.log'\n",
    "# param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "# metrics = extract_metrics(log_file, param_keys, max_gpus=3)\n",
    "# node_3_exp = {}\n",
    "# plot_metrics(metrics, \"3 Node Exp Version 2\", global_metric_dfs=node_3_exp)\n",
    "\n",
    "# log_file = '4_node_experiments_verify2.log'\n",
    "# param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "# metrics = extract_metrics(log_file, param_keys, max_gpus=3)\n",
    "# node_4_exp = {}\n",
    "# plot_metrics(metrics, \"4 Node Exp Version 2\", global_metric_dfs=node_4_exp)\n",
    "\n",
    "# indices_to_compare = [('200', '4096', '8')]\n",
    "# compare_two_plots(node_3_exp,\n",
    "#                   node_4_exp,\n",
    "#                   indices_to_compare, \n",
    "#                   title_prefix=\"Node 3 vs Node 4 at (200, 4096, 8) Version 2\", \n",
    "#                   labels=['TBORACLE_B', 'RANDOM', 'speedup_over_random'], \n",
    "#                   exp1_title=\"Node 3\",\n",
    "#                   exp2_title=\"Node 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_to_compare = [\n",
    "#     ('200', '4096', '6'),\n",
    "#     ('200', '4096', '8'),\n",
    "#     ('200', '4096', '8'),\n",
    "# ]\n",
    "# compare_n_plots(\n",
    "#     [node_2_exp, node_3_exp, node_4_exp],\n",
    "#     [\"Node2\", \"Node 3\", \"Node 4\"],\n",
    "#     indices_to_compare,\n",
    "#     title_prefix=\"Node2(200, 4096, 6), Node 3 & Node 4 at (200, 4096, 8)\\n \",\n",
    "#     labels=['TBORACLE_B', 'RANDOM', 'speedup_over_random']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"lp_scheduler_debug.log\"\n",
    "param_keys = [\"model_name\", \"num_prefix\", \"percent_random_prefixes\", \"num_requests\", \"rps\", \"policy\", \"exp_time\"]\n",
    "metrics = extract_metrics(log_file, param_keys, max_gpus=4)\n",
    "lp_scheduler_exp = {}\n",
    "plot_metrics(metrics, \"LP Scheduler 4 Node\", global_metric_dfs=lp_scheduler_exp, global_metric_columns=['LP_SCHEDULER',  'LOOGLE_ORACLE', 'RANDOM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
